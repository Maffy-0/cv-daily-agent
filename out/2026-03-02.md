# Daily CV Digest (2026-03-02)

- Total: 16

## 1. Curriculum Reinforcement Learning for Quadrotor Racing with Random Obstacles
- arXiv: http://arxiv.org/abs/2602.24030v1
- PDF: https://arxiv.org/pdf/2602.24030v1
- Authors: Fangyu Sun, Fanxing Li, Yu Hu, Linzuo Zhang, Yueqian Liu, Wenxian Yu, Danping Zou
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

Autonomous drone racing has attracted increasing interest as a research topic for exploring the limits of agile flight. However, existing studies primarily focus on obstacle-free racetracks, while the perception and dynamic challenges introduced by obstacles remain underexplored, often resulting in low success rates and limited robustness in real-world flight. To this end, we propose a novel vision-based curriculum reinforcement learning framework for training a robust controller capable of addressing unseen obstacles in drone racing. We combine multi-stage cu rriculum learning, domain randomization, and a multi-scene updating strategy to address the conflicting challenges of obstacle avoidance and gate traversal. Our end-to-end control policy is implemented as a single network, allowing high-speed flight of quadrotors in environments with variable obstacles. Both hardware-in-the-loop and real-world experiments demonstrate that our method achieves faster lap times and higher success rates than existing approaches, effectively advancing drone racing in obstacle-rich environments. The video and code are available at: https://github.com/SJTU-ViSYS-team/CRL-Drone-Racing.

</details>

**LLM Summary**

- What: 障害物のあるドローンレースのために、視覚ベースのカリキュラム強化学習フレームワークを提案した。
- Novelty: 障害物回避とゲート通過という相反する課題に対処するため、多段階カリキュラム学習、ドメインランダム化、マルチシーン更新戦略を組み合わせた。
- Why it matters: 未知の障害物に対処できる頑健な制御器を訓練し、障害物の多い環境でのドローンレースの性能を向上させる。

## 2. ABPolicy: Asynchronous B-Spline Flow Policy for Real-Time and Smooth Robotic Manipulation
- arXiv: http://arxiv.org/abs/2602.23901v1
- PDF: https://arxiv.org/pdf/2602.23901v1
- Authors: Fan Yang, Peiguang Jing, Kaihua Qu, Ningyuan Zhao, Yuting Su
- Keyword score: 3 / hits: real-time

<details><summary>Abstract</summary>

Robotic manipulation requires policies that are smooth and responsive to evolving observations. However, synchronous inference in the raw action space introduces several challenges, including intra-chunk jitter, inter-chunk discontinuities, and stop-and-go execution. These issues undermine a policy's smoothness and its responsiveness to environmental changes. We propose ABPolicy, an asynchronous flow-matching policy that operates in a B-spline control-point action space. First, the B-spline representation ensures intra-chunk smoothness. Second, we introduce bidirectional action prediction coupled with refitting optimization to enforce inter-chunk continuity. Finally, by leveraging asynchronous inference, ABPolicy delivers real-time, continuous updates. We evaluate ABPolicy across seven tasks encompassing both static settings and dynamic settings with moving objects. Empirical results indicate that ABPolicy reduces trajectory jerk, leading to smoother motion and improved performance. Project website: https://teee000.github.io/ABPolicy/.

</details>

**LLM Summary**

- What: リアルタイムで滑らかなロボット操作のための、非同期B-スプラインフローポリシー「ABPolicy」を提案した。
- Novelty: B-スプライン制御点アクション空間で動作し、双方向アクション予測とリフィッティング最適化により、チャンク内およびチャンク間の滑らかさと連続性を保証する。
- Why it matters: ロボット操作における滑らかさと応答性を向上させ、より自然で効率的な動きを実現する。

## 3. StemVLA:An Open-Source Vision-Language-Action Model with Future 3D Spatial Geometry Knowledge and 4D Historical Representation
- arXiv: http://arxiv.org/abs/2602.23721v1
- PDF: https://arxiv.org/pdf/2602.23721v1
- Authors: Jiasong Xiao, Yutao She, Kai Li, Yuyang Sha, Ziang Cheng, Ziang Tong
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-language-action (VLA) models integrate visual observations and language instructions to predict robot actions, demonstrating promising generalization in manipulation tasks. However, most existing approaches primarily rely on direct mappings from 2D visual inputs to action sequences, without explicitly modeling the underlying 3D spatial structure or temporal world dynamics. Such representations may limit spatial reasoning and long-horizon decision-making in dynamic environments. To address this limitation, we propose StemVLA, a novel framework that explicitly incorporates both future-oriented 3D spatial knowledge and historical 4D spatiotemporal representations into action prediction. First, instead of relying solely on observed images, StemVLA forecasts structured 3D future spatial-geometric world knowledge, enabling the model to anticipate upcoming scene geometry and object configurations. Second, to capture temporal consistency and motion dynamics, we feed historical image frames into a pretrained video-geometry transformer backbone to extract implicit 3D world representations, and further aggregate them across time using a temporal attention module, termed VideoFormer [20], forming a unified 4D historical spatiotemporal representation. By jointly modeling 2D observations, predicted 3D future structure, and aggregated 4D temporal dynamics, StemVLA enables more comprehensive world understanding for robot manipulation. Extensive experiments in simulation demonstrate that StemVLA significantly improves long-horizon task success and achieves state-of-the-art performance on the CALVIN ABC-D benchmark [46], achieving an average sequence length of XXX.

</details>

**LLM Summary**

- What: 将来の3D空間幾何学知識と4D履歴表現を組み込んだ、オープンソースのVision-Language-Action (VLA) モデル「StemVLA」を提案した。
- Novelty: 観測画像だけでなく、将来の3D空間幾何学知識を予測し、履歴フレームから3D世界表現を抽出・集約する。
- Why it matters: 3D空間推論と長期的な意思決定能力を向上させ、動的な環境でのロボット操作の汎化性能を高める。

## 4. EgoGraph: Temporal Knowledge Graph for Egocentric Video Understanding
- arXiv: http://arxiv.org/abs/2602.23709v1
- PDF: https://arxiv.org/pdf/2602.23709v1
- Authors: Shitong Sun, Ke Han, Yukai Huang, Weitong Cai, Jifei Song
- Keyword score: 3 / hits: video understanding

<details><summary>Abstract</summary>

Ultra-long egocentric videos spanning multiple days present significant challenges for video understanding. Existing approaches still rely on fragmented local processing and limited temporal modeling, restricting their ability to reason over such extended sequences. To address these limitations, we introduce EgoGraph, a training-free and dynamic knowledge-graph construction framework that explicitly encodes long-term, cross-entity dependencies in egocentric video streams. EgoGraph employs a novel egocentric schema that unifies the extraction and abstraction of core entities, such as people, objects, locations, and events, and structurally reasons about their attributes and interactions, yielding a significantly richer and more coherent semantic representation than traditional clip-based video models. Crucially, we develop a temporal relational modeling strategy that captures temporal dependencies across entities and accumulates stable long-term memory over multiple days, enabling complex temporal reasoning. Extensive experiments on the EgoLifeQA and EgoR1-bench benchmarks demonstrate that EgoGraph achieves state-of-the-art performance on long-term video question answering, validating its effectiveness as a new paradigm for ultra-long egocentric video understanding.

</details>

**LLM Summary**

- What: 長期間の自己中心ビデオ理解のための、トレーニング不要で動的な知識グラフ構築フレームワーク「EgoGraph」を提案した。
- Novelty: 自己中心的なスキーマを用いて、エンティティ（人、物、場所、イベント）を抽出し、それらの属性と相互作用を構造的に推論し、長期的な記憶を蓄積する。
- Why it matters: 長期間のビデオにおける複雑な時間的推論を可能にし、長時間のビデオ質問応答などのタスクで最先端の性能を達成する。

## 5. LE-NeuS: Latency-Efficient Neuro-Symbolic Video Understanding via Adaptive Temporal Verification
- arXiv: http://arxiv.org/abs/2602.23553v1
- PDF: https://arxiv.org/pdf/2602.23553v1
- Authors: Shawn Liang, Sahil Shah, Chengwei Zhou, SP Sharan, Harsh Goel, Arnab Sanyal, Sandeep Chinchali, Gourav Datta
- Keyword score: 3 / hits: video understanding

<details><summary>Abstract</summary>

Neuro-symbolic approaches to long-form video question answering (LVQA) have demonstrated significant accuracy improvements by grounding temporal reasoning in formal verification. However, existing methods incur prohibitive latency overheads, up to 90x slower than base VLM prompting, rendering them impractical for latency-sensitive edge deployments. We present LE-NeuS, a latency-efficient neuro-symbolic framework that preserves the accuracy benefits of temporal logic-guided video understanding while drastically reducing inference latency. Our key insight is that the dominant computational bottleneck arises from sequential and dense proposition detection across video frames during automaton construction. We address this through two principled optimizations: (1) CLIP guided two-stage adaptive sampling that exploits visual redundancy to skip semantically similar frames while preserving temporal boundaries, and (2) batched proposition detection that parallelizes VLM inference across temporal windows. Theoretically, we derive latency bounds as a function of video length, proposition complexity, and sampling density, establishing conditions under which latency efficiency is achievable. Empirically, on LongVideoBench and Video-MME benchmarks deployed on NVIDIA H100 GPUs, LE-NeuS reduces the latency gap from 90x to approximately 10x while maintaining >10% accuracy gains on temporally complex queries.

</details>

**LLM Summary**

- What: 低遅延で高精度な長編ビデオ質問応答（LVQA）を実現する、適応的時系列検証を用いたニューロシンボリックフレームワーク「LE-NeuS」を提案した。
- Novelty: CLIP誘導の2段階適応サンプリングとバッチ化された命題検出により、推論遅延を大幅に削減しつつ、時間論理によるビデオ理解の精度を維持する。
- Why it matters: エッジデバイスでの展開に適した、低遅延で高精度なビデオ理解を可能にする。

## 6. Planning from Observation and Interaction
- arXiv: http://arxiv.org/abs/2602.24121v1
- PDF: https://arxiv.org/pdf/2602.24121v1
- Authors: Tyler Han, Siyang Shen, Rohan Baijal, Harine Ravichandiran, Bat Nemekhbold, Kevin Huang, Sanghun Jung, Byron Boots
- Keyword score: 2 / hits: reinforcement learning, behavior cloning

<details><summary>Abstract</summary>

Observational learning requires an agent to learn to perform a task by referencing only observations of the performed task. This work investigates the equivalent setting in real-world robot learning where access to hand-designed rewards and demonstrator actions are not assumed. To address this data-constrained setting, this work presents a planning-based Inverse Reinforcement Learning (IRL) algorithm for world modeling from observation and interaction alone. Experiments conducted entirely in the real-world demonstrate that this paradigm is effective for learning image-based manipulation tasks from scratch in under an hour, without assuming prior knowledge, pre-training, or data of any kind beyond task observations. Moreover, this work demonstrates that the learned world model representation is capable of online transfer learning in the real-world from scratch. In comparison to existing approaches, including IRL, RL, and Behavior Cloning (BC), which have more restrictive assumptions, the proposed approach demonstrates significantly greater sample efficiency and success rates, enabling a practical path forward for online world modeling and planning from observation and interaction. Videos and more at: https://uwrobotlearning.github.io/mpail2/.

</details>

**LLM Summary**

- What: 観察とインタラクションのみから、ロボットがタスクを実行する方法を学習する逆強化学習アルゴリズムを提案した。
- Novelty: 事前の報酬設計やデモンストレーターの行動を仮定せず、実世界のデータ制約下で機能する。
- Why it matters: 実世界でのロボット学習を効率化し、画像ベースの操作タスクを短時間で学習可能にする。オンラインでの転移学習も可能。

## 7. AoE: Always-on Egocentric Human Video Collection for Embodied AI
- arXiv: http://arxiv.org/abs/2602.23893v1
- PDF: https://arxiv.org/pdf/2602.23893v1
- Authors: Bowen Yang, Zishuo Li, Yang Sun, Changtao Miao, Yifan Yang, Man Luo, Xiaotong Yan, Feng Jiang...
- Keyword score: 2 / hits: real-time, embodied agent

<details><summary>Abstract</summary>

Embodied foundation models require large-scale, high-quality real-world interaction data for pre-training and scaling. However, existing data collection methods suffer from high infrastructure costs, complex hardware dependencies, and limited interaction scope, making scalable expansion challenging. In fact, humans themselves are ideal physically embodied agents. Therefore, obtaining egocentric real-world interaction data from globally distributed "human agents" offers advantages of low cost and sustainability. To this end, we propose the Always-on Egocentric (AoE) data collection system, which aims to simplify hardware dependencies by leveraging humans themselves and their smartphones, enabling low-cost, highly efficient, and scene-agnostic real-world interaction data collection to address the challenge of data scarcity. Specifically, we first employ an ergonomic neck-mounted smartphone holder to enable low-barrier, large-scale egocentric data collection through a cloud-edge collaborative architecture. Second, we develop a cross-platform mobile APP that leverages on-device compute for real-time processing, while the cloud hosts automated labeling and filtering pipelines that transform raw videos into high-quality training data. Finally, the AoE system supports distributed Ego video data collection by anyone, anytime, and anywhere. We evaluate AoE on data preprocessing quality and downstream tasks, demonstrating that high-quality egocentric data significantly boosts real-world generalization.

</details>

**LLM Summary**

- What: 身体性AIの学習データ収集のために、スマートフォンとクラウド・エッジ連携を活用した「Always-on Egocentric (AoE)」データ収集システムを提案した。
- Novelty: 人間自身をエージェントとし、低コストで持続可能な、シーンに依存しない実世界インタラクションデータの収集を可能にする。
- Why it matters: 身体性AIの学習に必要な大規模で高品質な実世界インタラクションデータの不足を解消し、AIの発展を加速させる。

## 8. Hierarchical Action Learning for Weakly-Supervised Action Segmentation
- arXiv: http://arxiv.org/abs/2602.24275v1
- PDF: https://arxiv.org/pdf/2602.24275v1
- Authors: Junxian Huang, Ruichu Cai, Hao Zhu, Juntao Fang, Boyan Xu, Weilin Chen, Zijian Li, Shenghua Gao
- Keyword score: 1 / hits: video understanding

<details><summary>Abstract</summary>

Humans perceive actions through key transitions that structure actions across multiple abstraction levels, whereas machines, relying on visual features, tend to over-segment. This highlights the difficulty of enabling hierarchical reasoning in video understanding. Interestingly, we observe that lower-level visual and high-level action latent variables evolve at different rates, with low-level visual variables changing rapidly, while high-level action variables evolve more slowly, making them easier to identify. Building on this insight, we propose the Hierarchical Action Learning (\textbf{HAL}) model for weakly-supervised action segmentation. Our approach introduces a hierarchical causal data generation process, where high-level latent action governs the dynamics of low-level visual features. To model these varying timescales effectively, we introduce deterministic processes to align these latent variables over time. The \textbf{HAL} model employs a hierarchical pyramid transformer to capture both visual features and latent variables, and a sparse transition constraint is applied to enforce the slower dynamics of high-level action variables. This mechanism enhances the identification of these latent variables over time. Under mild assumptions, we prove that these latent action variables are strictly identifiable. Experimental results on several benchmarks show that the \textbf{HAL} model significantly outperforms existing methods for weakly-supervised action segmentation, confirming its practical effectiveness in real-world applications.

</details>

**LLM Summary**

- What: 低レベルの視覚特徴と高レベルの行動潜在変数が異なる時間スケールで変化するという洞察に基づき、弱教師あり行動セグメンテーションのための階層的行動学習（HAL）モデルを提案した。
- Novelty: 階層的な因果データ生成プロセスと、異なる時間スケールを捉えるための決定論的プロセスを導入した。
- Why it matters: 人間のように階層的な推論を可能にし、ビデオ理解における行動セグメンテーションの精度を向上させる。

## 9. Learning Flexible Job Shop Scheduling under Limited Buffers and Material Kitting Constraints
- arXiv: http://arxiv.org/abs/2602.24180v1
- PDF: https://arxiv.org/pdf/2602.24180v1
- Authors: Shishun Zhang, Juzhan Xu, Yidan Fan, Chenyang Zhu, Ruizhen Hu, Yongjun Wang, Kai Xu
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

The Flexible Job Shop Scheduling Problem (FJSP) originates from real production lines, while some practical constraints are often ignored or idealized in current FJSP studies, among which the limited buffer problem has a particular impact on production efficiency. To this end, we study an extended problem that is closer to practical scenarios--the Flexible Job Shop Scheduling Problem with Limited Buffers and Material Kitting. In recent years, deep reinforcement learning (DRL) has demonstrated considerable potential in scheduling tasks. However, its capacity for state modeling remains limited when handling complex dependencies and long-term constraints. To address this, we leverage a heterogeneous graph network within the DRL framework to model the global state. By constructing efficient message passing among machines, operations, and buffers, the network focuses on avoiding decisions that may cause frequent pallet changes during long-sequence scheduling, thereby helping improve buffer utilization and overall decision quality. Experimental results on both synthetic and real production line datasets show that the proposed method outperforms traditional heuristics and advanced DRL methods in terms of makespan and pallet changes, and also achieves a good balance between solution quality and computational cost. Furthermore, a supplementary video is provided to showcase a simulation system that effectively visualizes the progression of the production line.

</details>

**LLM Summary**

- What: 限定されたバッファとマテリアルキッティング制約を持つフレキシブルジョブショップスケジューリング問題に対し、異種グラフネットワークを深層強化学習フレームワークに組み込んだ手法を提案した。
- Novelty: マシン、オペレーション、バッファ間の効率的なメッセージパッシングにより、複雑な依存関係と長期的な制約を考慮した状態モデリングを実現した。
- Why it matters: 生産効率に影響を与えるバッファ制約を考慮し、パレット交換の頻度を減らし、バッファ利用率と意思決定の質を向上させる。

## 10. GeoDiff4D: Geometry-Aware Diffusion for 4D Head Avatar Reconstruction
- arXiv: http://arxiv.org/abs/2602.24161v1
- PDF: https://arxiv.org/pdf/2602.24161v1
- Authors: Chao Xu, Xiaochen Zhao, Xiang Deng, Jingxiang Sun, Zhuo Su, Donglin Di, Yebin Liu
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Reconstructing photorealistic and animatable 4D head avatars from a single portrait image remains a fundamental challenge in computer vision. While diffusion models have enabled remarkable progress in image and video generation for avatar reconstruction, existing methods primarily rely on 2D priors and struggle to achieve consistent 3D geometry. We propose a novel framework that leverages geometry-aware diffusion to learn strong geometry priors for high-fidelity head avatar reconstruction. Our approach jointly synthesizes portrait images and corresponding surface normals, while a pose-free expression encoder captures implicit expression representations. Both synthesized images and expression latents are incorporated into 3D Gaussian-based avatars, enabling photorealistic rendering with accurate geometry. Extensive experiments demonstrate that our method substantially outperforms state-of-the-art approaches in visual quality, expression fidelity, and cross-identity generalization, while supporting real-time rendering.

</details>

**LLM Summary**

- What: 単一のポートレート画像からフォトリアルでアニメーション可能な4Dヘッドアバターを再構築するために、ジオメトリを考慮した拡散モデル「GeoDiff4D」を提案した。
- Novelty: ジオメトリ事前知識を学習し、画像と表面法線を同時に合成することで、高忠実度なアバター再構築を実現した。
- Why it matters: 既存手法よりも優れた視覚品質、表情忠実度、クロスアイデンティティ汎化性能を持ち、リアルタイムレンダリングをサポートする。

## 11. Foundation World Models for Agents that Learn, Verify, and Adapt Reliably Beyond Static Environments
- arXiv: http://arxiv.org/abs/2602.23997v1
- PDF: https://arxiv.org/pdf/2602.23997v1
- Authors: Florent Delgrange
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

The next generation of autonomous agents must not only learn efficiently but also act reliably and adapt their behavior in open worlds. Standard approaches typically assume fixed tasks and environments with little or no novelty, which limits world models' ability to support agents that must evolve their policies as conditions change. This paper outlines a vision for foundation world models: persistent, compositional representations that unify reinforcement learning, reactive/program synthesis, and abstraction mechanisms. We propose an agenda built around four components: (i) learnable reward models from specifications to support optimization with clear objectives; (ii) adaptive formal verification integrated throughout learning; (iii) online abstraction calibration to quantify the reliability of the model's predictions; and (iv) test-time synthesis and world-model generation guided by verifiers. Together, these components enable agents to synthesize verifiable programs, derive new policies from a small number of interactions, and maintain correctness while adapting to novelty. The resulting framework positions foundation world models as a substrate for learning, reasoning, and adaptation, laying the groundwork for agents that not only act well but can explain and justify the behavior they adopt.

</details>

**LLM Summary**

- What: 静的な環境を超えて信頼性高く学習、検証、適応するエージェントのための基盤ワールドモデルのビジョンを提案。学習可能な報酬モデル、統合された形式検証、オンライン抽象化キャリブレーション、検証器によるテスト時合成などを中心としたアジェンダを提示。
- Novelty: 永続的で構成可能な表現を用いて、強化学習、プログラム合成、抽象化メカニズムを統一する基盤ワールドモデルの概念と、それを実現するための具体的な4つのコンポーネントを提案。
- Why it matters: エージェントが検証可能なプログラムを合成し、少数のインタラクションから新しいポリシーを導き出し、新規性への適応中に正しさを維持できるようにする。

## 12. Thinking with Images as Continuous Actions: Numerical Visual Chain-of-Thought
- arXiv: http://arxiv.org/abs/2602.23959v1
- PDF: https://arxiv.org/pdf/2602.23959v1
- Authors: Kesen Zhao, Beier Zhu, Junbao Zhou, Xingyu Zhu, Zhongqi Yue, Hanwang Zhang
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Recent multimodal large language models (MLLMs) increasingly rely on visual chain-of-thought to perform region-grounded reasoning over images. However, existing approaches ground regions via either textified coordinates-causing modality mismatch and semantic fragmentation or fixed-granularity patches that both limit precise region selection and often require non-trivial architectural changes. In this paper, we propose Numerical Visual Chain-of-Thought (NV-CoT), a framework that enables MLLMs to reason over images using continuous numerical coordinates. NV-CoT expands the MLLM action space from discrete vocabulary tokens to a continuous Euclidean space, allowing models to directly generate bounding-box coordinates as actions with only minimal architectural modification. The framework supports both supervised fine-tuning and reinforcement learning. In particular, we replace categorical token policies with a Gaussian (or Laplace) policy over coordinates and introduce stochasticity via reparameterized sampling, making NV-CoT fully compatible with GRPO-style policy optimization. Extensive experiments on three benchmarks against eight representative visual reasoning baselines demonstrate that NV-CoT significantly improves localization precision and final answer accuracy, while also accelerating training convergence, validating the effectiveness of continuous-action visual reasoning in MLLMs. The code is available in https://github.com/kesenzhao/NV-CoT.

</details>

**LLM Summary**

- What: 画像を連続的なアクションとして扱う「Numerical Visual Chain-of-Thought (NV-CoT)」フレームワークを提案。これにより、マルチモーダル大規模言語モデル（MLLM）が連続的な数値座標を用いて画像上で推論できるようになる。
- Novelty: 離散的な語彙トークンから連続的なユークリッド空間へのアクション空間の拡張。バウンディングボックス座標を直接アクションとして生成可能にし、最小限のアーキテクチャ変更で済む。
- Why it matters: MLLMの画像理解と推論能力を向上させ、より精密な領域選択と、より柔軟な推論を可能にする。

## 13. The Auton Agentic AI Framework
- arXiv: http://arxiv.org/abs/2602.23720v1
- PDF: https://arxiv.org/pdf/2602.23720v1
- Authors: Sheng Cao, Zhao Chang, Chang Li, Hannan Li, Liyao Fu, Ji Tang
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

The field of Artificial Intelligence is undergoing a transition from Generative AI -- probabilistic generation of text and images -- to Agentic AI, in which autonomous systems execute actions within external environments on behalf of users. This transition exposes a fundamental architectural mismatch: Large Language Models (LLMs) produce stochastic, unstructured outputs, whereas the backend infrastructure they must control -- databases, APIs, cloud services -- requires deterministic, schema-conformant inputs. The present paper describes the Auton Agentic AI Framework, a principled architecture for standardizing the creation, execution, and governance of autonomous agent systems. The framework is organized around a strict separation between the Cognitive Blueprint, a declarative, language-agnostic specification of agent identity and capabilities, and the Runtime Engine, the platform-specific execution substrate that instantiates and runs the agent. This separation enables cross-language portability, formal auditability, and modular tool integration via the Model Context Protocol (MCP). The paper formalizes the agent execution model as an augmented Partially Observable Markov Decision Process (POMDP) with a latent reasoning space, introduces a hierarchical memory consolidation architecture inspired by biological episodic memory systems, defines a constraint manifold formalism for safety enforcement via policy projection rather than post-hoc filtering, presents a three-level self-evolution framework spanning in-context adaptation through reinforcement learning, and describes runtime optimizations -- including parallel graph execution, speculative inference, and dynamic context pruning -- that reduce end-to-end latency for multi-step agent workflows.

</details>

**LLM Summary**

- What: 生成AIからエージェントAIへの移行に対応する「Auton Agentic AI Framework」を提案。エージェントの作成、実行、ガバナンスを標準化するアーキテクチャを記述。
- Novelty: エージェントのアイデンティティと能力を宣言的に記述する「Cognitive Blueprint」と、プラットフォーム固有の実行基盤である「Runtime Engine」を厳密に分離。
- Why it matters: 言語非依存のポータビリティ、形式的な監査可能性、モジュール化されたツール統合を可能にし、エージェントAIシステムの開発と運用を効率化する。

## 14. Interpretable Multimodal Gesture Recognition for Drone and Mobile Robot Teleoperation via Log-Likelihood Ratio Fusion
- arXiv: http://arxiv.org/abs/2602.23694v1
- PDF: https://arxiv.org/pdf/2602.23694v1
- Authors: Seungyeol Baek, Jaspreet Singh, Lala Shakti Swarup Ray, Hymalai Bello, Paul Lukowicz, Sungho Suh
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Human operators are still frequently exposed to hazardous environments such as disaster zones and industrial facilities, where intuitive and reliable teleoperation of mobile robots and Unmanned Aerial Vehicles (UAVs) is essential. In this context, hands-free teleoperation enhances operator mobility and situational awareness, thereby improving safety in hazardous environments. While vision-based gesture recognition has been explored as one method for hands-free teleoperation, its performance often deteriorates under occlusions, lighting variations, and cluttered backgrounds, limiting its applicability in real-world operations. To overcome these limitations, we propose a multimodal gesture recognition framework that integrates inertial data (accelerometer, gyroscope, and orientation) from Apple Watches on both wrists with capacitive sensing signals from custom gloves. We design a late fusion strategy based on the log-likelihood ratio (LLR), which not only enhances recognition performance but also provides interpretability by quantifying modality-specific contributions. To support this research, we introduce a new dataset of 20 distinct gestures inspired by aircraft marshalling signals, comprising synchronized RGB video, IMU, and capacitive sensor data. Experimental results demonstrate that our framework achieves performance comparable to a state-of-the-art vision-based baseline while significantly reducing computational cost, model size, and training time, making it well suited for real-time robot control. We therefore underscore the potential of sensor-based multimodal fusion as a robust and interpretable solution for gesture-driven mobile robot and drone teleoperation.

</details>

**LLM Summary**

- What: ドローンやモバイルロボットの遠隔操作における、解釈可能なマルチモーダルジェスチャー認識フレームワークを提案。手首の慣性データとカスタムグローブの静電容量センシング信号を、対数尤度比（LLR）ベースの late fusion で統合。
- Novelty: LLRベースのフュージョン戦略により、認識性能の向上だけでなく、モダリティごとの寄与を定量化することで解釈可能性を提供。
- Why it matters: 視覚情報のみに依存する従来のジェスチャー認識の限界（遮蔽、照明変動など）を克服し、危険な環境でのロボット遠隔操作の安全性と信頼性を向上させる。

## 15. FAVLA: A Force-Adaptive Fast-Slow VLA model for Contact-Rich Robotic Manipulation
- arXiv: http://arxiv.org/abs/2602.23648v1
- PDF: https://arxiv.org/pdf/2602.23648v1
- Authors: Yao Li, Peiyuan Tang, Wuyang Zhang, Chengyang Zhu, Yifan Duan, Weikai Shi, Xiaodong Zhang, Zijiang Yang...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Force/torque feedback can substantially improve Vision-Language-Action (VLA) models on contact-rich manipulation, but most existing approaches fuse all modalities at a single operating frequency. This design ignores the mismatched sampling rates of real robot sensors, forcing downsampling of the high-frequency contact cues needed for reactive correction. Combined with common VLM-action-expert (AE) pipelines that execute action chunks largely open loop between expensive VLM updates, unified-frequency fusion often yields delayed responses to impacts, stick-slip, and force spikes. We propose FAVLA, a force-adaptive fast-slow VLA that decouples slow perception planning from fast contact-aware control. FAVLA runs a slow VLM at a fixed low frequency to encode modalities to produce latent representations and to predict near-future force variation. A fast AE then executes at a variable high frequency, conditioning on the latest force sequence data to generate reactive actions. We further introduce a force adapter that injects high-frequency force features into multiple AE layers, and adaptively schedules the AE's execution frequency based on the VLM's predicted force variation. Extensive experiments on contact-rich tasks demonstrate that FAVLA significantly outperforms baselines, achieving superior reactivity and success rates, especially with a smaller contact force during manipulation.

</details>

**LLM Summary**

- What: 接触を伴うロボット操作のための、フォース適応型高速・低速VLAモデル「FAVLA」を提案。
- Novelty: 遅い知覚計画と速い接触認識制御を分離し、低頻度のVLMと高頻度のAEを組み合わせる。高頻度のフォース特徴をAEの複数層に注入し、VLMの予測フォース変動に基づいてAEの実行頻度を適応的にスケジューリング。
- Why it matters: センサーのサンプリングレートの不一致に対処し、接触時の遅延応答やフォーススパイクへの対応を改善し、より堅牢で反応性の高いロボット操作を実現する。

## 16. VCA: Vision-Click-Action Framework for Precise Manipulation of Segmented Objects in Target Ambiguous Environments
- arXiv: http://arxiv.org/abs/2602.23583v1
- PDF: https://arxiv.org/pdf/2602.23583v1
- Authors: Donggeon Kim, Seungwon Jan, Hyeonjun Park, Daegyu Lim
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

The reliance on language in Vision-Language-Action (VLA) models introduces ambiguity, cognitive overhead, and difficulties in precise object identification and sequential task execution, particularly in environments with multiple visually similar objects. To address these limitations, we propose Vision-Click-Action (VCA), a framework that replaces verbose textual commands with direct, click-based visual interaction using pretrained segmentation models. By allowing operators to specify target objects clearly through visual selection in the robot's 2D camera view, VCA reduces interpretation errors, lowers cognitive load, and provides a practical and scalable alternative to language-driven interfaces for real-world robotic manipulation. Experimental results validate that the proposed VCA framework achieves effective instance-level manipulation of specified target objects. Experiment videos are available at https://robrosinc.github.io/vca/.

</details>

**LLM Summary**

- What: 言語による指示の曖昧さを解消するため、視覚的なクリック操作で対象オブジェクトを直接指定するVision-Click-Action (VCA) フレームワークを提案した。
- Novelty: 事前学習済みのセグメンテーションモデルを利用し、ロボットの2Dカメラビュー上で直接クリックすることで、対象オブジェクトを明確に指定する。
- Why it matters: ロボットによる精密な操作、特に視覚的に類似したオブジェクトが多い環境でのタスク実行における、言語指示の曖昧さや認知負荷を低減し、実用的なインターフェースを提供する。
