# Daily CV Digest (2026-02-10)

- Total: 16

## 1. Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique
- arXiv: http://arxiv.org/abs/2602.06620v1
- PDF: https://arxiv.org/pdf/2602.06620v1
- Authors: Hiroshi Sato, Sho Sakaino, Toshiaki Tsuji
- Keyword score: 4 / hits: vision-language-action, imitation learning

<details><summary>Abstract</summary>

In contact-rich tasks, while position trajectories are often easy to obtain, appropriate force commands are typically unknown. Although it is conceivable to generate force commands using a pretrained foundation model such as Vision-Language-Action (VLA) models, force control is highly dependent on the specific hardware of the robot, which makes the application of such models challenging. To bridge this gap, we propose a force generative model that estimates force commands from given position trajectories. However, when dealing with unseen position trajectories, the model struggles to generate accurate force commands. To address this, we introduce a feedback control mechanism. Our experiments reveal that feedback control does not converge when the force generative model has memory. We therefore adopt a model without memory, enabling stable feedback control. This approach allows the system to generate force commands effectively, even for unseen position trajectories, improving generalization for real-world robot writing tasks.

</details>

**LLM Summary**

- What: 位置軌跡からロボットアームの力コマンドを生成するモデルを提案。特に、未知の位置軌跡に対しては、フィードバック制御機構を導入し、メモリを持たないモデルを採用することで安定した制御を実現した。
- Novelty: 位置軌跡から力コマンドを生成するモデルと、未知の軌跡に対するフィードバック制御の組み合わせ。メモリを持たないモデルによる安定化。
- Why it matters: ロボットの接触を伴うタスクにおいて、位置軌跡は入手しやすいが力コマンドは未知であることが多い。この手法により、汎用的な力コマンド生成と、未知の状況への適応が可能になり、ロボットの書き込みタスクなどの実世界応用における汎化性能を向上させる。

## 2. LIBERO-X: Robustness Litmus for Vision-Language-Action Models
- arXiv: http://arxiv.org/abs/2602.06556v1
- PDF: https://arxiv.org/pdf/2602.06556v1
- Authors: Guodong Wang, Chenkai Zhang, Qingjie Liu, Jinjin Zhang, Jiancheng Cai, Junjie Liu, Xinmin Liu
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Reliable benchmarking is critical for advancing Vision-Language-Action (VLA) models, as it reveals their generalization, robustness, and alignment of perception with language-driven manipulation tasks. However, existing benchmarks often provide limited or misleading assessments due to insufficient evaluation protocols that inadequately capture real-world distribution shifts. This work systematically rethinks VLA benchmarking from both evaluation and data perspectives, introducing LIBERO-X, a benchmark featuring: 1) A hierarchical evaluation protocol with progressive difficulty levels targeting three core capabilities: spatial generalization, object recognition, and task instruction understanding. This design enables fine-grained analysis of performance degradation under increasing environmental and task complexity; 2) A high-diversity training dataset collected via human teleoperation, where each scene supports multiple fine-grained manipulation objectives to bridge the train-evaluation distribution gap. Experiments with representative VLA models reveal significant performance drops under cumulative perturbations, exposing persistent limitations in scene comprehension and instruction grounding. By integrating hierarchical evaluation with diverse training data, LIBERO-X offers a more reliable foundation for assessing and advancing VLA development.

</details>

**LLM Summary**

- What: Vision-Language-Action (VLA) モデルの頑健性を評価するためのベンチマーク「LIBERO-X」を提案。階層的な評価プロトコルと、多様な操作目標を持つ訓練データセットを特徴とする。
- Novelty: VLAモデルの評価とデータセットの両面から再考されたベンチマーク設計。段階的な難易度を持つ階層的評価プロトコルと、実世界の分布シフトに対応する高多様性訓練データセット。
- Why it matters: VLAモデルの汎化性能、頑健性、知覚と言語駆動型操作タスクとの整合性を正確に評価するために不可欠。既存のベンチマークの限界を克服し、モデルの弱点をより詳細に特定することで、VLAモデルの進歩を促進する。

## 3. DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving
- arXiv: http://arxiv.org/abs/2602.06521v1
- PDF: https://arxiv.org/pdf/2602.06521v1
- Authors: Feiyang jia, Lin Liu, Ziying Song, Caiyan Jia, Hangjun Ye, Xiaoshuai Hao, Long Chen
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

End-to-end (E2E) autonomous driving has recently attracted increasing interest in unifying Vision-Language-Action (VLA) with World Models to enhance decision-making and forward-looking imagination. However, existing methods fail to effectively unify future scene evolution and action planning within a single architecture due to inadequate sharing of latent states, limiting the impact of visual imagination on action decisions. To address this limitation, we propose DriveWorld-VLA, a novel framework that unifies world modeling and planning within a latent space by tightly integrating VLA and world models at the representation level, which enables the VLA planner to benefit directly from holistic scene-evolution modeling and reducing reliance on dense annotated supervision. Additionally, DriveWorld-VLA incorporates the latent states of the world model as core decision-making states for the VLA planner, facilitating the planner to assess how candidate actions impact future scene evolution. By conducting world modeling entirely in the latent space, DriveWorld-VLA supports controllable, action-conditioned imagination at the feature level, avoiding expensive pixel-level rollouts. Extensive open-loop and closed-loop evaluations demonstrate the effectiveness of DriveWorld-VLA, which achieves state-of-the-art performance with 91.3 PDMS on NAVSIMv1, 86.8 EPDMS on NAVSIMv2, and 0.16 3-second average collision rate on nuScenes. Code and models will be released in https://github.com/liulin815/DriveWorld-VLA.git.

</details>

**LLM Summary**

- What: 自律走行における世界モデリングと行動計画を統合する「DriveWorld-VLA」フレームワークを提案。VLAと世界モデルを潜在空間で統合し、将来のシーン進化を予測する能力を向上させる。
- Novelty: VLAと世界モデルを表現レベルで緊密に統合し、潜在空間内で世界モデリングと計画を統一。世界モデルの潜在状態をVLAプランナーの意思決定状態として活用。
- Why it matters: 自律走行における意思決定と将来予測能力を強化する。ピクセルレベルのロールアウトを回避し、制御可能で行動条件付きの想像を可能にすることで、より効率的で安全な自律走行システムの開発に貢献する。

## 4. Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation
- arXiv: http://arxiv.org/abs/2602.06512v1
- PDF: https://arxiv.org/pdf/2602.06512v1
- Authors: Junhong Zhu, Ji Zhang, Jingkuan Song, Lianli Gao, Heng Tao Shen
- Keyword score: 3 / hits: imitation learning

<details><summary>Abstract</summary>

While generalist robot policies hold significant promise for learning diverse manipulation skills through imitation, their performance is often hindered by the long-tail distribution of training demonstrations. Policies learned on such data, which is heavily skewed towards a few data-rich head tasks, frequently exhibit poor generalization when confronted with the vast number of data-scarce tail tasks. In this work, we conduct a comprehensive analysis of the pervasive long-tail challenge inherent in policy learning. Our analysis begins by demonstrating the inefficacy of conventional long-tail learning strategies (e.g., re-sampling) for improving the policy's performance on tail tasks. We then uncover the underlying mechanism for this failure, revealing that data scarcity on tail tasks directly impairs the policy's spatial reasoning capability. To overcome this, we introduce Approaching-Phase Augmentation (APA), a simple yet effective scheme that transfers knowledge from data-rich head tasks to data-scarce tail tasks without requiring external demonstrations. Extensive experiments in both simulation and real-world manipulation tasks demonstrate the effectiveness of APA. Our code and demos are publicly available at: https://mldxy.github.io/Project-VLA-long-tail/.

</details>

**LLM Summary**

- What: ロボット操作における「ロングテール」問題（データが少ないタスクの性能低下）に対処するため、「Approaching-Phase Augmentation (APA）」という知識転移手法を提案。
- Novelty: データ豊富なタスクからデータ不足のタスクへ、外部デモンストレーションなしに知識を転移させるAPA。ロングテール問題の根本原因（空間推論能力の低下）を特定。
- Why it matters: データ不足のロボット操作タスクにおける汎化性能を向上させる。従来のロングテール学習戦略の非効率性を明らかにし、より効果的な学習方法を提供することで、多様なロボット操作スキルの習得を促進する。

## 5. Towards Adaptive Environment Generation for Training Embodied Agents
- arXiv: http://arxiv.org/abs/2602.06366v1
- PDF: https://arxiv.org/pdf/2602.06366v1
- Authors: Teresa Yeo, Dulaj Weerakoon, Dulanga Weerakoon, Archan Misra
- Keyword score: 3 / hits: embodied agent

<details><summary>Abstract</summary>

Embodied agents struggle to generalize to new environments, even when those environments share similar underlying structures to their training settings. Most current approaches to generating these training environments follow an open-loop paradigm, without considering the agent's current performance. While procedural generation methods can produce diverse scenes, diversity without feedback from the agent is inefficient. The generated environments may be trivially easy, providing limited learning signal. To address this, we present a proof-of-concept for closed-loop environment generation that adapts difficulty to the agent's current capabilities. Our system employs a controllable environment representation, extracts fine-grained performance feedback beyond binary success or failure, and implements a closed-loop adaptation mechanism that translates this feedback into environment modifications. This feedback-driven approach generates training environments that more challenging in the ways the agent needs to improve, enabling more efficient learning and better generalization to novel settings.

</details>

**LLM Summary**

- What: 具現化されたエージェント（Embodied Agents）の学習効率と汎化性能を向上させるため、エージェントの能力に合わせて難易度を適応させる「閉ループ環境生成」システムを提案。
- Novelty: エージェントのパフォーマンスフィードバックに基づき、環境の難易度を動的に調整する閉ループ生成。二値的な成功/失敗以上の詳細なパフォーマンスフィードバックを活用。
- Why it matters: エージェントが改善を必要とする箇所をより挑戦的にすることで、学習を効率化し、新しい環境への汎化能力を高める。現在のオープンループ環境生成の非効率性を克服し、より効果的なエージェントトレーニングを実現する。

## 6. World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy
- arXiv: http://arxiv.org/abs/2602.06508v1
- PDF: https://arxiv.org/pdf/2602.06508v1
- Authors: Xiaokang Liu, Zechen Bai, Hai Ci, Kevin Yuchen Ma, Mike Zheng Shou
- Keyword score: 2 / hits: vision-language-action, reinforcement learning

<details><summary>Abstract</summary>

Recent progress in robotic world models has leveraged video diffusion transformers to predict future observations conditioned on historical states and actions. While these models can simulate realistic visual outcomes, they often exhibit poor action-following precision, hindering their utility for downstream robotic learning. In this work, we introduce World-VLA-Loop, a closed-loop framework for the joint refinement of world models and Vision-Language-Action (VLA) policies. We propose a state-aware video world model that functions as a high-fidelity interactive simulator by jointly predicting future observations and reward signals. To enhance reliability, we introduce the SANS dataset, which incorporates near-success trajectories to improve action-outcome alignment within the world model. This framework enables a closed-loop for reinforcement learning (RL) post-training of VLA policies entirely within a virtual environment. Crucially, our approach facilitates a co-evolving cycle: failure rollouts generated by the VLA policy are iteratively fed back to refine the world model precision, which in turn enhances subsequent RL optimization. Evaluations across simulation and real-world tasks demonstrate that our framework significantly boosts VLA performance with minimal physical interaction, establishing a mutually beneficial relationship between world modeling and policy learning for general-purpose robotics. Project page: https://showlab.github.io/World-VLA-Loop/.

</details>

**LLM Summary**

- What: 視覚情報と行動ポリシーを共同で洗練する閉ループ学習フレームワーク「World-VLA-Loop」を提案。状態を考慮したビデオ世界モデルは、将来の観測と報酬信号を予測し、高忠実度のインタラクティブシミュレータとして機能する。SANSデータセットで行動と結果の整合性を向上させ、仮想環境内で強化学習によるポリシーの最適化を可能にする。失敗したロールアウトを世界モデルの改善にフィードバックする。
- Novelty: 世界モデルとVLAポリシーの共同洗練、状態を考慮したビデオ世界モデル、SANSデータセット、閉ループでの強化学習最適化、失敗ロールアウトによる世界モデルの反復的改善。
- Why it matters: ロボット学習における行動追従精度を向上させ、仮想環境での効率的なポリシー学習と実世界での汎用的なタスク実行を可能にする。

## 7. Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation
- arXiv: http://arxiv.org/abs/2602.06356v1
- PDF: https://arxiv.org/pdf/2602.06356v1
- Authors: Gang He, Zhenyang Liu, Kepeng Xu, Li Xu, Tong Qiao, Wenxin Yu, Chang Wu, Weiying Xie
- Keyword score: 2 / hits: embodied agent, imitation learning

<details><summary>Abstract</summary>

Vision-Language Navigation (VLN) requires embodied agents to interpret natural language instructions and navigate through complex continuous 3D environments. However, the dominant imitation learning paradigm suffers from exposure bias, where minor deviations during inference lead to compounding errors. While DAgger-style approaches attempt to mitigate this by correcting error states, we identify a critical limitation: Instruction-State Misalignment. Forcing an agent to learn recovery actions from off-track states often creates supervision signals that semantically conflict with the original instruction. In response to these challenges, we introduce BudVLN, an online framework that learns from on-policy rollouts by constructing supervision to match the current state distribution. BudVLN performs retrospective rectification via counterfactual re-anchoring and decision-conditioned supervision synthesis, using a geodesic oracle to synthesize corrective trajectories that originate from valid historical states, ensuring semantic consistency. Experiments on the standard R2R-CE and RxR-CE benchmarks demonstrate that BudVLN consistently mitigates distribution shift and achieves state-of-the-art performance in both Success Rate and SPL.

</details>

**LLM Summary**

- What: Vision-Language Navigation (VLN) における暴露バイアスと指示-状態の不一致に対処するため、オンラインフレームワーク「BudVLN」を提案。現在の状態分布に合わせた教師信号を構築し、対数再固定と決定条件付き教師信号合成による後方的修正を行う。
- Novelty: 指示-状態の不一致を解決する後方的修正手法、対数再固定、決定条件付き教師信号合成、測地線オラクルを用いた補正軌道の合成。
- Why it matters: VLNエージェントの分布シフトを軽減し、指示と状態の間の意味的な一貫性を保ちながら、よりロバストで正確なナビゲーションを実現する。

## 8. Context Forcing: Consistent Autoregressive Video Generation with Long Context
- arXiv: http://arxiv.org/abs/2602.06028v1
- PDF: https://arxiv.org/pdf/2602.06028v1
- Authors: Shuo Chen, Cong Wei, Sun Sun, Ping Nie, Kai Zhou, Ge Zhang, Ming-Hsuan Yang, Wenhu Chen
- Keyword score: 2 / hits: streaming, real-time

<details><summary>Abstract</summary>

Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.

</details>

**LLM Summary**

- What: 長期コンテキストでの一貫した自動回帰ビデオ生成を実現する「Context Forcing」フレームワークを提案。長期コンテキストを持つ教師モデルを使用し、学生モデルを訓練することで、教師と学生のミスマッチを解消する。計算効率のために「Slow-Fast Memory」アーキテクチャを導入。
- Novelty: 長期コンテキストを持つ教師モデルによる訓練、教師-学生ミスマッチの解消、Slow-Fast Memoryアーキテクチャ。
- Why it matters: 従来の短時間ウィンドウの教師モデルでは不可能だった、長期間にわたるビデオ生成の一貫性と品質を大幅に向上させる。

## 9. DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos
- arXiv: http://arxiv.org/abs/2602.06949v1
- PDF: https://arxiv.org/pdf/2602.06949v1
- Authors: Shenyuan Gao, William Liang, Kaiyuan Zheng, Ayaan Malik, Seonghyeon Ye, Sihyun Yu, Wei-Cheng Tseng, Yuzhu Dong...
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.

</details>

**LLM Summary**

- What: 大規模な人間のビデオから多様な相互作用と器用な制御を学習する汎用ロボット世界モデル「DreamDojo」を開発。44k時間の自己中心ビデオデータを使用し、連続的な潜在行動を統一プロキシ行動として導入。
- Novelty: 大規模な人間のビデオデータセット、連続潜在行動の導入、汎用ロボット世界モデル。
- Why it matters: 物理法則の理解と正確な行動制御能力を持つ基盤世界モデルを提供し、ロボット開発の効率化と汎用エージェントのスケールアップを促進する。

## 10. Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation
- arXiv: http://arxiv.org/abs/2602.06575v1
- PDF: https://arxiv.org/pdf/2602.06575v1
- Authors: Fangyuan Wang, Peng Zhou, Jiaming Qi, Shipeng Lyu, David Navarro-Alarcon, Guodong Guo
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-language-action (VLA) models typically inject proprioception only as a late conditioning signal, which prevents robot state from shaping instruction understanding and from influencing which visual tokens are attended throughout the policy. We introduce ThinkProprio, which converts proprioception into a sequence of text tokens in the VLM embedding space and fuses them with the task instruction at the input. This early fusion lets embodied state participate in subsequent visual reasoning and token selection, biasing computation toward action-critical evidence while suppressing redundant visual tokens. In a systematic ablation over proprioception encoding, state entry point, and action-head conditioning, we find that text tokenization is more effective than learned projectors, and that retaining roughly 15% of visual tokens can match the performance of using the full token set. Across CALVIN, LIBERO, and real-world manipulation, ThinkProprio matches or improves over strong baselines while reducing end-to-end inference latency over 50%.

</details>

**LLM Summary**

- What: プロプリオセプション（自己受容感覚）をテキストトークンに変換し、タスク指示と早期に融合させる「ThinkProprio」を提案。これにより、ロボットの状態が指示理解と視覚トークンの選択に影響を与えるようにする。
- Novelty: プロプリオセプションのテキストトークン化と早期融合、自己受容感覚による視覚的推論のバイアス。
- Why it matters: ロボットの自己受容感覚を効果的に活用し、指示理解と視覚的推論を改善することで、より効率的で正確なVLA操作を実現し、推論レイテンシを削減する。

## 11. SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs
- arXiv: http://arxiv.org/abs/2602.06566v1
- PDF: https://arxiv.org/pdf/2602.06566v1
- Authors: Niccolo Avogaro, Nayanika Debnath, Li Mi, Thomas Frick, Junling Wang, Zexue He, Hang Hua, Konrad Schindler...
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses "thinking with images" by 4.6 points on a challenging OOD task despite requiring a 200$\times$ lower token budget.

</details>

**LLM Summary**

- What: 視覚と言語のモデル（VLM）において、推論時にトークン予算を動的に拡張するテスト時スケーリングの課題に対処するため、知覚と推論の回路を分離するモジュールフレームワーク「SPARC」を提案。
- Novelty: 知覚と推論を明示的に分離し、脳の感覚から認知への処理にインスパイアされた2段階パイプライン（視覚検索→推論）を採用。
- Why it matters: テスト時の独立したスケーリング、非対称な計算リソース配分、ボトルネックとなっている知覚ステージの選択的最適化を可能にし、VLMの効率と性能向上に貢献する。

## 12. Action Hallucination in Generative Visual-Language-Action Models
- arXiv: http://arxiv.org/abs/2602.06339v1
- PDF: https://arxiv.org/pdf/2602.06339v1
- Authors: Harold Soh, Eugene Lim
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Robot Foundation Models such as Vision-Language-Action models are rapidly reshaping how robot policies are trained and deployed, replacing hand-designed planners with end-to-end generative action models. While these systems demonstrate impressive generalization, it remains unclear whether they fundamentally resolve the long-standing challenges of robotics. We address this question by analyzing action hallucinations that violate physical constraints and their extension to plan-level failures. Focusing on latent-variable generative policies, we show that hallucinations often arise from structural mismatches between feasible robot behavior and common model architectures. We study three such barriers -- topological, precision, and horizon -- and show how they impose unavoidable tradeoffs. Our analysis provides mechanistic explanations for reported empirical failures of generative robot policies and suggests principled directions for improving reliability and trustworthiness, without abandoning their expressive power.

</details>

**LLM Summary**

- What: ロボットの視覚・言語・行動モデル（VLAモデル）における、物理的制約を破る「アクションの幻覚」とその計画レベルへの影響を分析。
- Novelty: 生成モデルのアーキテクチャとロボットの実現可能な行動との間の構造的ミスマッチ（トポロジー、精度、ホライズン）に起因する幻覚のメカニズムを解明。
- Why it matters: 生成ロボットポリシーの信頼性と信頼性を向上させるための原理的な方向性を示唆し、表現力を維持しながらロボット制御の課題解決に貢献する。

## 13. ASMa: Asymmetric Spatio-temporal Masking for Skeleton Action Representation Learning
- arXiv: http://arxiv.org/abs/2602.06251v1
- PDF: https://arxiv.org/pdf/2602.06251v1
- Authors: Aman Anand, Amir Eskandari, Elyas Rahsno, Farhana Zulkernine
- Keyword score: 1 / hits: action recognition

<details><summary>Abstract</summary>

Self-supervised learning (SSL) has shown remarkable success in skeleton-based action recognition by leveraging data augmentations to learn meaningful representations. However, existing SSL methods rely on data augmentations that predominantly focus on masking high-motion frames and high-degree joints such as joints with degree 3 or 4. This results in biased and incomplete feature representations that struggle to generalize across varied motion patterns. To address this, we propose Asymmetric Spatio-temporal Masking (ASMa) for Skeleton Action Representation Learning, a novel combination of masking to learn a full spectrum of spatio-temporal dynamics inherent in human actions. ASMa employs two complementary masking strategies: one that selectively masks high-degree joints and low-motion, and another that masks low-degree joints and high-motion frames. These masking strategies ensure a more balanced and comprehensive skeleton representation learning. Furthermore, we introduce a learnable feature alignment module to effectively align the representations learned from both masked views. To facilitate deployment in resource-constrained settings and on low-resource devices, we compress the learned and aligned representation into a lightweight model using knowledge distillation. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate that our approach outperforms existing SSL methods with an average improvement of 2.7-4.4% in fine-tuning and up to 5.9% in transfer learning to noisy datasets and achieves competitive performance compared to fully supervised baselines. Our distilled model achieves 91.4% parameter reduction and 3x faster inference on edge devices while maintaining competitive accuracy, enabling practical deployment in resource-constrained scenarios.

</details>

**LLM Summary**

- What: スケルトンベースのアクション認識のための自己教師あり学習において、より包括的な時空間ダイナミクスを学習するための新しいマスキング手法「ASMa」を提案。
- Novelty: 高次関節と低運動フレーム、低次関節と高運動フレームを補完的にマスキングする2つの戦略を組み合わせ、学習済み表現を効果的に整列させるための学習可能な特徴アライメントモジュールを導入。
- Why it matters: 偏りのない、よりバランスの取れたスケルトン表現学習を可能にし、多様な運動パターンに対する汎化性能を向上させる。リソース制約のある環境での展開にも対応。

## 14. Self-Improving World Modelling with Latent Actions
- arXiv: http://arxiv.org/abs/2602.06130v1
- PDF: https://arxiv.org/pdf/2602.06130v1
- Authors: Yifu Qiu, Zheng Zhao, Waylon Li, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Internal modelling of the world -- predicting transitions between previous states $X$ and next states $Y$ under actions $Z$ -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) $P_θ(Y|X,Z)$ and an Inverse Dynamics Modelling (IDM) $Q_φ(Z|X,Y)$. SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.

</details>

**LLM Summary**

- What: アクションラベル付き軌跡を必要とせずに、状態のみのシーケンスから自己改善型のワールドモデルを学習するフレームワーク「SWIRL」を提案。
- Novelty: アクションを潜在変数として扱い、前方ワールドモデリング（FWM）と逆動力学モデリング（IDM）を交互に学習する。変分情報最大化とELBO最大化の2つのフェーズで構成。
- Why it matters: LLMやVLMが、コストのかかるアクションラベルなしで、より効率的に世界のダイナミクスをモデリングし、推論や計画能力を向上させることを可能にする。

## 15. Can vision language models learn intuitive physics from interaction?
- arXiv: http://arxiv.org/abs/2602.06033v1
- PDF: https://arxiv.org/pdf/2602.06033v1
- Authors: Luca M. Schulze Buschoff, Konstantinos Voudouris, Can Demircan, Eric Schulz
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.

</details>

**LLM Summary**

- What: 事前学習済み視覚言語モデルが直感的な物理法則を学習できるか、特に相互作用を通じて学習する能力を検証。
- Novelty: 相互作用を通じて学習するモデルを強化学習で訓練し、タスク内パフォーマンスは向上するものの、新しい文脈への汎化は限定的であることを発見。
- Why it matters: モデルが物理世界を理解するためには、相互作用が重要であるという仮説を検証し、現在のモデルが汎用的な物理的直感を獲得することの難しさを示唆する。

## 16. Pathwise Test-Time Correction for Autoregressive Long Video Generation
- arXiv: http://arxiv.org/abs/2602.05871v1
- PDF: https://arxiv.org/pdf/2602.05871v1
- Authors: Xunzhi Xiang, Zixuan Duan, Guiyu Zhang, Haiyu Zhang, Zhe Gao, Junta Wu, Shaofeng Zhang, Tengfei Wang...
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.

</details>

**LLM Summary**

- What: 長いビデオ生成における誤差蓄積問題を解決するため、テスト時補正（TTC）という訓練不要な手法を提案。初期フレームを基準に、サンプリング経路上の状態を補正する。
- Novelty: 蒸留された自己回帰型拡散モデルのテスト時最適化（TTO）が長いシーケンスで失敗する問題を特定し、それを克服する訓練不要な補正手法を開発した点。
- Why it matters: 長いビデオ生成の品質を向上させ、生成時間を大幅に延長する。既存の蒸留モデルに容易に統合でき、計算コストも低い。
