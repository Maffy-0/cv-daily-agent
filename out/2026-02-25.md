# Daily CV Digest (2026-02-25)

- Total: 20

## 1. QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models
- arXiv: http://arxiv.org/abs/2602.20309v1
- PDF: https://arxiv.org/pdf/2602.20309v1
- Authors: Jingxuan Zhang, Yunta Hsieh, Zhongwei Wang, Haokun Lin, Xin Wang, Ziqi Wang, Yingtie Lei, Mi Zhang
- Keyword score: 4 / hits: vision-language-action, embodied agent

<details><summary>Abstract</summary>

Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.

</details>

**LLM Summary**

- What: Vision-Language-Action (VLA)モデルの計算量とメモリ使用量を削減するための、学習不要な後学習量子化 (PTQ) フレームワーク「QuantVLA」を提案。
- Novelty: VLAシステム向けの初のPTQアプローチであり、拡散トランスフォーマー (DiT) のアクションヘッドの量子化に成功した。選択的量子化レイアウト、アテンション温度マッチング、出力ヘッドバランシングといった3つのスケール調整コンポーネントを導入。
- Why it matters: VLAモデルの実用的な展開における計算リソースの制約を緩和し、より効率的な組み込みエージェントの開発を可能にする。

## 2. NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning
- arXiv: http://arxiv.org/abs/2602.21172v1
- PDF: https://arxiv.org/pdf/2602.21172v1
- Authors: Ishaan Rawal, Shubh Gupta, Yihan Hu, Wei Zhan
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \modelname (\textbf{No} \textbf{R}easoning for \textbf{D}riving). Compared to existing VLAs, \modelname achieves competitive performance while being fine-tuned on $<$60\% of the data and no reasoning annotations, resulting in 3$\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.

</details>

**LLM Summary**

- What: データ効率の良いVision-Language-Action (VLA) モデル「NoRD」を提案。大規模なデータ収集と密な推論アノテーションの必要性を軽減する。
- Novelty: 標準的なGRPOアルゴリズムの難易度バイアス問題を克服するために、LLM向けの難易度バイアス緩和アルゴリズム「Dr. GRPO」を組み込んだ。
- Why it matters: 少ないデータと推論アノテーションで競争力のある自動運転性能を達成し、VLAモデルの開発と展開をより効率的にする。

## 3. HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning
- arXiv: http://arxiv.org/abs/2602.21157v1
- PDF: https://arxiv.org/pdf/2602.21157v1
- Authors: Quanxin Shou, Fangqi Zhu, Shawn Chen, Puxin Yan, Zhengyang Yan, Yikun Miao, Xiaoyi Pang, Zicong Hong...
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models have shown strong performance in robotic manipulation, but often struggle in long-horizon or out-of-distribution scenarios due to the lack of explicit mechanisms for multimodal reasoning and anticipating how the world will evolve under action. Recent works introduce textual chain-of-thought or visual subgoal prediction within VLA models to reason, but still fail to offer a unified human-like reasoning framework for joint textual reasoning, visual foresight, and action prediction. To this end, we propose HALO, a unified VLA model that enables embodied multimodal chain-of-thought (EM-CoT) reasoning through a sequential process of textual task reasoning, visual subgoal prediction for fine-grained guidance, and EM-CoT-augmented action prediction. We instantiate HALO with a Mixture-of-Transformers (MoT) architecture that decouples semantic reasoning, visual foresight, and action prediction into specialized experts while allowing seamless cross-expert collaboration. To enable HALO learning at scale, we introduce an automated pipeline to synthesize EM-CoT training data along with a carefully crafted training recipe. Extensive experiments demonstrate that: (1) HALO achieves superior performance in both simulated and real-world environments, surpassing baseline policy pi_0 by 34.1% on RoboTwin benchmark; (2) all proposed components of the training recipe and EM-CoT design help improve task success rate; and (3) HALO exhibits strong generalization capabilities under aggressive unseen environmental randomization with our proposed EM-CoT reasoning.

</details>

**LLM Summary**

- What: 組み込みマルチモーダル連鎖思考 (EM-CoT) 推論を可能にする統一Vision-Language-Action (VLA) モデル「HALO」を提案。
- Novelty: テキストタスク推論、視覚的サブゴール予測、EM-CoT拡張アクション予測をシーケンシャルに行う統一フレームワーク。Mixture-of-Transformers (MoT) アーキテクチャを採用し、専門家間の協調を促進。
- Why it matters: 長期的なタスクや分布外のシナリオにおけるロボット操作の性能を向上させ、より人間らしい推論能力を持つ組み込みエージェントを実現する。

## 4. ParkDiffusion++: Ego Intention Conditioned Joint Multi-Agent Trajectory Prediction for Automated Parking using Diffusion Models
- arXiv: http://arxiv.org/abs/2602.20923v1
- PDF: https://arxiv.org/pdf/2602.20923v1
- Authors: Jiarong Wei, Anna Rehr, Christian Feist, Abhinav Valada
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

Automated parking is a challenging operational domain for advanced driver assistance systems, requiring robust scene understanding and interaction reasoning. The key challenge is twofold: (i) predict multiple plausible ego intentions according to context and (ii) for each intention, predict the joint responses of surrounding agents, enabling effective what-if decision-making. However, existing methods often fall short, typically treating these interdependent problems in isolation. We propose ParkDiffusion++, which jointly learns a multi-modal ego intention predictor and an ego-conditioned multi-agent joint trajectory predictor for automated parking. Our approach makes several key contributions. First, we introduce an ego intention tokenizer that predicts a small set of discrete endpoint intentions from agent histories and vectorized map polylines. Second, we perform ego-intention-conditioned joint prediction, yielding socially consistent predictions of the surrounding agents for each possible ego intention. Third, we employ a lightweight safety-guided denoiser with different constraints to refine joint scenes during training, thus improving accuracy and safety. Fourth, we propose counterfactual knowledge distillation, where an EMA teacher refined by a frozen safety-guided denoiser provides pseudo-targets that capture how agents react to alternative ego intentions. Extensive evaluations demonstrate that ParkDiffusion++ achieves state-of-the-art performance on the Dragon Lake Parking (DLP) dataset and the Intersections Drone (inD) dataset. Importantly, qualitative what-if visualizations show that other agents react appropriately to different ego intentions.

</details>

**LLM Summary**

- What: 自動駐車のための、エゴの意図を条件付けたマルチエージェント軌道予測モデル「ParkDiffusion++」を提案。
- Novelty: エゴの意図を離散的なエンドポイントとして予測するエゴ意図トークナイザーと、各意図に対応する周囲のエージェントの共同軌道を予測するモデルを統合。安全性を向上させるための軽量な安全ガイド付きデノイザーも導入。
- Why it matters: 自動駐車における複雑なシーン理解と相互作用推論を改善し、より安全で効果的な意思決定を可能にする。

## 5. How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective
- arXiv: http://arxiv.org/abs/2602.20687v1
- PDF: https://arxiv.org/pdf/2602.20687v1
- Authors: Bo Peng, Pi Bu, Keyu Pan, Xinrun Xu, Yinxiu Zhao, Miao Chen, Yang Du, Lin Li...
- Keyword score: 3 / hits: embodied agent

<details><summary>Abstract</summary>

Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.

</details>

**LLM Summary**

- What: VLMベースの組み込みエージェントの評価のための、ネイティブな低レベルアクション空間を用いたベンチマーク「NativeEmbodied」を提案。
- Novelty: 高レベルコマンドや離散化されたアクション空間ではなく、統一されたネイティブな低レベルアクション空間を使用。高レベルタスクと低レベルスキルを同時に評価できる。
- Why it matters: VLMベースの組み込みエージェントの基本的な組み込みスキルにおける欠陥を明らかにし、より詳細な分析と改善を可能にする。

## 6. AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?
- arXiv: http://arxiv.org/abs/2602.20664v1
- PDF: https://arxiv.org/pdf/2602.20664v1
- Authors: Hailong Yan, Shice Liu, Tao Wang, Xiangtao Zhang, Yijie Zhong, Jinwei Chen, Le Zhang, Bo Li
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to "copy-paste" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's "Combination of Straight Ahead and Pose to Pose" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.

</details>

**LLM Summary**

- What: アニメーションのカスタムストーリーボード生成（CSG）のための、画像から動画（I2V）モデルベースのマルチエージェントフレームワーク「AnimeAgent」を提案。
- Novelty: I2Vの持つ暗黙的な動きの事前知識を活用し、一貫性と表現力を向上。主観的・客観的な評価者を組み合わせた反復的な洗練を実現。
- Why it matters: 従来の静的モデルの限界（表現力の欠如、反復修正の困難さ、評価の不確実性）を克服し、高品質で一貫性のあるアニメーションストーリーボード生成を可能にする。

## 7. Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion
- arXiv: http://arxiv.org/abs/2602.20577v1
- PDF: https://arxiv.org/pdf/2602.20577v1
- Authors: Jiaru Zhang, Manav Gagvani, Can Cui, Juntong Peng, Ruqi Zhang, Ziran Wang
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.

</details>

**LLM Summary**

- What: 効率的で説明可能なエンドツーエンド自動運転のための、マスクドビジョン言語アクション拡散モデル「MVLAD-AD」を提案。
- Novelty: 離散的なアクショントークン化戦略により、運動学的に実現可能なウェイポイントのコンパクトなコードブックを構築。幾何学を考慮した埋め込み学習とアクション優先デコーディング戦略を導入。
- Why it matters: LLM/VLMの自動運転への応用における推論遅延、アクション精度、説明可能性の課題を解決し、効率的かつセマンティックに説明可能な自動運転プランニングを実現する。

## 8. Beyond Human Performance: A Vision-Language Multi-Agent Approach for Quality Control in Pharmaceutical Manufacturing
- arXiv: http://arxiv.org/abs/2602.20543v1
- PDF: https://arxiv.org/pdf/2602.20543v1
- Authors: Subhra Jyoti Mandal, Lara Rachidi, Puneet Jain, Matthieu Duvinage, Sander W. Timmer
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

Colony-forming unit (CFU) detection is critical in pharmaceutical manufacturing, serving as a key component of Environmental Monitoring programs and ensuring compliance with stringent quality standards. Manual counting is labor-intensive and error-prone, while deep learning (DL) approaches, though accurate, remain vulnerable to sample quality variations and artifacts. Building on our earlier CNN-based framework (Beznik et al., 2020), we evaluated YOLOv5, YOLOv7, and YOLOv8 for CFU detection; however, these achieved only 97.08 percent accuracy, insufficient for pharmaceutical-grade requirements. A custom Detectron2 model trained on GSK's dataset of over 50,000 Petri dish images achieved 99 percent detection rate with 2 percent false positives and 0.6 percent false negatives. Despite high validation accuracy, Detectron2 performance degrades on outlier cases including contaminated plates, plastic artifacts, or poor optical clarity. To address this, we developed a multi-agent framework combining DL with vision-language models (VLMs). The VLM agent first classifies plates as valid or invalid. For valid samples, both DL and VLM agents independently estimate colony counts. When predictions align within 5 percent, results are automatically recorded in Postgres and SAP; otherwise, samples are routed for expert review. Expert feedback enables continuous retraining and self-improvement. Initial DL-based automation reduced human verification by 50 percent across vaccine manufacturing sites. With VLM integration, this increased to 85 percent, delivering significant operational savings. The proposed system provides a scalable, auditable, and regulation-ready solution for microbiological quality control, advancing automation in biopharmaceutical production.

</details>

**LLM Summary**

- What: 医薬品製造における品質管理のための、ビジョン言語モデル（VLM）と深層学習（DL）を組み合わせたマルチエージェントフレームワークを提案。
- Novelty: VLMエージェントがまずプレートを有効/無効に分類し、有効な場合はDLとVLMが独立してコロニー数を推定。予測が一致しない場合にのみ、より詳細な分析を行う。
- Why it matters: 従来のDL手法では困難だった、汚染されたプレートやアーティファクトなどの外れ値ケースに対する品質管理の精度と信頼性を向上させる。

## 9. UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models
- arXiv: http://arxiv.org/abs/2602.20231v1
- PDF: https://arxiv.org/pdf/2602.20231v1
- Authors: Manish Kumar Govind, Dominick Reilly, Pu Wang, Srijan Das
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Latent action representations learned from unlabeled videos have recently emerged as a promising paradigm for pretraining vision-language-action (VLA) models without explicit robot action supervision. However, latent actions derived solely from RGB observations primarily encode appearance-driven dynamics and lack explicit 3D geometric structure, which is essential for precise and contact-rich manipulation. To address this limitation, we introduce UniLACT, a transformer-based VLA model that incorporates geometric structure through depth-aware latent pretraining, enabling downstream policies to inherit stronger spatial priors. To facilitate this process, we propose UniLARN, a unified latent action learning framework based on inverse and forward dynamics objectives that learns a shared embedding space for RGB and depth while explicitly modeling their cross-modal interactions. This formulation produces modality-specific and unified latent action representations that serve as pseudo-labels for the depth-aware pretraining of UniLACT. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness of depth-aware unified latent action representations. UniLACT consistently outperforms RGB-based latent action baselines under in-domain and out-of-domain pretraining regimes, as well as on both seen and unseen manipulation tasks.

</details>

**LLM Summary**

- What: RGBと深度情報を統合した、深さ認識型のRGB潜在アクション学習フレームワーク「UniLACT」を提案。
- Novelty: 逆・順方向ダイナミクス目的関数に基づいた統一潜在アクション学習フレームワーク「UniLARN」により、RGBと深度の共有埋め込み空間を学習し、クロスモーダル相互作用を明示的にモデル化。
- Why it matters: 潜在アクション表現に3D幾何学的構造を組み込むことで、接触を伴う精密な操作に必要な空間的先験知識を強化し、VLAモデルの性能を向上させる。

## 10. ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking
- arXiv: http://arxiv.org/abs/2602.21161v1
- PDF: https://arxiv.org/pdf/2602.21161v1
- Authors: Guangming Wang, Qizhen Ying, Yixiong Jing, Olaf Wysocki, Brian Sheil
- Keyword score: 2 / hits: vision-language-action, multi-agent

<details><summary>Abstract</summary>

Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the scalability of embodied AI and general-purpose robots. Recent data-driven Vision-Language-Action (VLA) approaches aim to learn policies from large-scale simulation and real-world data. However, the continuous action space of the physical world significantly exceeds the representational capacity of linguistic tokens, making it unclear if scaling data alone can yield general robotic intelligence. To address this gap, we propose ActionReasoning, an LLM-driven framework that performs explicit action reasoning to produce physics-consistent, prior-guided decisions for robotic manipulation. ActionReasoning leverages the physical priors and real-world knowledge already encoded in Large Language Models (LLMs) and structures them within a multi-agent architecture. We instantiate this framework on a tractable case study of brick stacking, where the environment states are assumed to be already accurately measured. The environmental states are then serialized and passed to a multi-agent LLM framework that generates physics-aware action plans. The experiments demonstrate that the proposed multi-agent LLM framework enables stable brick placement while shifting effort from low-level domain-specific coding to high-level tool invocation and prompting, highlighting its potential for broader generalization. This work introduces a promising approach to bridging perception and execution in robotic manipulation by integrating physical reasoning with LLMs.

</details>

**LLM Summary**

- What: LLMを活用したロボットアクション推論フレームワーク「ActionReasoning」を提案し、レンガ積みタスクに適用。
- Novelty: LLMにエンコードされた物理的先験知識と実世界知識をマルチエージェントアーキテクチャ内で構造化し、物理的に整合性の取れた、先験知識に基づいた意思決定を行う。
- Why it matters: 従来のカスタムプランナーの汎化能力の限界を克服し、LLMの推論能力を活用して、より汎用的でスケーラブルなロボットマニピュレーションを実現する。

## 11. Cooperative-Competitive Team Play of Real-World Craft Robots
- arXiv: http://arxiv.org/abs/2602.21119v1
- PDF: https://arxiv.org/pdf/2602.21119v1
- Authors: Rui Zhao, Xihui Li, Yizheng Zhang, Yuzhen Liu, Zhong Zhang, Yufeng Zhang, Cheng Zhou, Zhengyou Zhang...
- Keyword score: 2 / hits: reinforcement learning, multi-agent

<details><summary>Abstract</summary>

Multi-agent deep Reinforcement Learning (RL) has made significant progress in developing intelligent game-playing agents in recent years. However, the efficient training of collective robots using multi-agent RL and the transfer of learned policies to real-world applications remain open research questions. In this work, we first develop a comprehensive robotic system, including simulation, distributed learning framework, and physical robot components. We then propose and evaluate reinforcement learning techniques designed for efficient training of cooperative and competitive policies on this platform. To address the challenges of multi-agent sim-to-real transfer, we introduce Out of Distribution State Initialization (OODSI) to mitigate the impact of the sim-to-real gap. In the experiments, OODSI improves the Sim2Real performance by 20%. We demonstrate the effectiveness of our approach through experiments with a multi-robot car competitive game and a cooperative task in real-world settings.

</details>

**LLM Summary**

- What: 現実世界のロボットチームによる協調・競争プレイのためのマルチエージェント強化学習システムを開発した。シミュレーション、分散学習フレームワーク、物理ロボットコンポーネントを構築し、効率的な学習と実世界への転移を可能にする。特に、シムツーリアルギャップを軽減するために、分布外状態初期化（OODSI）を導入した。
- Novelty: 現実世界のロボットを用いたマルチエージェント強化学習の効率的な学習と、シムツーリアル転移の課題に対処するOODSIの導入。
- Why it matters: ロボットチームの協調・競争行動の学習を効率化し、実世界での応用を可能にする。これにより、より高度なロボットシステム開発に貢献する。

## 12. PyVision-RL: Forging Open Agentic Vision Models via RL
- arXiv: http://arxiv.org/abs/2602.20739v1
- PDF: https://arxiv.org/pdf/2602.20739v1
- Authors: Shitian Zhao, Shaoheng Lin, Ming Li, Haoquan Zhang, Wenshuo Peng, Kaipeng Zhang, Chen Wei
- Keyword score: 2 / hits: video understanding, reinforcement learning

<details><summary>Abstract</summary>

Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.

</details>

**LLM Summary**

- What: エージェント型マルチモーダルモデルの強化学習におけるインタラクション崩壊を防ぎ、ツール使用とマルチターン推論を維持するためのフレームワーク「PyVision-RL」を提案した。オーバーサンプリング・フィルタリング・ランキングのロールアウト戦略と累積ツール報酬を組み合わせ、PyVision-ImageとPyVision-Videoを開発した。
- Novelty: エージェント型マルチモーダルモデルの強化学習におけるインタラクション崩壊を抑制し、持続的なインタラクションを促進する新しいフレームワークと学習戦略。
- Why it matters: マルチモーダルモデルがより効果的にツールを使用し、複雑な推論を行えるようにすることで、AIエージェントの能力を向上させ、スケーラブルなマルチモーダルエージェントの開発を促進する。

## 13. BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model
- arXiv: http://arxiv.org/abs/2602.20566v1
- PDF: https://arxiv.org/pdf/2602.20566v1
- Authors: Haosheng Li, Weixin Mao, Zihan Lan, Hongwei Xiong, Hongan Wang, Chenyang Si, Ziwei Liu, Xiaoming Deng...
- Keyword score: 2 / hits: real-time, vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models have achieved significant breakthroughs by leveraging Large Vision Language Models (VLMs) to jointly interpret instructions and visual inputs. However, the substantial increase in visual tokens, particularly from multi-view inputs, poses serious challenges to real-time robotic manipulation. Existing acceleration techniques for VLMs, such as token pruning, often result in degraded performance when directly applied to VLA models, as they overlook the relationships between different views and fail to account for the dynamic and task-specific characteristics of robotic operation. To address this, we propose BFA++, a dynamic token pruning framework designed specifically for VLA models. BFA++ introduces a hierarchical pruning strategy guided by two-level importance predictors: an intra-view predictor highlights task-relevant regions within each image to suppress spatial noise, while an inter-view predictor identifies critical camera views throughout different manipulation phases to reduce cross-view redundancy. This design enables efficient token selection while preserving essential visual cues, resulting in improved computational efficiency and higher manipulation success rates. Evaluations on the RoboTwin benchmark and real-world robotic tasks demonstrate that BFA++ consistently outperforms existing methods. BFA++ improves the success rate by about 10% on both the π0 and RDT models, achieving speedup of 1.8X and 1.5X, respectively. Our results highlight that context-sensitive and task-aware token pruning serves as a more effective strategy than full visual processing, enabling faster inference and improved manipulation accuracy in real-world robotic systems.

</details>

**LLM Summary**

- What: マルチビュー画像と言語、行動を統合するVLAモデルの計算効率を向上させるため、階層的な「BFA++」トークン削減フレームワークを提案した。ビュー内の重要度予測とビュー間の重要度予測により、タスク関連領域と重要なカメラビューを選択的に保持する。
- Novelty: VLAモデルに特化した、ビュー内・ビュー間の階層的な重要度予測に基づく動的なトークン削減フレームワーク。
- Why it matters: ロボット操作におけるリアルタイム処理の課題を解決し、計算効率と性能を両立させることで、より高速で効率的なロボット操作システムの開発を可能にする。

## 14. Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination
- arXiv: http://arxiv.org/abs/2602.20517v1
- PDF: https://arxiv.org/pdf/2602.20517v1
- Authors: Rakshit Trivedi, Kartik Sharma, David C Parkes
- Keyword score: 2 / hits: imitation learning, behavior cloning

<details><summary>Abstract</summary>

Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.

</details>

**LLM Summary**

- What: 人間の行動多様性と非マルコフ性を捉え、推論時に行動を制御できる「MIMIC」フレームワークを提案した。内的な言語表現（インナースピーチ）を生成し、それを条件に行動を選択する。
- Novelty: 人間の認知プロセスに着想を得た、インナースピーチを生成・利用して行動を模倣・制御する新しいフレームワーク。
- Why it matters: 人間とAIの協調を促進し、AIエージェントが人間の多様な行動を模倣し、文脈に応じて柔軟に行動を調整できるようにする。

## 15. Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques
- arXiv: http://arxiv.org/abs/2602.20342v1
- PDF: https://arxiv.org/pdf/2602.20342v1
- Authors: Christos Maikos, Georgios Angelidis, Georgios Th. Papadopoulos
- Keyword score: 2 / hits: streaming, real-time

<details><summary>Abstract</summary>

In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.

</details>

**LLM Summary**

- What: ドローン映像から高忠実度の3Dシーンをリアルタイムに再構築するエンドツーエンドのパイプラインを提案した。RTMPストリーミング、センサーフュージョン、カメラポーズ推定、3D Gaussian Splatting（3DGS）最適化を統合し、低遅延で継続的なモデル更新を実現する。
- Novelty: ドローン映像からのリアルタイム3D再構築において、3DGSを統合し、低遅延で高忠実度な出力を実現するエンドツーエンドのパイプライン。
- Why it matters: ドローンを用いたリアルタイムの空中知覚アプリケーションや、AR/VR環境での没入型体験を向上させる。NeRFベースのアプローチと比較して、より高速で効率的な3D再構築を可能にする。

## 16. UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics
- arXiv: http://arxiv.org/abs/2602.21137v1
- PDF: https://arxiv.org/pdf/2602.21137v1
- Authors: Joseph Raj Vishal, Nagasiri Poluri, Katha Naik, Rutuja Patil, Kashyap Hegde Kota, Krishna Vinod, Prithvi Jai Ramesh, Mohammad Farhadi...
- Keyword score: 1 / hits: multi-agent

<details><summary>Abstract</summary>

Understanding the complex, multi-agent dynamics of urban traffic remains a fundamental challenge for video language models. This paper introduces Urban Dynamics VideoQA, a benchmark dataset that captures the unscripted real-world behavior of dynamic urban scenes. UDVideoQA is curated from 16 hours of traffic footage recorded at multiple city intersections under diverse traffic, weather, and lighting conditions. It employs an event-driven dynamic blur technique to ensure privacy preservation without compromising scene fidelity. Using a unified annotation pipeline, the dataset contains 28K question-answer pairs generated across 8 hours of densely annotated video, averaging one question per second. Its taxonomy follows a hierarchical reasoning level, spanning basic understanding and attribution to event reasoning, reverse reasoning, and counterfactual inference, enabling systematic evaluation of both visual grounding and causal reasoning. Comprehensive experiments benchmark 10 SOTA VideoLMs on UDVideoQA and 8 models on a complementary video question generation benchmark. Results reveal a persistent perception-reasoning gap, showing models that excel in abstract inference often fail with fundamental visual grounding. While models like Gemini Pro achieve the highest zero-shot accuracy, fine-tuning the smaller Qwen2.5-VL 7B model on UDVideoQA bridges this gap, achieving performance comparable to proprietary systems. In VideoQGen, Gemini 2.5 Pro, and Qwen3 Max generate the most relevant and complex questions, though all models exhibit limited linguistic diversity, underscoring the need for human-centric evaluation. The UDVideoQA suite, including the dataset, annotation tools, and benchmarks for both VideoQA and VideoQGen, provides a foundation for advancing robust, privacy-aware, and real-world multimodal reasoning. UDVideoQA is available at https://ud-videoqa.github.io/UD-VideoQA/UD-VideoQA/.

</details>

**LLM Summary**

- What: 都市の交通状況に関する質問応答データセット「UDVideoQA」を提案。
- Novelty: 実際の都市交通映像から、プライバシー保護された28Kの質問応答ペアを生成。階層的な推論レベル（基本理解から反事実推論まで）を導入。
- Why it matters: マルチオブジェクトの時空間推論能力を評価し、既存のビデオ言語モデルの知覚と推論のギャップを明らかにする。

## 17. From Perception to Action: An Interactive Benchmark for Vision Reasoning
- arXiv: http://arxiv.org/abs/2602.21015v1
- PDF: https://arxiv.org/pdf/2602.21015v1
- Authors: Yuhao Wu, Maojia Song, Yihuai Lan, Lei Wang, Zhiqiang Hu, Yao Xiao, Heng Zhou, Weihua Zheng...
- Keyword score: 1 / hits: embodied agent

<details><summary>Abstract</summary>

Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.

</details>

**LLM Summary**

- What: 物理構造の理解と行動計画能力を評価するインタラクティブなベンチマーク「CHAIN」を提案。
- Novelty: 受動的な知覚評価から、物理的制約に基づいた構造化された行動シーケンスの理解、計画、実行を評価する能動的な問題解決へ移行。
- Why it matters: 物理的構造と因果関係の制約をモデルがどの程度理解できるかを評価し、実世界での応用（例：ロボットエージェント）に不可欠な能力を測定する。

## 18. Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks
- arXiv: http://arxiv.org/abs/2602.21013v1
- PDF: https://arxiv.org/pdf/2602.21013v1
- Authors: Sanjay Haresh, Daniel Dijkman, Apratim Bhattacharyya, Roland Memisevic
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Many dexterous manipulation tasks are non-markovian in nature, yet little attention has been paid to this fact in the recent upsurge of the vision-language-action (VLA) paradigm. Although they are successful in bringing internet-scale semantic understanding to robotics, existing VLAs are primarily "stateless" and struggle with memory-dependent long horizon tasks. In this work, we explore a way to impart both spatial and temporal memory to a VLA by incorporating a language scratchpad. The scratchpad makes it possible to memorize task-specific information, such as object positions, and it allows the model to keep track of a plan and progress towards subgoals within that plan. We evaluate this approach on a split of memory-dependent tasks from the ClevrSkills environment, on MemoryBench, as well as on a challenging real-world pick-and-place task. We show that incorporating a language scratchpad significantly improves generalization on these tasks for both non-recurrent and recurrent models.

</details>

**LLM Summary**

- What: 言語スクラッチパッドを導入し、記憶に依存する操作タスクにおけるビジョン・言語・アクション（VLA）モデルの記憶能力を向上させる手法を提案。
- Novelty: スクラッチパッドにより、オブジェクトの位置などのタスク固有情報を記憶し、計画の進捗を追跡可能にする。
- Why it matters: 非マルコフ性タスクや長期間のタスクにおいて、既存のVLAモデルの汎化性能を大幅に向上させる。

## 19. Recursive Belief Vision Language Model
- arXiv: http://arxiv.org/abs/2602.20659v1
- PDF: https://arxiv.org/pdf/2602.20659v1
- Authors: Vaidehi Bagaria, Bijo Sebastian, Nirav Patel
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.

</details>

**LLM Summary**

- What: 部分観測下での長期間操作タスクに対応する、信念中心のアーキテクチャ「RB-VLA」を提案。
- Novelty: 観測を直接保存せず、タスク関連の履歴、ダイナミクス、オブジェクトインタラクションをエンコードするコンパクトな潜在状態を維持。
- Why it matters: 推論レイテンシを削減し、タスクの進行状況を追跡し、部分観測下でもロバストな制御を実現する。

## 20. Grounding LLMs in Scientific Discovery via Embodied Actions
- arXiv: http://arxiv.org/abs/2602.20639v1
- PDF: https://arxiv.org/pdf/2602.20639v1
- Authors: Bo Zhang, Jinfeng Zhou, Yuxuan Chen, Jianing Yin, Minlie Huang, Hongning Wang
- Keyword score: 1 / hits: embodied agent

<details><summary>Abstract</summary>

Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive "execute-then-response" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.

</details>

**LLM Summary**

- What: 大規模言語モデル（LLM）を物理シミュレーションと連動させ、科学的発見を促進するフレームワーク「EmbodiedAct」を提案。
- Novelty: LLMを「実行・応答」ループから解放し、リアルタイムの知覚と実行ループを統合することで、科学ソフトウェアを能動的なエージェント化。
- Why it matters: 長期間シミュレーションの信頼性と安定性を向上させ、複雑な工学設計や科学モデリングタスクにおける性能を向上させる。
