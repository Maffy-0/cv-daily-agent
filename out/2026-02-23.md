# Daily CV Digest (2026-02-23)

- Total: 11

## 1. MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance
- arXiv: http://arxiv.org/abs/2602.17930v1
- PDF: https://arxiv.org/pdf/2602.17930v1
- Authors: Narjes Nourzad, Carlee Joe-Wong
- Keyword score: 4 / hits: real-time, reinforcement learning

<details><summary>Abstract</summary>

Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/

</details>

**LLM Summary**

- What: LLMのガイダンスを限定的に利用しつつ、構造化されたメモリグラフを用いて強化学習エージェントの初期学習を促進するMIRAを提案。メモリグラフはエージェントの経験とLLMの出力を統合し、ユーティリティ信号を生成してポリシー更新を調整する。
- Novelty: LLMの出力をリアルタイム監視ではなく、永続的なメモリに蓄積し、それを活用して強化学習の効率を高める点。
- Why it matters: スパースな報酬設定や遅延報酬設定における強化学習エージェントのサンプル効率を改善し、LLMへの依存度を下げつつ学習を加速できる。

## 2. Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition
- arXiv: http://arxiv.org/abs/2602.18043v1
- PDF: https://arxiv.org/pdf/2602.18043v1
- Authors: Hongyu Qu, Xiangbo Shu, Rui Yan, Hailiang Gao, Wenguan Wang, Jinhui Tang
- Keyword score: 3 / hits: action recognition

<details><summary>Abstract</summary>

Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>

**LLM Summary**

- What: Few-Shot Action Recognition (FSAR) のために、LLMから抽出した空間的・時間的な知識を分離・統合するDiSTフレームワークを提案。アクション名を詳細な属性記述に分解し、それを用いて多粒度のプロトタイプを学習する。
- Novelty: LLMから得られるアクション名を、空間的・時間的な属性記述に分解し、それらを活用して少数の例からでも効果的にアクションを認識する手法。
- Why it matters: 限られたデータで新しいアクションカテゴリを認識する際の精度を向上させ、より詳細で背景知識に基づいたアクション理解を可能にする。

## 3. DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE
- arXiv: http://arxiv.org/abs/2602.18019v1
- PDF: https://arxiv.org/pdf/2602.18019v1
- Authors: Yujie Jin, Wenxin Zhang, Jingjing Wang, Guodong Zhou
- Keyword score: 3 / hits: video understanding

<details><summary>Abstract</summary>

In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>

**LLM Summary**

- What: 脅威の検出・特定だけでなく、その原因の帰属と評価まで行う「In-depth Security-oriented Video Understanding (DeepSVU)」タスクを提案。物理世界の情報を統合的にモデル化し、要素間のトレードオフを調整するUnified Physical-world Regularized MoE (UPRM) アプローチを開発。
- Novelty: 単なる脅威検出に留まらず、脅威の原因分析まで行う新しいビデオ理解タスクと、それを実現するための物理世界情報を統合したMoEモデル。
- Why it matters: より深いレベルでのセキュリティ関連ビデオ分析を可能にし、事件や事故の原因究明や予防策の検討に貢献する。

## 4. ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models
- arXiv: http://arxiv.org/abs/2602.17951v1
- PDF: https://arxiv.org/pdf/2602.17951v1
- Authors: Guoheng Sun, Tingting Du, Kaixi Feng, Chenxiang Luo, Xingguo Ding, Zheyu Shen, Ziyao Wang, Yexiao He...
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.

</details>

**LLM Summary**

- What: 3D空間認識能力が不足しがちなVision-Language-Action (VLA) モデルに対し、強力な3Dビジョン基盤モデルの知識を多層的にアラインメントするROCKETフレームワークを提案。残差ストリームのアラインメントとして定式化し、共有プロジェクターを使用。
- Novelty: VLAモデルと3Dビジョン基盤モデルの複数層を、残差ストリームとしてアラインメントする新しい手法と、そのための共有プロジェクターの利用。
- Why it matters: ロボット操作における指示追従能力を向上させ、2Dデータのみで学習したモデルの3D空間理解能力を強化する。

## 5. Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models
- arXiv: http://arxiv.org/abs/2602.17869v1
- PDF: https://arxiv.org/pdf/2602.17869v1
- Authors: Yuxiao Chen, Jue Wang, Zhikang Zhang, Jingru Yi, Xu Zhang, Yang Zou, Zhaowei Cai, Jianbo Yuan...
- Keyword score: 3 / hits: video understanding

<details><summary>Abstract</summary>

With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>

**LLM Summary**

- What: 長時間動画理解のために、情報密度に基づいた適応型ビデオサンプラー(AVS)と、オートエンコーダーベースの時空間ビデオコンプレッサー(SVC)を統合した新しいエンドツーエンドのスキーマを提案。
- Novelty: 長時間動画の冗長性を効率的に削減し、重要な情報を保持しながら高い圧縮率を実現する適応型サンプリングと圧縮手法。
- Why it matters: メモリ制約内でより多くのフレームを処理し、大量の入力データから識別性の高い情報を抽出することで、大規模マルチモーダルモデルによる長時間動画理解の効率と精度を向上させる。

## 6. KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding
- arXiv: http://arxiv.org/abs/2602.17768v1
- PDF: https://arxiv.org/pdf/2602.17768v1
- Authors: Boda Lin, Yongjie Zhu, Xiaocheng Gong, Wenyu Qin, Meng Wang
- Keyword score: 3 / hits: video understanding

<details><summary>Abstract</summary>

Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>

**LLM Summary**

- What: 人間の複雑な動きを詳細に分解・記述するための自動アノテーションパイプラインを開発し、それを用いて運動解析モーションベンチマーク（KPM-Bench）という新しいデータセットと評価セットを構築した。
- Novelty: 運動学ベースの計算と言語解析を統合し、手足レベルの動きに焦点を当てた詳細なビデオキャプションと、動きの理解に特化した質問応答ペアを提供する。また、動きの記述における幻覚現象を評価するための専用セットも含む。
- Why it matters: 細粒度な動き中心のビデオ理解のギャップを埋め、ビデオキャプションモデルの幻覚問題を軽減し、より正確で詳細な動きの描写を可能にする。

## 7. How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf
- arXiv: http://arxiv.org/abs/2602.18397v1
- PDF: https://arxiv.org/pdf/2602.18397v1
- Authors: Wenqi Jiang, Jason Clemons, Karu Sankaralingam, Christos Kozyrakis
- Keyword score: 2 / hits: real-time, vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.

</details>

**LLM Summary**

- What: Vision-Language-Action (VLA) モデルの推論パフォーマンスを分析するための分析モデル「VLA-Perf」を開発し、それを用いてVLAモデルと推論システムの組み合わせにおけるパフォーマンスの包括的な研究を行った。
- Novelty: VLAモデルの推論パフォーマンスの広範なランドスケープを体系的に調査し、モデルのスケーリング、アーキテクチャ、長文脈ビデオ入力、非同期推論、デュアルシステムパイプライン、および推論実行場所（デバイス、エッジ、クラウド）の影響を分析した。
- Why it matters: 将来のVLAモデルとシステムをリアルタイム推論に対応させるための設計指針を提供し、ロボット工学におけるVLAモデルの実装と最適化を支援する。

## 8. Learning Optimal and Sample-Efficient Decision Policies with Guarantees
- arXiv: http://arxiv.org/abs/2602.17978v1
- PDF: https://arxiv.org/pdf/2602.17978v1
- Authors: Daqian Shao
- Keyword score: 2 / hits: reinforcement learning, imitation learning

<details><summary>Abstract</summary>

The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.

</details>

**LLM Summary**

- What: 隠れた交絡因子が存在するオフラインデータセットから、保証付きで最適かつサンプル効率の良い決定ポリシーを学習する手法を提案した。
- Novelty: 隠れた交絡因子に対処するために操作変数（IV）を利用し、条件付きモーメント制約（CMR）問題として定式化。ダブル/デバイアス機械学習に着想を得て、収束性と最適性の保証を持つサンプル効率的なアルゴリズムを導出した。
- Why it matters: 医療や金融などの高リスク分野で、オンラインでの相互作用が困難または危険な場合に、オフラインデータから信頼性の高い意思決定ポリシーを学習することを可能にする。

## 9. Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty
- arXiv: http://arxiv.org/abs/2602.18312v1
- PDF: https://arxiv.org/pdf/2602.18312v1
- Authors: Zhaoming Xie, Kevin Karol, Jessica Hodgins
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.

</details>

**LLM Summary**

- What: アクションヤコビアンペナルティを導入し、それを計算するための新しいアーキテクチャ「Linear Policy Net (LPN)」を提案することで、時間変化する線形ポリシーを学習する手法を開発した。
- Novelty: アクションヤコビアンペナルティを自動微分を通じて直接適用し、タスク固有のチューニングなしに非現実的な高周波制御信号を排除する。LPNは計算負荷を大幅に削減し、パラメータチューニング不要で学習収束が速い。
- Why it matters: 人間や物理ロボットが達成できないような不自然な高周波信号を生成するポリシーを回避し、より現実的で実行可能な制御ポリシーを学習することを可能にする。

## 10. SimVLA: A Simple VLA Baseline for Robotic Manipulation
- arXiv: http://arxiv.org/abs/2602.18224v1
- PDF: https://arxiv.org/pdf/2602.18224v1
- Authors: Yuankai Luo, Woping Chen, Tong Liang, Baiqiao Wang, Zhenguo Li
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA

</details>

**LLM Summary**

- What: ロボット操作のためのシンプルで透明性の高いVision-Language-Action (VLA) ベースラインモデル「SimVLA」を提案した。
- Novelty: 知覚と制御を厳密に分離し、標準的なビジョン言語バックボーンと軽量なアクションヘッドを使用し、重要なトレーニングダイナミクスを標準化することで、最小限の設計で最先端のパフォーマンスを達成する。
- Why it matters: VLA研究における明確な参照点を提供し、将来のアーキテクチャ的進歩の貢献を容易に特定できるようにする。また、少ないパラメータ数で高いパフォーマンスを発揮することを示す。

## 11. BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards
- arXiv: http://arxiv.org/abs/2602.18193v1
- PDF: https://arxiv.org/pdf/2602.18193v1
- Authors: Yiran Yang, Zhaowei Liu, Yuan Yuan, Yukun Song, Xiong Ma, Yinghao Song, Xiangji Zeng, Lu Sun...
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>

**LLM Summary**

- What: 短尺動画広告の不適切コンテンツを検出・説明するマルチモーダル広告モデレーションフレームワーク「BLM-Guard」を提案。Chain-of-Thought (CoT) 推論とポリシー整合的な報酬を用いた強化学習でモデルを洗練。
- Novelty: ルール駆動のICoTデータ合成パイプラインによる学習初期化、因果的整合性とポリシー遵守を両立させる報酬設計、マルチモーダルな操作（画像誇張、字幕と音声の不一致など）を捉えるマルチタスクアーキテクチャ。
- Why it matters: 短尺動画プラットフォームにおける広告の信頼性向上、コミュニティ安全フィルターでは対応困難な微妙な不正広告の検出精度向上、アノテーションコスト削減に貢献。
