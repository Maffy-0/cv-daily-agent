# Daily CV Digest (2026-02-16)

- Total: 20

## 1. Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution
- arXiv: http://arxiv.org/abs/2602.12684v1
- PDF: https://arxiv.org/pdf/2602.12684v1
- Authors: Rui Cai, Jun Guo, Xinze He, Piaopiao Jin, Jie Li, Bingxuan Lin, Futeng Liu, Wei Liu...
- Keyword score: 6 / hits: real-time, vision-language-action

<details><summary>Abstract</summary>

In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io

</details>

**LLM Summary**

- What: 高性能でリアルタイム実行可能な、オープンソースのビジョン・言語・アクション（VLA）モデル「Xiaomi-Robotics-0」を提案。大規模データセットでの事前学習と、非同期実行のための後処理技術、タイムステップのアライメントにより、高速でスムーズなリアルタイム実行を実現。
- Novelty: VLAモデルのリアルタイム実行における推論遅延を克服するための学習レシピとデプロイ戦略。
- Why it matters: ロボットの高度な操作やインタラクションをリアルタイムで実現し、実世界での応用を加速させる。

## 2. Artic: AI-oriented Real-time Communication for MLLM Video Assistant
- arXiv: http://arxiv.org/abs/2602.12641v1
- PDF: https://arxiv.org/pdf/2602.12641v1
- Authors: Jiangkai Wu, Zhiyuan Ren, Junquan Zhong, Liming Liu, Xinggong Zhang
- Keyword score: 5 / hits: streaming, real-time, video understanding

<details><summary>Abstract</summary>

AI Video Assistant emerges as a new paradigm for Real-time Communication (RTC), where one peer is a Multimodal Large Language Model (MLLM) deployed in the cloud. This makes interaction between humans and AI more intuitive, akin to chatting with a real person. However, a fundamental mismatch exists between current RTC frameworks and AI Video Assistants, stemming from the drastic shift in Quality of Experience (QoE) and more challenging networks. Measurements on our production prototype also confirm that current RTC fails, causing latency spikes and accuracy drops. To address these challenges, we propose Artic, an AI-oriented RTC framework for MLLM Video Assistants, exploring the shift from "humans watching video" to "AI understanding video." Specifically, Artic proposes: (1) Response Capability-aware Adaptive Bitrate, which utilizes MLLM accuracy saturation to proactively cap bitrate, reserving bandwidth headroom to absorb future fluctuations for latency reduction; (2) Zero-overhead Context-aware Streaming, which allocates limited bitrate to regions most important for the response, maintaining accuracy even under ultra-low bitrates; and (3) Degraded Video Understanding Benchmark, the first benchmark evaluating how RTC-induced video degradation affects MLLM accuracy. Prototype experiments using real-world uplink traces show that compared with existing methods, Artic significantly improves accuracy by 15.12% and reduces latency by 135.31 ms. We will release the benchmark and codes at https://github.com/pku-netvideo/DeViBench.

</details>

**LLM Summary**

- What: マルチモーダル大規模言語モデル（MLLM）を活用したAIビデオアシスタントのための、AI指向のリアルタイム通信（RTC）フレームワーク「Artic」を提案。
- Novelty: 従来のRTCフレームワークとAIビデオアシスタント間のミスマッチを解消するため、応答能力を考慮したアダプティブビットレート制御と、重要領域に帯域幅を割り当てるゼロオーバーヘッドのコンテキストアウェアストリーミングを導入。
- Why it matters: AIビデオアシスタントとのより直感的で高品質なリアルタイムコミュニケーションを実現し、人間とAIのインタラクションを向上させる。

## 3. ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training
- arXiv: http://arxiv.org/abs/2602.12691v1
- PDF: https://arxiv.org/pdf/2602.12691v1
- Authors: Rushuai Yang, Hecheng Wang, Chiming Liu, Xiaohan Yan, Yunlong Wang, Xuan Du, Shuoyu Yue, Yongcheng Liu...
- Keyword score: 4 / hits: vision-language-action, reinforcement learning

<details><summary>Abstract</summary>

We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.

</details>

**LLM Summary**

- What: 実世界でのオンライン強化学習によるビジョン・言語・アクション（VLA）システムの改善を目指し、アクションレベルでのオフポリシー評価フレームワーク「ALOE」を提案。
- Novelty: 従来のオンポリシー評価の限界を克服し、個々のアクションシーケンスを評価するチャンクベースの時間差ブートストラッピングにより、スパースな報酬下での効果的な信用割り当てと安定したポリシー改善を可能にする。
- Why it matters: 実世界でのロボット操作タスクにおいて、より効率的かつ安定的にVLAモデルを学習・改善し、実用性を高める。

## 4. RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models
- arXiv: http://arxiv.org/abs/2602.12628v1
- PDF: https://arxiv.org/pdf/2602.12628v1
- Authors: Liangzhi Shi, Shuaihang Chen, Feng Gao, Yinuo Chen, Kang Chen, Tonghe Zhang, Hongzhi Zhang, Weinan Zhang...
- Keyword score: 4 / hits: vision-language-action, reinforcement learning

<details><summary>Abstract</summary>

Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \underline{\textit{RL}}-based sim-real \underline{\textit{Co}}-training \modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.

</details>

**LLM Summary**

- What: シミュレーションと実世界のデータを活用した、VLAモデルのための強化学習ベースのSim-Real Co-Trainingフレームワーク「RL-Co」を提案。
- Novelty: スーパーバイズドファインチューニング（SFT）に依存せず、インタラクティブなシミュレーションでの強化学習と、実世界データへの補助的な教師あり学習を組み合わせることで、実世界での能力を維持しつつ、シミュレーションの利点を活用する。
- Why it matters: より効率的かつ効果的にVLAモデルを学習させ、実世界でのタスク遂行能力と汎化性能を向上させる。

## 5. LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning
- arXiv: http://arxiv.org/abs/2602.12314v1
- PDF: https://arxiv.org/pdf/2602.12314v1
- Authors: Junwoon Lee, Yulun Tian
- Keyword score: 4 / hits: streaming, real-time

<details><summary>Abstract</summary>

We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time. Specifically, our approach associates each Gaussian primitive with a compact query vector that can be converted into approximate VLM embeddings using an attention mechanism with a learnable dictionary. The dictionary is initialized efficiently from streaming observations and optimized online to adapt to evolving scene semantics under trust-region regularization. To scale to long trajectories and large environments, we further propose an efficient map management strategy based on voxel hashing, where optimization is restricted to an active local map on the GPU, while the global map is stored and indexed on the CPU to maintain bounded GPU memory usage. Experiments on public benchmarks and a large-scale custom dataset demonstrate that LatentAM attains significantly better feature reconstruction fidelity compared to state-of-the-art methods, while achieving near-real-time speed (12-35 FPS) on the evaluated datasets. Our project page is at: https://junwoonlee.github.io/projects/LatentAM

</details>

**LLM Summary**

- What: ストリーミングRGB-D観測から大規模な潜在特徴マップを構築する、リアルタイムの3Dガウシアン近似（3DGS）マッピングフレームワーク「LatentAM」を提案。
- Novelty: モデルに依存せず、事前学習も不要なオンライン辞書学習アプローチにより、異なるVLMとのプラグアンドプレイ統合を可能にする。また、効率的なマップ管理戦略により、大規模環境へのスケーリングを実現。
- Why it matters: ロボットの知覚能力を向上させ、オープンボキャブラリーでの環境理解やナビゲーションを可能にする。

## 6. Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control
- arXiv: http://arxiv.org/abs/2602.13193v1
- PDF: https://arxiv.org/pdf/2602.13193v1
- Authors: William Chen, Jagdeep Singh Bhatia, Catherine Glossop, Nikhil Mathihalli, Ria Doshi, Andy Tang, Danny Driess, Karl Pertsch...
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks. Website: steerable-policies.github.io

</details>

**LLM Summary**

- What: 視覚と言語、行動を連動させる「Steerable Policies」を提案。これは、サブタスク、動作、ピクセル座標など、様々な抽象度を持つ合成コマンドで学習されたVLA（Vision-Language-Action）モデルである。
- Novelty: 高レベルの推論と低レベルの行動制御の間のインターフェースを改善し、VLM（Vision-Language Model）の知識をロボットの行動に効果的に結びつける。
- Why it matters: ロボットのタスク汎化能力を向上させ、より複雑なタスクをこなせるようにする。

## 7. Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models
- arXiv: http://arxiv.org/abs/2602.12734v1
- PDF: https://arxiv.org/pdf/2602.12734v1
- Authors: Nick Heppert, Minh Quang Nguyen, Abhinav Valada
- Keyword score: 3 / hits: imitation learning

<details><summary>Abstract</summary>

Imitation learning is a popular paradigm to teach robots new tasks, but collecting robot demonstrations through teleoperation or kinesthetic teaching is tedious and time-consuming. In contrast, directly demonstrating a task using our human embodiment is much easier and data is available in abundance, yet transfer to the robot can be non-trivial. In this work, we propose Real2Gen to train a manipulation policy from a single human demonstration. Real2Gen extracts required information from the demonstration and transfers it to a simulation environment, where a programmable expert agent can demonstrate the task arbitrarily many times, generating an unlimited amount of data to train a flow matching policy. We evaluate Real2Gen on human demonstrations from three different real-world tasks and compare it to a recent baseline. Real2Gen shows an average increase in the success rate of 26.6% and better generalization of the trained policy due to the abundance and diversity of training data. We further deploy our purely simulation-trained policy zero-shot in the real world. We make the data, code, and trained models publicly available at real2gen.cs.uni-freiburg.de.

</details>

**LLM Summary**

- What: 単一の人間デモンストレーションからロボットの模倣学習を行うための「Real2Gen」を提案。デモンストレーション情報をシミュレーション環境に転送し、生成モデルを用いて大量のデータを生成してポリシーを学習する。
- Novelty: 少ないデモンストレーションデータから、生成モデルを活用して豊富な学習データを生成し、模倣学習の効率と汎化性能を向上させる。
- Why it matters: ロボットへのタスク学習に必要なデモンストレーションの収集コストを大幅に削減し、より多様なタスクを学習可能にする。

## 8. TRANS: Terrain-aware Reinforcement Learning for Agile Navigation of Quadruped Robots under Social Interactions
- arXiv: http://arxiv.org/abs/2602.12724v1
- PDF: https://arxiv.org/pdf/2602.12724v1
- Authors: Wei Zhu, Irfan Tito Kurniawan, Ye Zhao, Mistuhiro Hayashibe
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

This study introduces TRANS: Terrain-aware Reinforcement learning for Agile Navigation under Social interactions, a deep reinforcement learning (DRL) framework for quadrupedal social navigation over unstructured terrains. Conventional quadrupedal navigation typically separates motion planning from locomotion control, neglecting whole-body constraints and terrain awareness. On the other hand, end-to-end methods are more integrated but require high-frequency sensing, which is often noisy and computationally costly. In addition, most existing approaches assume static environments, limiting their use in human-populated settings. To address these limitations, we propose a two-stage training framework with three DRL pipelines. (1) TRANS-Loco employs an asymmetric actor-critic (AC) model for quadrupedal locomotion, enabling traversal of uneven terrains without explicit terrain or contact observations. (2) TRANS-Nav applies a symmetric AC framework for social navigation, directly mapping transformed LiDAR data to ego-agent actions under differential-drive kinematics. (3) A unified pipeline, TRANS, integrates TRANS-Loco and TRANS-Nav, supporting terrain-aware quadrupedal navigation in uneven and socially interactive environments. Comprehensive benchmarks against locomotion and social navigation baselines demonstrate the effectiveness of TRANS. Hardware experiments further confirm its potential for sim-to-real transfer.

</details>

**LLM Summary**

- What: 四足歩行ロボットが不整地を移動しつつ、人間との社会的相互作用を考慮したナビゲーションを行うための深層強化学習フレームワーク「TRANS」を提案。
- Novelty: 地形認識と社会的相互作用を統合し、従来の運動計画と制御の分離や静的環境の仮定を克服する。
- Why it matters: 人間がいる環境でも、不整地をアジリティ高く安全に移動できる四足歩行ロボットの実現に貢献する。

## 9. Flow-Factory: A Unified Framework for Reinforcement Learning in Flow-Matching Models
- arXiv: http://arxiv.org/abs/2602.12529v1
- PDF: https://arxiv.org/pdf/2602.12529v1
- Authors: Bowen Ping, Chengyou Jia, Minnan Luo, Hangwei Qian, Ivor Tsang
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

Reinforcement learning has emerged as a promising paradigm for aligning diffusion and flow-matching models with human preferences, yet practitioners face fragmented codebases, model-specific implementations, and engineering complexity. We introduce Flow-Factory, a unified framework that decouples algorithms, models, and rewards through through a modular, registry-based architecture. This design enables seamless integration of new algorithms and architectures, as demonstrated by our support for GRPO, DiffusionNFT, and AWM across Flux, Qwen-Image, and WAN video models. By minimizing implementation overhead, Flow-Factory empowers researchers to rapidly prototype and scale future innovations with ease. Flow-Factory provides production-ready memory optimization, flexible multi-reward training, and seamless distributed training support. The codebase is available at https://github.com/X-GenGroup/Flow-Factory.

</details>

**LLM Summary**

- What: 強化学習とフローマッチングモデルを統合するための統一フレームワーク「Flow-Factory」を開発。アルゴリズム、モデル、報酬をモジュール化し、容易な拡張性と効率的な学習を可能にする。
- Novelty: 既存の強化学習アルゴリズムや拡散モデル、フローマッチングモデルを統一的なアーキテクチャで扱えるようにし、研究開発の複雑さを軽減する。
- Why it matters: 研究者が新しいアルゴリズムやモデルを迅速にプロトタイピングし、スケーリングすることを容易にし、この分野のイノベーションを加速させる。

## 10. CoPE-VideoLM: Codec Primitives For Efficient Video Language Models
- arXiv: http://arxiv.org/abs/2602.13191v1
- PDF: https://arxiv.org/pdf/2602.13191v1
- Authors: Sayan Deb Sarkar, Rémi Pautrat, Ondrej Miksik, Marc Pollefeys, Iro Armeni, Mahdi Rad, Mihai Dusmanu
- Keyword score: 1 / hits: video understanding

<details><summary>Abstract</summary>

Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.

</details>

**LLM Summary**

- What: ビデオ言語モデル（VideoLM）の効率を向上させるために、ビデオコーデックのプリミティブ（動きベクトルや残差）を活用する「CoPE-VideoLM」を提案。
- Novelty: フルフレームのエンコーディングを避け、コーデックプリミティブを利用することで、計算コストを大幅に削減しつつ、時間的なダイナミクスを捉える。
- Why it matters: VideoLMの処理速度を向上させ、より長いビデオコンテキストを効率的に処理できるようにすることで、ビデオ理解タスクの性能向上に貢献する。

## 11. UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph
- arXiv: http://arxiv.org/abs/2602.13086v1
- PDF: https://arxiv.org/pdf/2602.13086v1
- Authors: Haichao Liu, Yuanjiang Xue, Yuheng Zhou, Haoyuan Deng, Yinan Liang, Lihua Xie, Ziwei Wang
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Achieving general-purpose robotic manipulation requires robots to seamlessly bridge high-level semantic intent with low-level physical interaction in unstructured environments. However, existing approaches falter in zero-shot generalization: end-to-end Vision-Language-Action (VLA) models often lack the precision required for long-horizon tasks, while traditional hierarchical planners suffer from semantic rigidity when facing open-world variations. To address this, we present UniManip, a framework grounded in a Bi-level Agentic Operational Graph (AOG) that unifies semantic reasoning and physical grounding. By coupling a high-level Agentic Layer for task orchestration with a low-level Scene Layer for dynamic state representation, the system continuously aligns abstract planning with geometric constraints, enabling robust zero-shot execution. Unlike static pipelines, UniManip operates as a dynamic agentic loop: it actively instantiates object-centric scene graphs from unstructured perception, parameterizes these representations into collision-free trajectories via a safety-aware local planner, and exploits structured memory to autonomously diagnose and recover from execution failures. Extensive experiments validate the system's robust zero-shot capability on unseen objects and tasks, demonstrating a 22.5% and 25.0% higher success rate compared to state-of-the-art VLA and hierarchical baselines, respectively. Notably, the system enables direct zero-shot transfer from fixed-base setups to mobile manipulation without fine-tuning or reconfiguration. Our open-source project page can be found at https://henryhcliu.github.io/unimanip.

</details>

**LLM Summary**

- What: ロボットによる汎用的な操作を実現するため、高レベルの意図と低レベルの物理的相互作用を統合するフレームワーク「UniManip」を提案。エージェント型オペレーショナルグラフ（AOG）に基づき、タスク調整のための高レベルエージェント層と動的状態表現のための低レベルシーン層を連携させる。
- Novelty: 静的なパイプラインではなく、動的なエージェントループとして機能し、知覚から軌道生成、失敗からの回復までを自律的に行う。
- Why it matters: 未知の環境で、複雑なタスクをゼロショットで実行できる汎用的なロボット操作の実現に貢献する。

## 12. Quantization-Aware Collaborative Inference for Large Embodied AI Models
- arXiv: http://arxiv.org/abs/2602.13052v1
- PDF: https://arxiv.org/pdf/2602.13052v1
- Authors: Zhonghao Lyu, Ming Xiao, Mikael Skoglund, Merouane Debbah, H. Vincent Poor
- Keyword score: 1 / hits: embodied agent

<details><summary>Abstract</summary>

Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.

</details>

**LLM Summary**

- What: リソース制限のある組み込みAIエージェント向けに、大規模AIモデル（LAIM）の量子化を考慮した協調推論（co-inference）手法を提案。
- Novelty: 量子化による推論歪みを近似し、その歪みと量子化ビット幅の関係を特徴づける下限と上限を導出。遅延とエネルギー制約下で歪みを最小化する量子化ビット幅と計算頻度の共同設計問題を定式化。
- Why it matters: 組み込みAIシステムにおけるLAIMの効率的な利用を可能にし、リアルタイム性やエネルギー効率を向上させる。

## 13. Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions
- arXiv: http://arxiv.org/abs/2602.13013v1
- PDF: https://arxiv.org/pdf/2602.13013v1
- Authors: Yunheng Li, Hengrui Zhang, Meng-Hao Guo, Wenzhao Gao, Shaoyong Jia, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng
- Keyword score: 1 / hits: video understanding

<details><summary>Abstract</summary>

Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.

</details>

**LLM Summary**

- What: 汎用的なビデオ多モーダル言語モデル（MLLM）のために、属性構造化され品質検証された指示データセット「ASID-1M」と、そのデータキュレーションパイプライン「ASID-Verify」、およびビデオ理解モデル「ASID-Captioner」を提案。
- Novelty: 100万件の構造化された詳細な視聴覚指示アノテーションを提供し、自動検証と洗練によりアノテーションの品質と一貫性を保証。
- Why it matters: ビデオの細かい視聴覚情報を理解し、より正確で、幻覚が少なく、指示に従うビデオキャプション生成や質問応答を実現する。

## 14. Detecting Object Tracking Failure via Sequential Hypothesis Testing
- arXiv: http://arxiv.org/abs/2602.12983v1
- PDF: https://arxiv.org/pdf/2602.12983v1
- Authors: Alejandro Monroy Muñoz, Rajeev Verma, Alexander Timans
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.

</details>

**LLM Summary**

- What: オブジェクト追跡の失敗を検出するために、逐次仮説検定（sequential hypothesis testing）を提案。
- Novelty: 追跡失敗の証拠を時間とともに蓄積する逐次検定（e-processとして形式化）を導入し、追跡失敗を迅速に特定しつつ、誤ったアラートを所望のレートで制御する。
- Why it matters: 追跡システムの信頼性を formally に保証し、不要な再調整や介入を削減することで、実用的な追跡システムの安全性と効率を向上させる。

## 15. Learning Native Continuation for Action Chunking Flow Policies
- arXiv: http://arxiv.org/abs/2602.12978v1
- PDF: https://arxiv.org/pdf/2602.12978v1
- Authors: Yufeng Liu, Hang Yu, Juntu Zhao, Bocheng Li, Di Zhang, Mingzhu Li, Wenxuan Wu, Yingdong Hu...
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.

</details>

**LLM Summary**

- What: アクションチャンク化されたフローベースのビジョン・言語・アクション（VLA）モデルにおいて、学習時の継続性（native continuation）を確保する手法「Legato」を提案。
- Novelty: 学習時に部分的なアクション情報にモデルを曝露させ、推論時と学習時でデノイジングプロセスの一貫性を保つように学習されたフローダイナミクスを整形する。
- Why it matters: より滑らかな軌道を生成し、タスク完了時間を短縮することで、リアルタイムなロボット操作のパフォーマンスを向上させる。

## 16. SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies
- arXiv: http://arxiv.org/abs/2602.12794v1
- PDF: https://arxiv.org/pdf/2602.12794v1
- Authors: Thies Oelerich, Gerald Ebmer, Christian Hartl-Nesic, Andreas Kugi
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.

</details>

**LLM Summary**

- What: 学習ベースのポリシーとオンライン最適化を組み合わせたロボットマニピュレータの安全な軌道計画手法「SafeFlowMPC」を提案。
- Novelty: フローマッチングとモデル予測制御（MPC）を統合し、学習の汎化能力と最適化の安全保証を両立。
- Why it matters: ロボットが日常環境で安全かつ柔軟に動作するための、リアルタイム実行可能な軌道計画を実現する。

## 17. VineetVC: Adaptive Video Conferencing Under Severe Bandwidth Constraints Using Audio-Driven Talking-Head Reconstruction
- arXiv: http://arxiv.org/abs/2602.12758v1
- PDF: https://arxiv.org/pdf/2602.12758v1
- Authors: Vineet Kumar Rakesh, Soumya Mazumdar, Tapas Samanta, Hemendra Kumar Pandey, Amitabha Das, Sarbajit Pal
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Intense bandwidth depletion within consumer and constrained networks has the potential to undermine the stability of real-time video conferencing: encoder rate management becomes saturated, packet loss escalates, frame rates deteriorate, and end-to-end latency significantly increases. This work delineates an adaptive conferencing system that integrates WebRTC media delivery with a supplementary audio-driven talking-head reconstruction pathway and telemetry-driven mode regulation. The system consists of a WebSocket signaling service, an optional SFU for multi-party transmission, a browser client capable of real-time WebRTC statistics extraction and CSV telemetry export, and an AI REST service that processes a reference face image and recorded audio to produce a synthesized MP4; the browser can substitute its outbound camera track with the synthesized stream with a median bandwidth of 32.80 kbps. The solution incorporates a bandwidth-mode switching strategy and a client-side mode-state logger.

</details>

**LLM Summary**

- What: 帯域幅が極端に制限された状況下でのビデオ会議を可能にする適応型システム「VineetVC」を開発。
- Novelty: WebRTCと音声駆動型トーキングヘッド再構成を組み合わせ、低帯域幅（中央値32.80 kbps）で高品質な映像を提供。
- Why it matters: ネットワーク帯域幅の制約が厳しい環境でも、安定したリアルタイムビデオ会議を実現する。

## 18. Unifying Model-Free Efficiency and Model-Based Representations via Latent Dynamics
- arXiv: http://arxiv.org/abs/2602.12643v1
- PDF: https://arxiv.org/pdf/2602.12643v1
- Authors: Jashaswimalya Acharjee, Balaraman Ravindran
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

We present Unified Latent Dynamics (ULD), a novel reinforcement learning algorithm that unifies the efficiency of model-free methods with the representational strengths of model-based approaches, without incurring planning overhead. By embedding state-action pairs into a latent space in which the true value function is approximately linear, our method supports a single set of hyperparameters across diverse domains -- from continuous control with low-dimensional and pixel inputs to high-dimensional Atari games. We prove that, under mild conditions, the fixed point of our embedding-based temporal-difference updates coincides with that of a corresponding linear model-based value expansion, and we derive explicit error bounds relating embedding fidelity to value approximation quality. In practice, ULD employs synchronized updates of encoder, value, and policy networks, auxiliary losses for short-horizon predictive dynamics, and reward-scale normalization to ensure stable learning under sparse rewards. Evaluated on 80 environments spanning Gym locomotion, DeepMind Control (proprioceptive and visual), and Atari, our approach matches or exceeds the performance of specialized model-free and general model-based baselines -- achieving cross-domain competence with minimal tuning and a fraction of the parameter footprint. These results indicate that value-aligned latent representations alone can deliver the adaptability and sample efficiency traditionally attributed to full model-based planning.

</details>

**LLM Summary**

- What: モデルフリー手法の効率性とモデルベース手法の表現力を統合する新しい強化学習アルゴリズム「Unified Latent Dynamics (ULD)」を提案。
- Novelty: 状態行動ペアを潜在空間に埋め込み、真の価値関数が線形に近くなるようにすることで、計画オーバーヘッドなしに両者の利点を享受。
- Why it matters: 様々なドメイン（連続制御、アタリゲームなど）で、単一のハイパーパラメータセットを用いて高い性能を発揮する汎用的な強化学習手法を提供する。

## 19. CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning
- arXiv: http://arxiv.org/abs/2602.12532v1
- PDF: https://arxiv.org/pdf/2602.12532v1
- Authors: Yike Zhang, Yaonan Wang, Xinxin Sun, Kaizhen Huang, Zhiyuan Xu, Junjie Ji, Zhengping Che, Jian Tang...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control. To address this, we introduce CRAFT, a force-aware curriculum fine-tuning framework that integrates a variational information bottleneck module to regulate vision and language embeddings during early training. This curriculum strategy encourages the model to prioritize force signals initially, before progressively restoring access to the full multimodal information. To enable force-aware learning, we further design a homologous leader-follower teleoperation system that collects synchronized vision, language, and force data across diverse contact-rich tasks. Real-world experiments demonstrate that CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.

</details>

**LLM Summary**

- What: 接触を伴う操作タスクにおいて、Vision-Language-Action (VLA) モデルをフォース情報を活用してファインチューニングするフレームワーク「CRAFT」を提案。
- Novelty: 変分情報ボトルネックモジュールを用いて、初期段階ではフォース信号を優先し、徐々に多モーダル情報へのアクセスを回復させるカリキュラム学習を採用。
- Why it matters: 接触を伴う複雑な操作タスクにおけるVLAモデルの性能を向上させ、未知の物体やタスクバリエーションへの汎化能力を高める。

## 20. On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs
- arXiv: http://arxiv.org/abs/2602.12506v1
- PDF: https://arxiv.org/pdf/2602.12506v1
- Authors: Rosie Zhao, Anshul Shah, Xiaoyu Zhu, Xinke Deng, Zhongyu Jiang, Yang Yang, Joerg Liebelt, Arnab Mondal
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.

</details>

**LLM Summary**

- What: 強化学習（RL）でファインチューニングされたVision-Language Models (VLMs) の頑健性とChain-of-Thought (CoT) の一貫性に関する脆弱性を分析。
- Novelty: テキスト摂動がモデルの頑健性と信頼性に与える影響を定量化し、RLファインチューニングが精度向上とCoT信頼性の低下というトレードオフを生むことを発見。
- Why it matters: RLファインチューニングされたVLMsの信頼性と頑健性を理解し、改善するための洞察を提供する。
