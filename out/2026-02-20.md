# Daily CV Digest (2026-02-20)

- Total: 5

## 1. Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control
- arXiv: http://arxiv.org/abs/2602.17068v1
- PDF: https://arxiv.org/pdf/2602.17068v1
- Authors: Xiaocai Zhang, Neema Nassir, Milad Haghani
- Keyword score: 5 / hits: reinforcement learning, multi-agent, marl

<details><summary>Abstract</summary>

Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance. This paper proposes STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning), a scalable multi-agent deep reinforcement learning framework that follows a centralized training and decentralized execution paradigm. The proposed method captures spatio-temporal dependencies through a novel dual-stage hypergraph attention mechanism that models interactions across both spatial and temporal hyperedges. In addition, a hybrid discrete action space is introduced to jointly determine the next signal phase configuration and its corresponding green duration, enabling more adaptive signal timing decisions. Experiments conducted on a corridor network under five traffic scenarios demonstrate that STDSH-MARL consistently improves multimodal performance and provides clear benefits for public transportation priority. Compared with state-of-the-art baseline methods, the proposed approach achieves superior overall performance. Further ablation studies confirm the contribution of each component of STDSH-MARL, with temporal hyperedges identified as the most influential factor driving the observed performance gains.

</details>

**LLM Summary**

- What: 人間中心のマルチモーダル回廊交通信号制御のための時空間二段ハイパーグラフMARLを提案。
- Novelty: 時空間依存性を捉える二段ハイパーグラフ注意機構と、信号位相と緑時間長を同時に決定するハイブリッド離散アクション空間を導入。
- Why it matters: 公共交通機関の優先度向上を含むマルチモーダル交通のパフォーマンスを向上させ、交通信号制御をより適応的にする。

## 2. When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs
- arXiv: http://arxiv.org/abs/2602.17659v1
- PDF: https://arxiv.org/pdf/2602.17659v1
- Authors: Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.

</details>

**LLM Summary**

- What: Vision-Language-Action (VLA) モデルにおけるカウンターファクチュアルな失敗を評価し、軽減する手法を提案。
- Novelty: カウンターファクチュアルなベンチマーク「LIBERO-CF」を導入し、言語条件付けを明示的に正則化する「Counterfactual Action Guidance (CAG)」を提案。
- Why it matters: VLAモデルが視覚的なショートカットに依存せず、言語指示に忠実に従う能力を向上させ、ロボット制御の信頼性を高める。

## 3. RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward
- arXiv: http://arxiv.org/abs/2602.17558v1
- PDF: https://arxiv.org/pdf/2602.17558v1
- Authors: Qiucheng Wu, Jing Shi, Simon Jenni, Kushal Kafle, Tianyu Wang, Shiyu Chang, Handong Zhao
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.

</details>

**LLM Summary**

- What: 指示ベースの画像レタッチのためのMLLMエージェントフレームワーク「RetouchIQ」を提案。
- Novelty: 従来の固定参照画像との類似度に基づく報酬ではなく、ケースバイケースで評価を行う「汎用報酬モデル」を導入。
- Why it matters: 主観的なクリエイティブ編集の評価を可能にし、高レベルな美的目標と精密なパラメータ制御を橋渡しすることで、より直感的で効果的な画像編集を実現する。

## 4. IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control
- arXiv: http://arxiv.org/abs/2602.17537v1
- PDF: https://arxiv.org/pdf/2602.17537v1
- Authors: Qilong Cheng, Matthew Mackay, Ali Bereyhi
- Keyword score: 1 / hits: imitation learning

<details><summary>Abstract</summary>

Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.

</details>

**LLM Summary**

- What: 自律的な学習駆動型シネマティックモーション制御のためのタスク特化型6自由度ロボットアーム「IRIS」を提案。
- Novelty: 軽量な3Dプリントハードウェアと、ACT（Action Chunking with Transformers）に基づく目標条件付き視覚運動模倣学習フレームワークを統合。
- Why it matters: 低コストで高精度なロボットカメラシステムを提供し、人間によるデモンストレーションから直接、オブジェクト認識可能で滑らかなカメラ軌道を学習することで、映画制作におけるロボットアームの導入を容易にする。

## 5. Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot
- arXiv: http://arxiv.org/abs/2602.16758v1
- PDF: https://arxiv.org/pdf/2602.16758v1
- Authors: Sina Akhbari, Mehran Mahboubkhah
- Keyword score: 1 / hits: multi-agent

<details><summary>Abstract</summary>

This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.

</details>

**LLM Summary**

- What: 4自由度並列運動型フライス加工ロボットのための滑らかな軌道生成とハイブリッドB-スプライン-クォータニオンベースのツールパス補間手法を提案。
- Novelty: B-スプラインとクォータニオン補間を統合し、位置と姿勢の非線形関係を滑らかなベジェ曲線で記述。最小ジャーク軌道最適化を二段階で実施。
- Why it matters: ジンバルロックを回避し、滑らかで連続的な回転を可能にするクォータニオンを使用し、高精度で効率的なツールパス補間を実現することで、ロボットの加工精度と性能を向上させる。
