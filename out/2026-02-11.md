# Daily CV Digest (2026-02-11)

- Total: 20

## 1. Squeezing More from the Stream : Learning Representation Online for Streaming Reinforcement Learning
- arXiv: http://arxiv.org/abs/2602.09396v1
- PDF: https://arxiv.org/pdf/2602.09396v1
- Authors: Nilaksh, Antoine Clavaud, Mathieu Reymond, François Rivest, Sarath Chandar
- Keyword score: 6 / hits: streaming, reinforcement learning

<details><summary>Abstract</summary>

In streaming Reinforcement Learning (RL), transitions are observed and discarded immediately after a single update. While this minimizes resource usage for on-device applications, it makes agents notoriously sample-inefficient, since value-based losses alone struggle to extract meaningful representations from transient data. We propose extending Self-Predictive Representations (SPR) to the streaming pipeline to maximize the utility of every observed frame. However, due to the highly correlated samples induced by the streaming regime, naively applying this auxiliary loss results in training instabilities. Thus, we introduce orthogonal gradient updates relative to the momentum target and resolve gradient conflicts arising from streaming-specific optimizers. Validated across the Atari, MinAtar, and Octax suites, our approach systematically outperforms existing streaming baselines. Latent-space analysis, including t-SNE visualizations and effective-rank measurements, confirms that our method learns significantly richer representations, bridging the performance gap caused by the absence of a replay buffer, while remaining efficient enough to train on just a few CPU cores.

</details>

**LLM Summary**

- What: ストリーミング強化学習において、各遷移からより多くの情報を抽出するために、自己予測表現（SPR）を拡張し、訓練の安定性を向上させる手法を提案した。
- Novelty: ストリーミング環境におけるSPRの適用と、それに伴う訓練不安定性を解決するための勾配更新手法の導入。
- Why it matters: リプレイバッファなしで、効率的にリッチな表現を学習し、ストリーミング強化学習のサンプル効率を大幅に改善する。

## 2. BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation
- arXiv: http://arxiv.org/abs/2602.09849v1
- PDF: https://arxiv.org/pdf/2602.09849v1
- Authors: Yucheng Hu, Jianke Zhang, Yuanfei Luo, Yanjiang Guo, Xiaoyu Chen, Xinshu Sun, Kun Feng, Qingzhou Lu...
- Keyword score: 4 / hits: vision-language-action, embodied agent

<details><summary>Abstract</summary>

Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.

</details>

**LLM Summary**

- What: 長期的な操作タスクにおいて、言語計画、視覚予測、行動生成を単一のフレームワークで統合したBagelVLAモデルを提案した。
- Novelty: 言語計画と視覚予測を行動生成ループに直接組み込み、Residual Flow Guidance (RFG) を用いて低遅延でモダリティを連携させる。
- Why it matters: 複雑で長期的な操作タスクにおいて、より高度な推論と正確な行動生成を可能にし、エージェントの汎用性を向上させる。

## 3. ST4VLA: Spatially Guided Training for Vision-Language-Action Models
- arXiv: http://arxiv.org/abs/2602.10109v1
- PDF: https://arxiv.org/pdf/2602.10109v1
- Authors: Jinhui Ye, Fangjing Wang, Ning Gao, Junqiu Yu, Yangkun Zhu, Bin Wang, Jinyu Zhang, Weiyang Jin...
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/

</details>

**LLM Summary**

- What: 大規模言語モデルを組み込んだ視覚・言語・行動（VLA）モデルにおいて、空間的ガイダンスを用いた訓練（ST4VLA）を提案した。
- Novelty: 空間的グラウンディング事前学習と、空間的プロンプトによる行動生成をガイドする空間的ガイダンス行動後学習の二段階アプローチ。
- Why it matters: VLAモデルの行動学習を空間的知識と整合させ、ロボット操作タスクにおける性能、汎化能力、頑健性を大幅に向上させる。

## 4. VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model
- arXiv: http://arxiv.org/abs/2602.10098v1
- PDF: https://arxiv.org/pdf/2602.10098v1
- Authors: Jingwen Sun, Wenyao Zhang, Zekun Qi, Shaojie Ren, Zezhi Liu, Hanxin Zhu, Guangzhong Sun, Xin Jin...
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.

</details>

**LLM Summary**

- What: 視覚・言語・行動（VLA）モデルの事前学習において、漏洩のない状態予測（leakage-free state prediction）を特徴とするJEPA（Joint Embedding Predictive Architecture）ベースのフレームワークVLA-JEPAを提案した。
- Novelty: ピクセル空間ではなく潜在空間で予測を行い、カメラの動きや背景の変化に頑健なダイナミクス抽象化を学習する。
- Why it matters: インターネットスケールのビデオデータから、より汎用的で頑健なVLAモデルを効率的に学習させ、様々な操作タスクで性能を向上させる。

## 5. Latent Poincaré Shaping for Agentic Reinforcement Learning
- arXiv: http://arxiv.org/abs/2602.09375v1
- PDF: https://arxiv.org/pdf/2602.09375v1
- Authors: Hanchen Xia, Baoyou Chen, Zelin Zang, Yutang Ge, Guojiang Zhao, Siyu Zhu
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

We propose LaPha, a method for training AlphaZero-like LLM agents in a Poincaré latent space. Under LaPha, the search process can be visualized as a tree rooted at the prompt and growing outward from the origin toward the boundary of the Poincaré ball, where negative curvature provides exponentially increasing capacity with radius. Using hyperbolic geodesic distance to rule-verified correctness, we define a node potential and assign dense process rewards by potential differences. We further attach a lightweight value head on the same shared latent space, enabling self-guided test-time scaling with almost no additional overhead. On MATH-500, LaPha improves Qwen2.5-Math-1.5B from 66.0% to 88.2%. With value-head-guided search, LaPha-1.5B reaches 56.7% accuracy on AIME'24, and LaPha-7B further achieves 60.0% on AIME'24 and 53.3% on AIME'25.

</details>

**LLM Summary**

- What: AlphaZeroライクなLLMエージェントをポアンカレ潜在空間で訓練するLaPha手法を提案した。
- Novelty: ポアンカレボールの幾何学的性質を利用した探索プロセスと、ポテンシャル差に基づく報酬設計。
- Why it matters: 数学的な推論タスクにおいて、LLMエージェントの性能を大幅に向上させ、特に複雑な問題に対する解法生成能力を高める。

## 6. SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning
- arXiv: http://arxiv.org/abs/2602.09463v1
- PDF: https://arxiv.org/pdf/2602.09463v1
- Authors: Furong Jia, Ling Dai, Wenjin Deng, Fan Zhang, Chen Hu, Daxin Jiang, Yu Liu
- Keyword score: 2 / hits: reinforcement learning, multi-agent

<details><summary>Abstract</summary>

Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence. To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram. We introduce a 3-stage post-training pipeline starting with a Supervised Fine-Tuning (SFT) stage for basic alignment, followed by an Agentic Cold Start phase utilizing high-quality trajectories synthesized via a Multi-Agent framework, aiming to instill tool-calling expertise. Subsequently, the model's reasoning capabilities are refined through Reinforcement Learning. We propose a Spatially-Aware Dynamic Filtering strategy to enhance the efficiency of the RL stage by prioritizing learnable samples based on spatial difficulty. Extensive experiments on standard benchmarks demonstrate that SpotAgent achieves state-of-the-art performance, effectively mitigating hallucinations while delivering precise and verifiable geo-localization.

</details>

**LLM Summary**

- What: 地理位置特定（Geo-localization）のためのエージェントベースの推論フレームワーク「SpotAgent」を提案。外部ツール（Web検索、マップなど）を活用し、視覚情報と検証プロセスを統合する。
- Novelty: 視覚情報と外部ツールの連携による検証プロセスをエージェント的な推論として形式化し、地理位置特定タスクに適用した点。
- Why it matters: 曖昧で情報が少ない実世界の地理位置特定タスクにおいて、より信頼性の高い、検証可能な結果を提供できるようになる。

## 7. CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments
- arXiv: http://arxiv.org/abs/2602.09367v1
- PDF: https://arxiv.org/pdf/2602.09367v1
- Authors: Jinghan Yang, Jingyi Hou, Xinbo Yu, Wei He, Yifan Wu
- Keyword score: 2 / hits: vision-language-action, reinforcement learning

<details><summary>Abstract</summary>

Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.

</details>

**LLM Summary**

- What: ロボット科学実験のための制約付き手続き的推論フレームワーク「CAPER」を提案。タスクレベルの推論、マルチモーダルな中レベルのグラウンディング、低レベルの制御を分離し、手続き的な正しさを保証する。
- Novelty: 科学実験のような手続きが重要なタスクにおいて、推論と学習の責任を分離し、明示的な制約を導入することで、実行時の論理違反を防ぐアプローチ。
- Why it matters: ロボットが科学実験をより正確かつ信頼性高く実行できるようになり、限られたデモンストレーションでもロバストな性能を発揮できる。

## 8. SAGE: Scalable Agentic 3D Scene Generation for Embodied AI
- arXiv: http://arxiv.org/abs/2602.10116v1
- PDF: https://arxiv.org/pdf/2602.10116v1
- Authors: Hongchi Xia, Xuan Li, Zhaoshuo Li, Qianli Ma, Jiashu Xu, Ming-Yu Liu, Yin Cui, Tsung-Yi Lin...
- Keyword score: 1 / hits: embodied agent

<details><summary>Abstract</summary>

Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.

</details>

**LLM Summary**

- What: 具現化AI（Embodied AI）のためのスケーラブルな3Dシーン生成フレームワーク「SAGE」を提案。ユーザーの意図に基づいて、シミュレーション可能な3D環境を自動生成する。
- Novelty: エージェントが複数のジェネレーターとクリティックを組み合わせ、意味論的な妥当性、視覚的なリアルさ、物理的な安定性を評価しながら、自己修正的に3Dシーンを生成する点。
- Why it matters: 具現化AIの学習に必要な、スケーラブルでリアル、かつ物理的に妥当な3D環境を効率的に生成できるようになり、ポリシーの学習と汎化性能を向上させる。

## 9. EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration
- arXiv: http://arxiv.org/abs/2602.10106v1
- PDF: https://arxiv.org/pdf/2602.10106v1
- Authors: Modi Shi, Shijia Peng, Jin Chen, Haoran Jiang, Yinghui Li, Di Huang, Ping Luo, Hongyang Li...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.

</details>

**LLM Summary**

- What: ヒューマノイドロボットのロコ・マニピュレーション（移動と操作）を、ロボットを使わない人間のエゴセントリック（自己中心）なデモンストレーションで学習させるフレームワーク「EgoHumanoid」を提案。
- Novelty: 人間のエゴセントリックなデモンストレーションと少量のロボットデータを組み合わせて、視覚・言語・行動（VLA）ポリシーを共同学習させることで、人間とロボットの身体的・視点的な差異を埋めるアプローチ。
- Why it matters: ヒューマノイドロボットが、多様な実世界環境でロコ・マニピュレーションを実行できるようになり、データ収集のコストと安全性を改善する。

## 10. UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking
- arXiv: http://arxiv.org/abs/2602.10093v1
- PDF: https://arxiv.org/pdf/2602.10093v1
- Authors: Baijun Chen, Weijie Wan, Tianxing Chen, Xianda Guo, Congsheng Xu, Yuanyang Qi, Haojie Zhang, Longyan Wu...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.

</details>

**LLM Summary**

- What: 視覚・触覚（Visuo-Tactile）マニピュレーションのデータ生成、学習、ベンチマークのための統合シミュレーションプラットフォーム「UniVTAC」を提案。
- Novelty: 複数の視覚・触覚センサーに対応し、接触を伴うインタラクションのデータをスケーラブルに生成できるシミュレーションプラットフォームと、それを用いて学習されたエンコーダー、およびベンチマークタスクを提供。
- Why it matters: 物理世界での触覚データの収集コストと課題を克服し、視覚・触覚マニピュレーションポリシーの学習と評価を促進する。

## 11. CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs
- arXiv: http://arxiv.org/abs/2602.10085v1
- PDF: https://arxiv.org/pdf/2602.10085v1
- Authors: Richard Bornemann, Pierluigi Vito Amadori, Antoine Cully
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\href{https://sites.google.com/view/code-sharp/homepage}{here}$.

</details>

**LLM Summary**

- What: CODE-SHARPという、階層的な報酬プログラムとしてスキルアーカイブを拡張・洗練するフレームワークを提案した。
- Novelty: Foundation Model (FM) を活用し、コード化された実行可能な報酬関数の有向グラフとしてスキルアーカイブを継続的に進化させる。
- Why it matters: 人工知能における、未知のスキルを継続的に発見・学習する能力の向上に貢献する。

## 12. Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection
- arXiv: http://arxiv.org/abs/2602.10042v1
- PDF: https://arxiv.org/pdf/2602.10042v1
- Authors: Changjiang Jiang, Xinkuan Sha, Fengchang Yu, Jingjing Liu, Jian Liu, Mingqi Fang, Chenfeng Zhang, Wei Lu
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.

</details>

**LLM Summary**

- What: 合成画像検出のために、推論の必要性を適応的に判断するハイブリッド推論モデル「Fake-HR1」を提案した。
- Novelty: 生成検出タスクの特性に基づき、推論が必要かどうかを適応的に決定する。
- Why it matters: 推論の冗長性を排除し、検出性能と応答効率を向上させる。

## 13. A Collision-Free Sway Damping Model Predictive Controller for Safe and Reactive Forestry Crane Navigation
- arXiv: http://arxiv.org/abs/2602.10035v1
- PDF: https://arxiv.org/pdf/2602.10035v1
- Authors: Marc-Philip Ecker, Christoph Fröhlich, Johannes Huemer, David Gruber, Bernhard Bischof, Tobias Glück, Wolfgang Kemmetmüller
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Forestry cranes operate in dynamic, unstructured outdoor environments where simultaneous collision avoidance and payload sway control are critical for safe navigation. Existing approaches address these challenges separately, either focusing on sway damping with predefined collision-free paths or performing collision avoidance only at the global planning level. We present the first collision-free, sway-damping model predictive controller (MPC) for a forestry crane that unifies both objectives in a single control framework. Our approach integrates LiDAR-based environment mapping directly into the MPC using online Euclidean distance fields (EDF), enabling real-time environmental adaptation. The controller simultaneously enforces collision constraints while damping payload sway, allowing it to (i) replan upon quasi-static environmental changes, (ii) maintain collision-free operation under disturbances, and (iii) provide safe stopping when no bypass exists. Experimental validation on a real forestry crane demonstrates effective sway damping and successful obstacle avoidance. A video can be found at https://youtu.be/tEXDoeLLTxA.

</details>

**LLM Summary**

- What: 林業用クレーンの安全で反応性の高いナビゲーションのために、衝突回避とペイロードのスウェイ（揺れ）制御を統合したモデル予測制御器を開発した。
- Novelty: LiDARベースのマッピングをMPCに統合し、リアルタイムで環境に適応しながら衝突制約とスウェイダンピングを同時に実行する。
- Why it matters: 林業用クレーンの安全な操作と効率的な作業を可能にする。

## 14. RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments
- arXiv: http://arxiv.org/abs/2602.10015v1
- PDF: https://arxiv.org/pdf/2602.10015v1
- Authors: Dharmendra Sharma, Archit Sharma, John Reberio, Vaibhav Kesharwani, Peeyush Thakur, Narendra Kumar Dhar, Laxmidhar Behera
- Keyword score: 1 / hits: video understanding

<details><summary>Abstract</summary>

Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.

</details>

**LLM Summary**

- What: 人間からロボットへのスキル転移のための、長尺動画における時間的なサブタスクセグメンテーションフレームワーク「RoboSubtaskNet」を提案した。
- Novelty: 注意機構付きI3D特徴量と改良型MS-TCNを組み合わせ、ロボット実行可能なサブタスクラベルを抽出する。
- Why it matters: ロボットとの安全な協調作業を可能にし、ヘルスケアや産業分野でのロボットスキル転移を促進する。

## 15. RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation
- arXiv: http://arxiv.org/abs/2602.09973v1
- PDF: https://arxiv.org/pdf/2602.09973v1
- Authors: Hao Li, Ziqin Wang, Zi-han Ding, Shuai Yang, Yilun Chen, Yang Tian, Xiaolin Hu, Tai Wang...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.

</details>

**LLM Summary**

- What: ロボット操作のための、中間表現（Intermediate Representation）の包括的なスイート「RoboInter」を提案した。
- Novelty: データ、ベンチマーク、モデルを含む統合リソースであり、半自動アノテーションツールと大規模データセットを提供する。
- Why it matters: VLAモデルの汎化能力を向上させ、ロボット操作におけるデータ収集とアノテーションのコストを削減する。

## 16. Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation
- arXiv: http://arxiv.org/abs/2602.09940v1
- PDF: https://arxiv.org/pdf/2602.09940v1
- Authors: Archit Sharma, Dharmendra Sharma, John Rebeiro, Peeyush Thakur, Narendra Dhar, Laxmidhar Behera
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Robots often struggle to follow free-form human instructions in real-world settings due to computational and sensing limitations. We address this gap with a lightweight, fully on-device pipeline that converts natural-language commands into reliable manipulation. Our approach has two stages: (i) the instruction to actions module (Instruct2Act), a compact BiLSTM with a multi-head-attention autoencoder that parses an instruction into an ordered sequence of atomic actions (e.g., reach, grasp, move, place); and (ii) the robot action network (RAN), which uses the dynamic adaptive trajectory radial network (DATRN) together with a vision-based environment analyzer (YOLOv8) to generate precise control trajectories for each sub-action. The entire system runs on a modest system with no cloud services. On our custom proprietary dataset, Instruct2Act attains 91.5% sub-actions prediction accuracy while retaining a small footprint. Real-robot evaluations across four tasks (pick-place, pick-pour, wipe, and pick-give) yield an overall 90% success; sub-action inference completes in < 3.8s, with end-to-end executions in 30-60s depending on task complexity. These results demonstrate that fine-grained instruction-to-action parsing, coupled with DATRN-based trajectory generation and vision-guided grounding, provides a practical path to deterministic, real-time manipulation in resource-constrained, single-camera settings.

</details>

**LLM Summary**

- What: 人間の指示をロボットの操作コマンドに変換する軽量なオンデバイスパイプライン「Instruct2Act」を提案。指示を原子的な行動シーケンスに分解し、ロボットアクションネットワーク（RAN）が制御軌道を生成する。
- Novelty: 自然言語指示からロボットの操作シーケンスと実行までを、クラウドサービスなしで軽量かつオンデバイスで実現。
- Why it matters: ロボットが複雑な指示を理解し、より柔軟で実用的な操作を実行できるようになる。

## 17. Safeguarding Privacy: Privacy-Preserving Detection of Mind Wandering and Disengagement Using Federated Learning in Online Education
- arXiv: http://arxiv.org/abs/2602.09904v1
- PDF: https://arxiv.org/pdf/2602.09904v1
- Authors: Anna Bodonhelyi, Mengdi Wang, Efe Bozkir, Babette Bühler, Enkelejda Kasneci
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Since the COVID-19 pandemic, online courses have expanded access to education, yet the absence of direct instructor support challenges learners' ability to self-regulate attention and engagement. Mind wandering and disengagement can be detrimental to learning outcomes, making their automated detection via video-based indicators a promising approach for real-time learner support. However, machine learning-based approaches often require sharing sensitive data, raising privacy concerns. Federated learning offers a privacy-preserving alternative by enabling decentralized model training while also distributing computational load. We propose a framework exploiting cross-device federated learning to address different manifestations of behavioral and cognitive disengagement during remote learning, specifically behavioral disengagement, mind wandering, and boredom. We fit video-based cognitive disengagement detection models using facial expressions and gaze features. By adopting federated learning, we safeguard users' data privacy through privacy-by-design and introduce a novel solution with the potential for real-time learner support. We further address challenges posed by eyeglasses by incorporating related features, enhancing overall model performance. To validate the performance of our approach, we conduct extensive experiments on five datasets and benchmark multiple federated learning algorithms. Our results show great promise for privacy-preserving educational technologies promoting learner engagement.

</details>

**LLM Summary**

- What: オンライン教育における学習者のマインドワンダリングやエンゲージメント低下を検出するための、プライバシーを保護する連合学習フレームワークを提案。
- Novelty: 顔表情と視線特徴を用いた行動的・認知的エンゲージメント低下の検出モデルを、クロスデバイス連合学習でプライバシーを保護しつつ構築。
- Why it matters: 学習者の注意散漫をリアルタイムで検出し、個別サポートを提供することで、オンライン学習の効果を高める。

## 18. TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback
- arXiv: http://arxiv.org/abs/2602.09888v1
- PDF: https://arxiv.org/pdf/2602.09888v1
- Authors: Zihao Li, Yanan Zhou, Ranpeng Qiu, Hangyu Wu, Guoqiang Ren, Weiming Zhi
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.

</details>

**LLM Summary**

- What: モバイルマニピュレータの全身協調型テレオペレーションシステム「TriPilot-FF」を提案。足踏みペダルによるベース制御と、腕のフォースフィードバックを統合。
- Novelty: 低コストLiDARを用いたペダルへの抵抗フィードバックにより、衝突回避行動を促す。腕のフォースフィードバックと視覚的ガイダンスも提供。
- Why it matters: 複雑なモバイルマニピュレータの操作性を向上させ、より安全かつ効率的な遠隔操作を可能にする。

## 19. Code2World: A GUI World Model via Renderable Code Generation
- arXiv: http://arxiv.org/abs/2602.09856v1
- PDF: https://arxiv.org/pdf/2602.09856v1
- Authors: Yuhao Zheng, Li'an Zhong, Yi Wang, Rui Dai, Kaikui Liu, Xiangxiang Chu, Linyuan Lv, Philip Torr...
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.

</details>

**LLM Summary**

- What: GUIの次の状態をレンダリング可能なコード生成によってシミュレートする「Code2World」を提案。視覚的忠実度と構造的制御可能性を両立。
- Novelty: GUI軌跡をHTMLに変換し、視覚フィードバックで洗練させたデータセット「AndroidCode」を構築。レンダリング結果を報酬とする強化学習でモデルを訓練。
- Why it matters: 自律GUIエージェントが、より人間らしい先読み能力を持ち、複雑なGUI環境でのインタラクションを改善する。

## 20. SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing
- arXiv: http://arxiv.org/abs/2602.09809v1
- PDF: https://arxiv.org/pdf/2602.09809v1
- Authors: Tong Zhang, Honglin Lin, Zhou Liu, Chong Chen, Wentao Zhang
- Keyword score: 1 / hits: multi-agent

<details><summary>Abstract</summary>

Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.

</details>

**LLM Summary**

- What: 科学図の構造的正確性を評価するためのベンチマーク「SciFlow-Bench」を提案。逆パースにより、ピクセルレベルの出力から構造グラフを抽出し評価。
- Novelty: 画像中心や主観的評価ではなく、生成された図を逆パースして構造グラフと比較することで、構造的正確性を直接評価。
- Why it matters: テキストから画像へのモデルによる科学図生成の精度を、構造的な観点から客観的に評価し、改善を促進する。
