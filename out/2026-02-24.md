# Daily CV Digest (2026-02-24)

- Total: 14

## 1. MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems
- arXiv: http://arxiv.org/abs/2602.19843v1
- PDF: https://arxiv.org/pdf/2602.19843v1
- Authors: Jin Jia, Zhiling Deng, Zhuangbin Chen, Yingqi Wang, Zibin Zheng
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems.

</details>

**LLM Summary**

- What: LLMベースのマルチエージェントシステム（MAS）の信頼性評価のための、フォールトインジェクションフレームワーク「MAS-FIRE」を提案。15種類のフォールトタイプを定義し、プロンプト改変、応答書き換え、メッセージルーティング操作の3つのメカニズムで注入。
- Novelty: MASにおける意味論的な失敗（幻覚、指示誤解、推論ドリフト）に焦点を当て、その発生メカニズムと回復能力を評価する体系的なアプローチ。フォールト耐性挙動を4つの階層（メカニズム、ルール、プロンプト、推論）に分類。
- Why it matters: 複雑なタスクで利用されるMASの信頼性を向上させ、潜在的な問題を早期に発見・診断するための基盤を提供する。

## 2. Universal Pose Pretraining for Generalizable Vision-Language-Action Policies
- arXiv: http://arxiv.org/abs/2602.19710v1
- PDF: https://arxiv.org/pdf/2602.19710v1
- Authors: Haitao Lin, Hanyang Yu, Jingshun Huang, He Zhang, Yonggen Ling, Ping Tan, Xiangyang Xue, Yanwei Fu
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns. To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision. Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.

</details>

**LLM Summary**

- What: 汎用的なVision-Language-Action (VLA) ポリシーのための、ポーズを基盤としたユニバーサルな事前学習手法「Pose-VLA」を提案。VLA学習を、3D空間情報を抽出する事前学習フェーズと、ロボット固有の行動空間への効率的なアライメントを行う事後学習フェーズに分離。
- Novelty: 離散的なポーズトークンをユニバーサルな表現として導入し、多様な3Dデータセットからの空間的グラウンディングとロボットデモンストレーションからの幾何学的軌道を統合。
- Why it matters: 特徴の崩壊や学習効率の低さを解消し、ロボットの行動認識と制御の汎用性を向上させる。

## 3. TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics
- arXiv: http://arxiv.org/abs/2602.19313v1
- PDF: https://arxiv.org/pdf/2602.19313v1
- Authors: Shirui Chen, Cole Harrison, Ying-Chun Lee, Angela Jin Yang, Zhongzheng Ren, Lillian J. Ratliff, Jiafei Duan, Dieter Fox...
- Keyword score: 3 / hits: vision-language-action, reinforcement learning, behavior cloning

<details><summary>Abstract</summary>

While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.

</details>

**LLM Summary**

- What: 事前学習済みビデオVision-Language Models (VLMs) の潜在的な世界知識を利用して、ロボットタスクの進捗をゼロショットで推定する新しい報酬モデル「TOPReward」を提案。
- Novelty: VLMに直接進捗値を直接出力させるのではなく、内部のトークンロジットからタスク進捗を抽出。これにより、数値的な誤表現を防ぎ、より信頼性の高い進捗評価を実現。
- Why it matters: ロボットの強化学習におけるサンプル効率と報酬のスパース性の問題を改善し、多様なタスクとロボットプラットフォームでの汎用的な学習を可能にする。

## 4. Safe and Interpretable Multimodal Path Planning for Multi-Agent Cooperation
- arXiv: http://arxiv.org/abs/2602.19304v1
- PDF: https://arxiv.org/pdf/2602.19304v1
- Authors: Haojun Shi, Suyu Ye, Katherine M. Guerrerio, Jianzhi Shen, Yifan Yin, Daniel Khashabi, Chien-Ming Huang, Tianmin Shu
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

Successful cooperation among decentralized agents requires each agent to quickly adapt its plan to the behavior of other agents. In scenarios where agents cannot confidently predict one another's intentions and plans, language communication can be crucial for ensuring safety. In this work, we focus on path-level cooperation in which agents must adapt their paths to one another in order to avoid collisions or perform physical collaboration such as joint carrying. In particular, we propose a safe and interpretable multimodal path planning method, CaPE (Code as Path Editor), which generates and updates path plans for an agent based on the environment and language communication from other agents. CaPE leverages a vision-language model (VLM) to synthesize a path editing program verified by a model-based planner, grounding communication to path plan updates in a safe and interpretable way. We evaluate our approach in diverse simulated and real-world scenarios, including multi-robot and human-robot cooperation in autonomous driving, household, and joint carrying tasks. Experimental results demonstrate that CaPE can be integrated into different robotic systems as a plug-and-play module, greatly enhancing a robot's ability to align its plan to language communication from other robots or humans. We also show that the combination of the VLM-based path editing program synthesis and model-based planning safety enables robots to achieve open-ended cooperation while maintaining safety and interpretability.

</details>

**LLM Summary**

- What: 分散型マルチエージェント協調のための、安全で解釈可能なマルチモーダル経路計画手法「CaPE (Code as Path Editor)」を提案。
- Novelty: Vision-Language Model (VLM) を活用して、環境情報と他エージェントからの言語コミュニケーションに基づいて経路編集プログラムを生成・検証。コミュニケーションを安全かつ解釈可能な形で経路計画に反映。
- Why it matters: マルチロボットや人間とロボットの協調タスクにおいて、衝突回避や共同作業を安全かつ効率的に実現するための、プラグアンドプレイ可能なモジュールを提供する。

## 5. Human-to-Robot Interaction: Learning from Video Demonstration for Robot Imitation
- arXiv: http://arxiv.org/abs/2602.19184v1
- PDF: https://arxiv.org/pdf/2602.19184v1
- Authors: Thanh Nguyen Canh, Thanh-Tuan Tran, Haolan Zhang, Ziyan Gao, Nak Young Chong, Xiem HoangVan
- Keyword score: 3 / hits: video understanding, reinforcement learning, imitation learning

<details><summary>Abstract</summary>

Learning from Demonstration (LfD) offers a promising paradigm for robot skill acquisition. Recent approaches attempt to extract manipulation commands directly from video demonstrations, yet face two critical challenges: (1) general video captioning models prioritize global scene features over task-relevant objects, producing descriptions unsuitable for precise robotic execution, and (2) end-to-end architectures coupling visual understanding with policy learning require extensive paired datasets and struggle to generalize across objects and scenarios. To address these limitations, we propose a novel ``Human-to-Robot'' imitation learning pipeline that enables robots to acquire manipulation skills directly from unstructured video demonstrations, inspired by the human ability to learn by watching and imitating. Our key innovation is a modular framework that decouples the learning process into two distinct stages: (1) Video Understanding, which combines Temporal Shift Modules (TSM) with Vision-Language Models (VLMs) to extract actions and identify interacted objects, and (2) Robot Imitation, which employs TD3-based deep reinforcement learning to execute the demonstrated manipulations. We validated our approach in PyBullet simulation environments with a UR5e manipulator and in a real-world experiment with a UF850 manipulator across four fundamental actions: reach, pick, move, and put. For video understanding, our method achieves 89.97% action classification accuracy and BLEU-4 scores of 0.351 on standard objects and 0.265 on novel objects, representing improvements of 76.4% and 128.4% over the best baseline, respectively. For robot manipulation, our framework achieves an average success rate of 87.5% across all actions, with 100% success on reaching tasks and up to 90% on complex pick-and-place operations. The project website is available at https://thanhnguyencanh.github.io/LfD4hri.

</details>

**LLM Summary**

- What: 非構造化されたビデオデモンストレーションからロボットが操作スキルを直接学習できる「Human-to-Robot」模倣学習パイプラインを提案。
- Novelty: 学習プロセスを「ビデオ理解」と「ロボット模倣」の2つのモジュールに分離。ビデオ理解では、Temporal Shift Modules (TSM) と Vision-Language Models (VLMs) を組み合わせて、操作と対象オブジェクトを抽出。ロボット模倣では、TD3ベースの強化学習で操作を実行。
- Why it matters: ロボットが人間のようにビデオを見て模倣することで、より効率的かつ汎用的に操作スキルを習得できるようになる。

## 6. UrbanAlign: Post-hoc Semantic Calibration for VLM-Human Preference Alignment
- arXiv: http://arxiv.org/abs/2602.19442v1
- PDF: https://arxiv.org/pdf/2602.19442v1
- Authors: Yecheng Zhang, Rong Zhao, Zhizhou Sha, Yong Li, Lei Wang, Ce Hou, Wen Ji, Hao Huang...
- Keyword score: 2 / hits: reinforcement learning, multi-agent

<details><summary>Abstract</summary>

Aligning vision-language model (VLM) outputs with human preferences in domain-specific tasks typically requires fine-tuning or reinforcement learning, both of which demand labelled data and GPU compute. We show that for subjective perception tasks, this alignment can be achieved without any model training: VLMs are already strong concept extractors but poor decision calibrators, and the gap can be closed externally. We propose a training-free post-hoc concept-bottleneck pipeline consisting of three tightly coupled stages: concept mining, multi-agent structured scoring, and geometric calibration, unified by an end-to-end dimension optimization loop. Interpretable evaluation dimensions are mined from a handful of human annotations; an Observer-Debater-Judge chain extracts robust continuous concept scores from a frozen VLM; and locally-weighted ridge regression on a hybrid visual-semantic manifold calibrates these scores against human ratings. Applied to urban perception as UrbanAlign, the framework achieves 72.2% accuracy ($κ=0.45$) on Place Pulse 2.0 across six categories, outperforming the best supervised baseline by +15.1 pp and uncalibrated VLM scoring by +16.3 pp, with full dimension-level interpretability and zero model-weight modification.

</details>

**LLM Summary**

- What: 事後的な意味的キャリブレーションにより、VLM（Vision-Language Model）の出力を人間の好みに合わせる手法「UrbanAlign」を提案。モデルの再学習なしに、概念抽出と意思決定キャリブレーションのギャップを埋める。
- Novelty: モデルの再学習を必要としない、事後的なキャリブレーションパイプライン（概念マイニング、マルチエージェント構造化スコアリング、幾何学的キャリブレーション）を提案。
- Why it matters: ラベル付きデータやGPUリソースなしに、ドメイン固有のタスクでVLMの出力を人間の好みに合わせることが可能になり、特に都市知覚タスクで高い精度を達成。

## 7. Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device
- arXiv: http://arxiv.org/abs/2602.20161v1
- PDF: https://arxiv.org/pdf/2602.20161v1
- Authors: Abdelrahman Shaker, Ahmed Heakl, Jaseel Muhammad, Ritesh Thawkar, Omkar Thawakar, Senmao Li, Hisham Cholakkal, Ian Reid...
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/

</details>

**LLM Summary**

- What: モバイルデバイス上で動作する、統一的なマルチモーダル理解・生成モデル「Mobile-O」を開発。軽量なMobile Conditioning Projector (MCP) を用いて、効率的なクロスモーダル条件付けを実現。
- Novelty: モバイルデバイスでの展開を可能にする、コンパクトで効率的なビジョン・言語・拡散モデル。深層分離畳み込みと層ごとのアライメントを用いたMCPが特徴。
- Why it matters: モバイルデバイス上で高度なマルチモーダルAI機能（理解と生成）を提供し、既存モデルと比較して高速かつ高性能。

## 8. RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection
- arXiv: http://arxiv.org/abs/2602.19974v1
- PDF: https://arxiv.org/pdf/2602.19974v1
- Authors: Tianyu Wang, Zhiyuan Ma, Qian Wang, Xinyi Zhang, Xinwei Long, Bowen Zhou
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Recent advancements in image generation have achieved impressive results in producing high-quality images. However, existing image generation models still generally struggle with a spatial reasoning dilemma, lacking the ability to accurately capture fine-grained spatial relationships from the prompt and correctly generate scenes with structural integrity. To mitigate this dilemma, we propose RL-RIG, a Reinforcement Learning framework for Reflection-based Image Generation. Our architecture comprises four primary components: Diffuser, Checker, Actor, and Inverse Diffuser, following a Generate-Reflect-Edit paradigm to spark the Chain of Thought reasoning ability in image generation for addressing the dilemma. To equip the model with better intuition over generation trajectories, we further develop Reflection-GRPO to train the VLM Actor for edit prompts and the Image Editor for better image quality under a given prompt, respectively. Unlike traditional approaches that solely produce visually stunning yet structurally unreasonable content, our evaluation metrics prioritize spatial accuracy, utilizing Scene Graph IoU and employing a VLM-as-a-Judge strategy to assess the spatial consistency of generated images on LAION-SG dataset. Experimental results show that RL-RIG outperforms existing state-of-the-art open-source models by up to 11% in terms of controllable and precise spatial reasoning in image generation.

</details>

**LLM Summary**

- What: 画像生成における空間的推論の課題に対処するため、内省に基づく画像生成フレームワーク「RL-RIG」を提案。Generate-Reflect-Editのパラダイムで、思考連鎖（Chain of Thought）推論能力を画像生成に導入。
- Novelty: 画像生成に内省（Reflection）と強化学習を組み合わせ、空間的関係性を正確に捉えるための新しいフレームワーク。
- Why it matters: 構造的な整合性を保ちつつ、より正確な空間関係を持つ画像を生成できるようになり、画像生成モデルの推論能力を向上させる。

## 9. BigMaQ: A Big Macaque Motion and Animation Dataset Bridging Image and 3D Pose Representations
- arXiv: http://arxiv.org/abs/2602.19874v1
- PDF: https://arxiv.org/pdf/2602.19874v1
- Authors: Lucas Martini, Alexander Lappe, Anna Bognár, Rufin Vogels, Martin A. Giese
- Keyword score: 1 / hits: action recognition

<details><summary>Abstract</summary>

The recognition of dynamic and social behavior in animals is fundamental for advancing ethology, ecology, medicine and neuroscience. Recent progress in deep learning has enabled automated behavior recognition from video, yet an accurate reconstruction of the three-dimensional (3D) pose and shape has not been integrated into this process. Especially for non-human primates, mesh-based tracking efforts lag behind those for other species, leaving pose descriptions restricted to sparse keypoints that are unable to fully capture the richness of action dynamics. To address this gap, we introduce the $\textbf{Big Ma}$ca$\textbf{Q}$ue 3D Motion and Animation Dataset ($\texttt{BigMaQ}$), a large-scale dataset comprising more than 750 scenes of interacting rhesus macaques with detailed 3D pose descriptions. Extending previous surface-based animal tracking methods, we construct subject-specific textured avatars by adapting a high-quality macaque template mesh to individual monkeys. This allows us to provide pose descriptions that are more accurate than previous state-of-the-art surface-based animal tracking methods. From the original dataset, we derive BigMaQ500, an action recognition benchmark that links surface-based pose vectors to single frames across multiple individual monkeys. By pairing features extracted from established image and video encoders with and without our pose descriptors, we demonstrate substantial improvements in mean average precision (mAP) when pose information is included. With these contributions, $\texttt{BigMaQ}$ establishes the first dataset that both integrates dynamic 3D pose-shape representations into the learning task of animal action recognition and provides a rich resource to advance the study of visual appearance, posture, and social interaction in non-human primates. The code and data are publicly available at https://martinivis.github.io/BigMaQ/ .

</details>

**LLM Summary**

- What: サル（マカク）の行動認識と3Dポーズ推定のための大規模データセット「BigMaQ」を構築。個体ごとにテクスチャ付きアバターを生成し、高精度な3Dポーズ記述を提供。
- Novelty: 非ヒト霊長類のインタラクションを含む、詳細な3Dポーズ記述を備えた大規模なモーション・アニメーションデータセット。
- Why it matters: 動物の動的・社会的行動の認識を促進し、エソロジー、生態学、医学、神経科学の進歩に貢献する。

## 10. Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling
- arXiv: http://arxiv.org/abs/2602.19764v1
- PDF: https://arxiv.org/pdf/2602.19764v1
- Authors: Yirui Sun, Guangyu Zhuge, Keliang Liu, Jie Gu, Zhihao xia, Qionglin Ren, Chunxu tian, Zhongxue Ga
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Realizing dexterous embodied manipulation necessitates the deep integration of heterogeneous multimodal sensory inputs. However, current vision-centric paradigms often overlook the critical force and geometric feedback essential for complex tasks. This paper presents DeMUSE, a Deep Multimodal Unified Sparse Experts framework leveraging a Diffusion Transformer to integrate RGB, depth, and 6-axis force into a unified serialized stream. Adaptive Modality-specific Normalization (AdaMN) is employed to recalibrate modality-aware features, mitigating representation imbalance and harmonizing the heterogeneous distributions of multi-sensory signals. To facilitate efficient scaling, the architecture utilizes a Sparse Mixture-of-Experts (MoE) with shared experts, increasing model capacity for physical priors while maintaining the low inference latency required for real-time control. A Joint denoising objective synchronously synthesizes environmental evolution and action sequences to ensure physical consistency. Achieving success rates of 83.2% and 72.5% in simulation and real-world trials, DeMUSE demonstrates state-of-the-art performance, validating the necessity of deep multi-sensory integration for complex physical interactions.

</details>

**LLM Summary**

- What: 複数の感覚情報（RGB、深度、力覚）を深く統合し、器用な身体操作を実現するフレームワーク「DeMUSE」を提案。拡散トランスフォーマーとスパース専門家混合モデル（MoE）を活用。
- Novelty: RGB、深度、6軸力覚を統合する深層マルチモーダルフレームワークと、効率的なスケーリングのためのスパースMoEアーキテクチャ。
- Why it matters: ロボット工学における器用な操作能力を向上させ、シミュレーションおよび実世界でのタスク成功率を高める。

## 11. Botson: An Accessible and Low-Cost Platform for Social Robotics Research
- arXiv: http://arxiv.org/abs/2602.19491v1
- PDF: https://arxiv.org/pdf/2602.19491v1
- Authors: Samuel Bellaire, Abdalmalek Abu-raddaha, Natalie Kim, Nathan Morhan, William Elliott, Samir Rawashdeh
- Keyword score: 1 / hits: embodied agent

<details><summary>Abstract</summary>

Trust remains a critical barrier to the effective integration of Artificial Intelligence (AI) into human-centric domains. Disembodied agents, such as voice assistants, often fail to establish trust due to their inability to convey non-verbal social cues. This paper introduces the architecture of Botson: an anthropomorphic social robot powered by a large language model (LLM). Botson was created as a low-cost and accessible platform for social robotics research.

</details>

**LLM Summary**

- What: LLMを搭載した擬人化ソーシャルロボット「Botson」のアーキテクチャを提案した。
- Novelty: 低コストでアクセスしやすいソーシャルロボティクス研究プラットフォームであること。
- Why it matters: AIの人間中心領域への統合における信頼性の向上に貢献する。

## 12. RAmmStein: Regime Adaptation in Mean-reverting Markets with Stein Thresholds -- Optimal Impulse Control in Concentrated AMMs
- arXiv: http://arxiv.org/abs/2602.19419v1
- PDF: https://arxiv.org/pdf/2602.19419v1
- Authors: Pranay Anchuri
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Concentrated liquidity provision in decentralized exchanges presents a fundamental Impulse Control problem. Liquidity Providers (LPs) face a non-trivial trade-off between maximizing fee accrual through tight price-range concentration and minimizing the friction costs of rebalancing, including gas fees and swap slippage. Existing methods typically employ heuristic or threshold strategies that fail to account for market dynamics. This paper formulates liquidity management as an optimal control problem and derives the corresponding Hamilton-Jacobi-Bellman quasi-variational inequality (HJB-QVI). We present an approximate solution RAmmStein, a Deep Reinforcement Learning method that incorporates the mean-reversion speed (theta) of an Ornstein-Uhlenbeck process among other features as input to the model. We demonstrate that the agent learns to separate the state space into regions of action and inaction. We evaluate the framework using high-frequency 1Hz Coinbase trade data comprising over 6.8M trades. Experimental results show that RAmmStein achieves a superior net ROI of 0.72% compared to both passive and aggressive strategies. Notably, the agent reduces rebalancing frequency by 67% compared to a greedy rebalancing strategy while maintaining 88% active time. Our results demonstrate that regime-aware laziness can significantly improve capital efficiency by preserving the returns that would otherwise be eroded by the operational costs.

</details>

**LLM Summary**

- What: 集中型AMMにおける流動性提供のインパルス制御問題を定式化し、深層強化学習による近似解「RAmmStein」を提案した。
- Novelty: 市場の平均回帰速度を考慮した深層強化学習手法。
- Why it matters: 流動性提供者の収益性を向上させ、リバランス頻度を大幅に削減する。

## 13. The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption
- arXiv: http://arxiv.org/abs/2602.19260v1
- PDF: https://arxiv.org/pdf/2602.19260v1
- Authors: Timothy Duggan, Pierrick Lorang, Hong Lu, Matthias Scheutz
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models have recently been proposed as a pathway toward generalist robotic policies capable of interpreting natural language and visual inputs to generate manipulation actions. However, their effectiveness and efficiency on structured, long-horizon manipulation tasks remain unclear. In this work, we present a head-to-head empirical comparison between a fine-tuned open-weight VLA model π0 and a neuro-symbolic architecture that combines PDDL-based symbolic planning with learned low-level control. We evaluate both approaches on structured variants of the Towers of Hanoi manipulation task in simulation while measuring both task performance and energy consumption during training and execution. On the 3-block task, the neuro-symbolic model achieves 95% success compared to 34% for the best-performing VLA. The neuro-symbolic model also generalizes to an unseen 4-block variant (78% success), whereas both VLAs fail to complete the task. During training, VLA fine-tuning consumes nearly two orders of magnitude more energy than the neuro-symbolic approach. These results highlight important trade-offs between end-to-end foundation-model approaches and structured reasoning architectures for long-horizon robotic manipulation, emphasizing the role of explicit symbolic structure in improving reliability, data efficiency, and energy efficiency. Code and models are available at https://price-is-not-right.github.io

</details>

**LLM Summary**

- What: 構造化された長期間の操作タスクにおいて、ニューロシンボリック手法がVision-Language-Action (VLA) モデルよりも優れた性能と低いエネルギー消費を示すことを実証した。
- Novelty: VLAモデルとニューロシンボリック手法の直接比較とエネルギー消費の評価。
- Why it matters: ロボット操作タスクにおける効率的で高性能なアプローチの選択肢を提供する。

## 14. IDSelect: A RL-Based Cost-Aware Selection Agent for Video-based Multi-Modal Person Recognition
- arXiv: http://arxiv.org/abs/2602.18990v1
- PDF: https://arxiv.org/pdf/2602.18990v1
- Authors: Yuyang Ji, Yixuan Shen, Kien Nguyen, Lifeng Zhou, Feng Liu
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Video-based person recognition achieves robust identification by integrating face, body, and gait. However, current systems waste computational resources by processing all modalities with fixed heavyweight ensembles regardless of input complexity. To address these limitations, we propose IDSelect, a reinforcement learning-based cost-aware selector that chooses one pre-trained model per modality per-sequence to optimize the accuracy-efficiency trade-off. Our key insight is that an input-conditioned selector can discover complementary model choices that surpass fixed ensembles while using substantially fewer resources. IDSelect trains a lightweight agent end-to-end using actor-critic reinforcement learning with budget-aware optimization. The reward balances recognition accuracy with computational cost, while entropy regularization prevents premature convergence. At inference, the policy selects the most probable model per modality and fuses modality-specific similarities for the final score. Extensive experiments on challenging video-based datasets demonstrate IDSelect's superior efficiency: on CCVID, it achieves 95.9% Rank-1 accuracy with 92.4% less computation than strong baselines while improving accuracy by 1.8%; on MEVID, it reduces computation by 41.3% while maintaining competitive performance.

</details>

**LLM Summary**

- What: 動画ベースのマルチモーダル人物認識において、計算リソースを最適化する強化学習ベースのコスト認識選択エージェント「IDSelect」を提案した。
- Novelty: 入力に応じて最適なモデルを選択する、コストを考慮した強化学習ベースの選択エージェント。
- Why it matters: 精度を維持しつつ、計算リソースを大幅に削減し、効率的な人物認識を実現する。
