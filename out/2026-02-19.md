# Daily CV Digest (2026-02-19)

- Total: 8

## 1. Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning
- arXiv: http://arxiv.org/abs/2602.16196v1
- PDF: https://arxiv.org/pdf/2602.16196v1
- Authors: Emile Anand, Richard Hoffmann, Sarah Liaw, Adam Wierman
- Keyword score: 7 / hits: reinforcement learning, multi-agent, marl

<details><summary>Abstract</summary>

Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\texttt{GMFS}$, a $\textbf{G}$raphon $\textbf{M}$ean-$\textbf{F}$ield $\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $κ$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\mathrm{poly}(κ)$ and optimality gap $O(1/\sqrtκ)$. We verify our theory with numerical simulations in robotic coordination, showing that $\texttt{GMFS}$ achieves near-optimal performance.

</details>

**LLM Summary**

- What: 大規模協調型異種マルチエージェント強化学習のために、グラフオン平均場サブサンプリング（GMFS）を提案した。
- Novelty: グラフオンベースのフレームワークの計算コストを削減し、エージェント間の異種相互作用を考慮したスケーラブルな学習を可能にした。
- Why it matters: ロボット協調などの複雑なマルチエージェントシステムにおいて、効率的かつ高性能な学習を実現する。

## 2. Let's Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding
- arXiv: http://arxiv.org/abs/2602.16545v1
- PDF: https://arxiv.org/pdf/2602.16545v1
- Authors: Kaiting Liu, Hazel Doughty
- Keyword score: 3 / hits: video understanding

<details><summary>Abstract</summary>

Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.

</details>

**LLM Summary**

- What: ファイングレイン動画理解のためのゼロショット分類器編集手法「カテゴリ分割」を提案した。
- Novelty: 追加データなしで、既存の分類器を粗いカテゴリから細かいサブカテゴリに編集し、他のカテゴリの精度を維持する。
- Why it matters: 動画認識モデルの柔軟性を高め、進化するタスクや定義に対応するための再学習コストを削減する。

## 3. ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding
- arXiv: http://arxiv.org/abs/2602.16412v1
- PDF: https://arxiv.org/pdf/2602.16412v1
- Authors: Daichi Yashima, Shuhei Kurita, Yusuke Oda, Komei Sugiura
- Keyword score: 3 / hits: video understanding

<details><summary>Abstract</summary>

While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating directly on their compressed representations. A sparse set of RGB keyframes is retained for appearance, while temporal dynamics are encoded as a motion representation, removing the need for sequential RGB frames. These motion representations act as a compact proxy for optical flow, capturing temporal dynamics without full frame decoding. To refine the noise and low fidelity of block-based motions, we introduce a module to denoise and generate a fine-grained motion representation. Furthermore, our model compresses these features in a way that scales linearly with sequence length. We demonstrate the effectiveness of ReMoRa through extensive experiments across a comprehensive suite of long-video understanding benchmarks. ReMoRa outperformed baseline methods on multiple challenging benchmarks, including LongVideoBench, NExT-QA, and MLVU.

</details>

**LLM Summary**

- What: 長尺動画理解のためのマルチモーダル大規模言語モデル（MLLM）「ReMoRa」を提案した。
- Novelty: RGBキーフレームとノイズ除去された微細な動き表現を組み合わせ、計算コストを線形にスケーリングする。
- Why it matters: 計算量問題を克服し、長尺動画の理解能力を大幅に向上させる。

## 4. Markerless Robot Detection and 6D Pose Estimation for Multi-Agent SLAM
- arXiv: http://arxiv.org/abs/2602.16308v1
- PDF: https://arxiv.org/pdf/2602.16308v1
- Authors: Markus Rueggeberg, Maximilian Ulmer, Maximilian Durner, Wout Boerdijk, Marcus Gerhard Mueller, Rudolph Triebel, Riccardo Giubilato
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

The capability of multi-robot SLAM approaches to merge localization history and maps from different observers is often challenged by the difficulty in establishing data association. Loop closure detection between perceptual inputs of different robotic agents is easily compromised in the context of perceptual aliasing, or when perspectives differ significantly. For this reason, direct mutual observation among robots is a powerful way to connect partial SLAM graphs, but often relies on the presence of calibrated arrays of fiducial markers (e.g., AprilTag arrays), which severely limits the range of observations and frequently fails under sharp lighting conditions, e.g., reflections or overexposure. In this work, we propose a novel solution to this problem leveraging recent advances in Deep-Learning-based 6D pose estimation. We feature markerless pose estimation as part of a decentralized multi-robot SLAM system and demonstrate the benefit to the relative localization accuracy among the robotic team. The solution is validated experimentally on data recorded in a test field campaign on a planetary analogous environment.

</details>

**LLM Summary**

- What: マルチエージェントSLAMのためのマーカーレスロボット検出と6D姿勢推定手法を提案した。
- Novelty: ディープラーニングベースの6D姿勢推定を活用し、マーカーに依存しないロボット間の相対位置推定を実現した。
- Why it matters: ロボット間のデータ関連付けを容易にし、分散型マルチロボットSLAMの精度と頑健性を向上させる。

## 5. World Action Models are Zero-shot Policies
- arXiv: http://arxiv.org/abs/2602.15922v1
- PDF: https://arxiv.org/pdf/2602.15922v1
- Authors: Seonghyeon Ye, Yunhao Ge, Kaiyuan Zheng, Shenyuan Gao, Sihyun Yu, George Kurian, Suneel Indupuru, You Liang Tan...
- Keyword score: 2 / hits: real-time, vision-language-action

<details><summary>Abstract</summary>

State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.

</details>

**LLM Summary**

- What: 世界行動モデル（WAM）に基づくゼロショットポリシー「DreamZero」を提案した。
- Novelty: 動画の物理的ダイナミクスを学習し、多様なスキルを効率的に習得し、未知のタスクや環境への汎化性能を向上させる。
- Why it matters: ロボット制御における汎化能力を大幅に向上させ、リアルタイム制御やクロスエンボディメント転移を可能にする。

## 6. World Model Failure Classification and Anomaly Detection for Autonomous Inspection
- arXiv: http://arxiv.org/abs/2602.16182v1
- PDF: https://arxiv.org/pdf/2602.16182v1
- Authors: Michelle Ho, Muhammad Fadhil Ginting, Isaac R. Ward, Andrzej Reinke, Mykel J. Kochenderfer, Ali-akbar Agha-Mohammadi, Shayegan Omidshafiei
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Autonomous inspection robots for monitoring industrial sites can reduce costs and risks associated with human-led inspection. However, accurate readings can be challenging due to occlusions, limited viewpoints, or unexpected environmental conditions. We propose a hybrid framework that combines supervised failure classification with anomaly detection, enabling classification of inspection tasks as a success, known failure, or anomaly (i.e., out-of-distribution) case. Our approach uses a world model backbone with compressed video inputs. This policy-agnostic, distribution-free framework determines classifications based on two decision functions set by conformal prediction (CP) thresholds before a human observer does. We evaluate the framework on gauge inspection feeds collected from office and industrial sites and demonstrate real-time deployment on a Boston Dynamics Spot. Experiments show over 90% accuracy in distinguishing between successes, failures, and OOD cases, with classifications occurring earlier than a human observer. These results highlight the potential for robust, anticipatory failure detection in autonomous inspection tasks or as a feedback signal for model training to assess and improve the quality of training data. Project website: https://autoinspection-classification.github.io

</details>

**LLM Summary**

- What: 自律検査ロボットのワールドモデルの失敗分類と異常検知を行うハイブリッドフレームワークを提案した。圧縮ビデオ入力を利用し、コンフォーマル予測を用いて成功、既知の失敗、異常（分布外）の3種類に分類する。
- Novelty: 検査タスクの成功/失敗/異常を、人間の観察者よりも早く、ポリシー非依存かつ分布フリーで分類する。
- Why it matters: 自律検査タスクにおける堅牢で予見的な失敗検出や、モデル学習のためのフィードバック信号として役立つ。

## 7. Edge Learning via Federated Split Decision Transformers for Metaverse Resource Allocation
- arXiv: http://arxiv.org/abs/2602.16174v1
- PDF: https://arxiv.org/pdf/2602.16174v1
- Authors: Fatih Temiz, Shavbo Salehi, Melike Erol-Kantarci
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Mobile edge computing (MEC) based wireless metaverse services offer an untethered, immersive experience to users, where the superior quality of experience (QoE) needs to be achieved under stringent latency constraints and visual quality demands. To achieve this, MEC-based intelligent resource allocation for virtual reality users needs to be supported by coordination across MEC servers to harness distributed data. Federated learning (FL) is a promising solution, and can be combined with reinforcement learning (RL) to develop generalized policies across MEC-servers. However, conventional FL incurs transmitting the full model parameters across the MEC-servers and the cloud, and suffer performance degradation due to naive global aggregation, especially in heterogeneous multi-radio access technology environments. To address these challenges, this paper proposes Federated Split Decision Transformer (FSDT), an offline RL framework where the transformer model is partitioned between MEC servers and the cloud. Agent-specific components (e.g., MEC-based embedding and prediction layers) enable local adaptability, while shared global layers in the cloud facilitate cooperative training across MEC servers. Experimental results demonstrate that FSDT enhances QoE for up to 10% in heterogeneous environments compared to baselines, while offloadingnearly 98% of the transformer model parameters to the cloud, thereby reducing the computational burden on MEC servers.

</details>

**LLM Summary**

- What: メタバースのリソース割り当てのために、エッジ学習を行うFederated Split Decision Transformer (FSDT) を提案した。TransformerモデルをMECサーバーとクラウドに分割し、ローカル適応性と協調学習を可能にする。
- Novelty: Transformerモデルを分割し、MECサーバーとクラウドで協調学習を行うオフラインRLフレームワーク。従来のFLのモデルパラメータ送信や性能低下の問題を解決する。
- Why it matters: 低遅延・高視覚品質が求められるメタバースサービスにおいて、MECサーバー間の協調によるリソース割り当ての効率化に貢献する。

## 8. IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models
- arXiv: http://arxiv.org/abs/2602.16138v1
- PDF: https://arxiv.org/pdf/2602.16138v1
- Authors: Parsa Madinei, Srijita Karmakar, Russell Cohen Hoffing, Felix Gervitz, Miguel P. Eckstein
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%) while maintaining performance on unambiguous queries. We evaluate our approach across state-of-the-art VLMs, showing consistent improvements when gaze data is incorporated in ambiguous image-question pairs, regardless of architectural differences. We release a new benchmark dataset to use eye movement data for disambiguated VQA, a novel real-time interactive protocol, and an evaluation suite.

</details>

**LLM Summary**

- What: オープンエンドなVQAにおける曖昧さ解消のために、リアルタイムの視線追跡データを利用するトレーニング不要のアプローチ「IRIS」を提案した。
- Novelty: 質問開始時の視線情報を利用して、大規模視覚言語モデル(VLM)の曖昧な質問に対する回答精度を大幅に向上させる。
- Why it matters: VQAタスクにおいて、曖昧な質問に対する回答の精度を向上させ、より自然で正確な対話を実現する。
