# Daily CV Digest (2026-02-18)

- Total: 14

## 1. EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use
- arXiv: http://arxiv.org/abs/2602.15329v1
- PDF: https://arxiv.org/pdf/2602.15329v1
- Authors: Siwei Wen, Zhangcheng Wang, Xingjian Zhang, Lei Huang, Wenjun Wu
- Keyword score: 5 / hits: streaming, video understanding, reinforcement learning

<details><summary>Abstract</summary>

Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.

</details>

**LLM Summary**

- What: オンライン動画理解のための階層的イベント中心メモリと適応的ツール使用を備えたエージェントフレームワーク「EventMemAgent」を提案。
- Novelty: 短期メモリでイベント境界を検出し、長期メモリでイベント単位で過去の観測を構造化してアーカイブ。マルチグラニュラー知覚ツールキットとエージェント的強化学習を統合。
- Why it matters: 無限の動画ストリームにおける長距離推論と詳細な情報キャプチャのトレードオフを解消し、オンライン動画理解の精度と効率を向上させる。

## 2. Feasibility-aware Imitation Learning from Observation with Multimodal Feedback
- arXiv: http://arxiv.org/abs/2602.15351v1
- PDF: https://arxiv.org/pdf/2602.15351v1
- Authors: Kei Takahashi, Hikaru Sasaki, Takamitsu Matsubara
- Keyword score: 4 / hits: imitation learning, behavior cloning

<details><summary>Abstract</summary>

Imitation learning frameworks that learn robot control policies from demonstrators' motions via hand-mounted demonstration interfaces have attracted increasing attention. However, due to differences in physical characteristics between demonstrators and robots, this approach faces two limitations: i) the demonstration data do not include robot actions, and ii) the demonstrated motions may be infeasible for robots. These limitations make policy learning difficult. To address them, we propose Feasibility-Aware Behavior Cloning from Observation (FABCO). FABCO integrates behavior cloning from observation, which complements robot actions using robot dynamics models, with feasibility estimation. In feasibility estimation, the demonstrated motions are evaluated using a robot-dynamics model, learned from the robot's execution data, to assess reproducibility under the robot's dynamics. The estimated feasibility is used for multimodal feedback and feasibility-aware policy learning to improve the demonstrator's motions and learn robust policies. Multimodal feedback provides feasibility through the demonstrator's visual and haptic senses to promote feasible demonstrated motions. Feasibility-aware policy learning reduces the influence of demonstrated motions that are infeasible for robots, enabling the learning of policies that robots can execute stably. We conducted experiments with 15 participants on two tasks and confirmed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.

</details>

**LLM Summary**

- What: ロボットの動作特性の違いによる模倣学習の課題（デモンストレーションデータにロボットの行動が含まれない、デモンストレーションされた動作がロボットにとって実行不可能であること）に対処するため、実現可能性を考慮した観測からの模倣学習手法「FABCO」を提案。
- Novelty: 行動クローニングと実現可能性推定を統合。ロボットのダイナミクスモデルを用いてデモンストレーションされた動作の再現性を評価し、その実現可能性をマルチモーダルフィードバックとして活用。
- Why it matters: ロボットが実行可能なデモンストレーションを促進し、より堅牢な制御ポリシーを学習することで、ロボット制御の精度と信頼性を向上させる。

## 3. Lifelong Scalable Multi-Agent Realistic Testbed and A Comprehensive Study on Design Choices in Lifelong AGV Fleet Management Systems
- arXiv: http://arxiv.org/abs/2602.15721v1
- PDF: https://arxiv.org/pdf/2602.15721v1
- Authors: Jingtian Yan, Yulun Zhang, Zhenting Liu, Han Zhang, He Jiang, Jingkai Chen, Stephen F. Smith, Jiaoyang Li
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

We present Lifelong Scalable Multi-Agent Realistic Testbed (LSMART), an open-source simulator to evaluate any Multi-Agent Path Finding (MAPF) algorithm in a Fleet Management System (FMS) with Automated Guided Vehicles (AGVs). MAPF aims to move a group of agents from their corresponding starting locations to their goals. Lifelong MAPF (LMAPF) is a variant of MAPF that continuously assigns new goals for agents to reach. LMAPF applications, such as autonomous warehouses, often require a centralized, lifelong system to coordinate the movement of a fleet of robots, typically AGVs. However, existing works on MAPF and LMAPF often assume simplified kinodynamic models, such as pebble motion, as well as perfect execution and communication for AGVs. Prior work has presented SMART, a software capable of evaluating any MAPF algorithms while considering agent kinodynamics, communication delays, and execution uncertainties. However, SMART is designed for MAPF, not LMAPF. Generalizing SMART to an FMS requires many more design choices. First, an FMS parallelizes planning and execution, raising the question of when to plan. Second, given planners with varying optimality and differing agent-model assumptions, one must decide how to plan. Third, when the planner fails to return valid solutions, the system must determine how to recover. In this paper, we first present LSMART, an open-source simulator that incorporates all these considerations to evaluate any MAPF algorithms in an FMS. We then provide experiment results based on state-of-the-art methods for each design choice, offering guidance on how to effectively design centralized lifelong AGV Fleet Management Systems. LSMART is available at https://smart-mapf.github.io/lifelong-smart.

</details>

**LLM Summary**

- What: 自動倉庫などのフリート管理システムにおける自動誘導車両（AGV）の群制御のための、ライフロン・スケーラブル・マルチエージェント・リアリスティック・テストベッド「LSMART」を開発。
- Novelty: AGVの運動学、通信遅延、実行不確実性を考慮したMAPF（Multi-Agent Path Finding）評価をライフロン（継続的）な目標割り当てに対応。フリート管理システムにおける計画と実行の並列化、計画実行タイミングなどの設計選択肢を包括的に研究。
- Why it matters: ライフロンAGVフリート管理システムの設計と評価のための、現実的でスケーラブルなシミュレーション環境と包括的な研究を提供し、効率的なロボット群制御システムの開発を支援する。

## 4. Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions
- arXiv: http://arxiv.org/abs/2602.15567v1
- PDF: https://arxiv.org/pdf/2602.15567v1
- Authors: Jieting Long, Dechuan Liu, Weidong Cai, Ian Manchester, Weiming Zhi
- Keyword score: 3 / hits: streaming

<details><summary>Abstract</summary>

Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot's workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot's control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.

</details>

**LLM Summary**

- What: ロボットの軌道分布の適応学習を可能にする、制約を考慮したストリーミングフロー（CASF）フレームワークを提案。
- Novelty: 既存のストリーミングフローポリシーに、制約依存のメトリックを組み込み、実行中に学習された速度場を再形成。制約を微分可能な距離関数としてモデル化し、制御空間に引き戻す。
- Why it matters: ロボットの関節制限や障害物回避などの安全制約をリアルタイムで満たす軌道を生成し、安全でタスク指向のロボット制御を実現する。

## 5. The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems
- arXiv: http://arxiv.org/abs/2602.15382v1
- PDF: https://arxiv.org/pdf/2602.15382v1
- Authors: Xiaoze Liu, Ruowang Zhang, Weichen Yu, Siheng Xiong, Liu He, Feijie Wu, Hoin Jung, Matt Fredrikson...
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas

</details>

**LLM Summary**

- What: 異種マルチエージェントシステムにおける、モデルに依存しないテキストフリーの通信を可能にする「Vision Wormhole」フレームワークを提案。
- Novelty: Vision-Language Models (VLMs) の視覚インターフェースを再利用し、ユニバーサルビジュアルコーデックを用いて異種エージェントの推論トレースを共有連続潜在空間にマッピング。ハブ＆スポークトポロジーと教師なし蒸留を使用。
- Why it matters: 大規模言語モデルベースのマルチエージェントシステムにおける、離散的なテキスト通信の非効率性を解消し、高帯域幅でスケーラブルなエージェント間通信を実現する。

## 6. Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition
- arXiv: http://arxiv.org/abs/2602.15124v1
- PDF: https://arxiv.org/pdf/2602.15124v1
- Authors: Shiyu Xuan, Dongkai Wang, Zechao Li, Jinhui Tang
- Keyword score: 3 / hits: action recognition

<details><summary>Abstract</summary>

Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.

</details>

**LLM Summary**

- What: 人間のオブジェクトインタラクション（HOI）検出のための、検出器に依存しないゼロショットインタラクション認識フレームワークを提案。マルチモーダル大規模言語モデル（MLLM）を活用し、インタラクション認識を視覚的な質問応答タスクとして定式化。
- Novelty: 検出器からインタラクション認識を分離し、MLLMを用いたトレーニング不要のゼロショット認識を実現。空間情報を考慮したプーリングモジュールと、一度のフォワードパスで全候補インタラクションを予測するマッチング手法を導入。
- Why it matters: 未知のインタラクションに対する汎化性能を向上させ、HOI検出の効率と精度を高める。

## 7. Selective Perception for Robot: Task-Aware Attention in Multimodal VLA
- arXiv: http://arxiv.org/abs/2602.15543v1
- PDF: https://arxiv.org/pdf/2602.15543v1
- Authors: Young-Chae Son, Jung-Woo Lee, Yoon-Ji Choi, Dae-Kwan Ko, Soo-Chul Lim
- Keyword score: 2 / hits: real-time, vision-language-action

<details><summary>Abstract</summary>

In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.

</details>

**LLM Summary**

- What: ロボットのためのタスクアウェアな注意機構を持つ、選択的知覚フレームワークを提案。マルチビュー入力を動的に融合し、タスクに関連性の低い情報をフィルタリングする。
- Novelty: 軽量なアダプティブルーティングアーキテクチャにより、テキストプロンプトと観測に基づいてカメラビューのタスク関連性をリアルタイムに予測。関連性の低いビューの計算を抑制し、必要な視覚特徴のみをポリシーネットワークに提供。
- Why it matters: VLAモデルの計算効率とロバスト性を向上させ、ロボットのタスク遂行能力を高める。

## 8. MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction
- arXiv: http://arxiv.org/abs/2602.15733v1
- PDF: https://arxiv.org/pdf/2602.15733v1
- Authors: Qiang Zhang, Jiahao Ma, Peiran Liu, Shuai Shi, Zeran Su, Zifan Wang, Jingkai Sun, Wei Cui...
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled "motion-terrain" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.

</details>

**LLM Summary**

- What: 3Dシーン再構成を通じて、ジオメトリを考慮したヒューマノイドモーション学習を行うフレームワーク「MeshMimic」を提案。
- Novelty: ビデオから直接、モーションと地形の相互作用を学習。最先端の3Dビジョンモデルを用いて、人間の軌跡と地形・オブジェクトの3Dジオメトリを正確にセグメント化・再構成。運動学的整合性に基づいた最適化アルゴリズムを導入。
- Why it matters: モーションとシーンのデカップリングによる物理的不整合を解消し、より現実に即したヒューマノイドロボットの動作生成を可能にする。

## 9. Semantic-Guided 3D Gaussian Splatting for Transient Object Removal
- arXiv: http://arxiv.org/abs/2602.15516v1
- PDF: https://arxiv.org/pdf/2602.15516v1
- Authors: Aditi Prabakaran, Priyesh Shukla
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.

</details>

**LLM Summary**

- What: 3Dガウシアン スプラッティング（3DGS）における一時的なオブジェクト（移動物体）によるゴーストアーティファクトを除去するための、セマンティックガイド付きフレームワークを提案。
- Novelty: Vision-Languageモデル（VLM）を用いたカテゴリ認識により、モーションベースの手法では困難な視差の曖昧さを解消。ガウシアンごとにCLIP類似度スコアを蓄積し、閾値を超えたガウシアンのオパシティを正規化・プルーニング。
- Why it matters: 3DGS再構成の品質を向上させ、一時的なオブジェクトの除去を効率的かつメモリ効率良く行う。

## 10. ActionCodec: What Makes for Good Action Tokenizers
- arXiv: http://arxiv.org/abs/2602.15397v1
- PDF: https://arxiv.org/pdf/2602.15397v1
- Authors: Zibin Dong, Yicheng Liu, Shiduo Zhang, Baijun Ye, Yifu Yuan, Fei Ni, Jingjing Gong, Xipeng Qiu...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.

</details>

**LLM Summary**

- What: Vision-Language-Action（VLA）モデルの最適化に焦点を当てた、新しいアクショントークナイザー「ActionCodec」を提案。
- Novelty: アクショントークナイザーの設計原則を、再構成忠実度だけでなくVLA最適化の観点から確立。情報理論的洞察に基づき、時間的トークン重複の最大化、語彙冗長性の最小化、マルチモーダル相互情報の強化、トークン独立性を設計指針とする。
- Why it matters: VLAモデルのトレーニング効率とパフォーマンスを大幅に向上させ、ロボット制御タスクにおける指示追従能力を高める。

## 11. World-Model-Augmented Web Agents with Action Correction
- arXiv: http://arxiv.org/abs/2602.15384v1
- PDF: https://arxiv.org/pdf/2602.15384v1
- Authors: Zhouzhou Shen, Xueyu Hu, Xiyun Li, Tianqing Fang, Juncheng Li, Shengyu Zhang
- Keyword score: 1 / hits: multi-agent

<details><summary>Abstract</summary>

Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.

</details>

**LLM Summary**

- What: Webエージェントが、世界モデルとの協調、結果のシミュレーション、フィードバックによる行動修正を統合することで、Webタスクの自動化における推論能力とリスク認識の限界を克服する手法を提案した。
- Novelty: マルチエージェント協調プロセス、世界モデルによる戦略的ガイダンス、環境状態遷移ダイナミクスを活用した行動提案、リスク認識のための2段階推論チェーン（世界モデルによる結果シミュレーションとジャッジモデルによる修正フィードバック）を導入した。
- Why it matters: Webエージェントのタスク実行における失敗を減らし、より堅牢で信頼性の高い自動化を実現する。

## 12. Automatic Funny Scene Extraction from Long-form Cinematic Videos
- arXiv: http://arxiv.org/abs/2602.15381v1
- PDF: https://arxiv.org/pdf/2602.15381v1
- Authors: Sibendu Paul, Haotian Jiang, Caren Chen
- Keyword score: 1 / hits: streaming

<details><summary>Abstract</summary>

Automatically extracting engaging and high-quality humorous scenes from cinematic titles is pivotal for creating captivating video previews and snackable content, boosting user engagement on streaming platforms. Long-form cinematic titles, with their extended duration and complex narratives, challenge scene localization, while humor's reliance on diverse modalities and its nuanced style add further complexity. This paper introduces an end-to-end system for automatically identifying and ranking humorous scenes from long-form cinematic titles, featuring shot detection, multimodal scene localization, and humor tagging optimized for cinematic content. Key innovations include a novel scene segmentation approach combining visual and textual cues, improved shot representations via guided triplet mining, and a multimodal humor tagging framework leveraging both audio and text. Our system achieves an 18.3% AP improvement over state-of-the-art scene detection on the OVSD dataset and an F1 score of 0.834 for detecting humor in long text. Extensive evaluations across five cinematic titles demonstrate 87% of clips extracted by our pipeline are intended to be funny, while 98% of scenes are accurately localized. With successful generalization to trailers, these results showcase the pipeline's potential to enhance content creation workflows, improve user engagement, and streamline snackable content generation for diverse cinematic media formats.

</details>

**LLM Summary**

- What: 長編映画から面白シーンを自動的に抽出し、ランク付けするエンドツーエンドシステムを開発した。
- Novelty: 視覚的・テキスト的情報を組み合わせた新しいシーンセグメンテーション手法、ガイド付きトリプレットマイニングによるショット表現の改善、音声とテキストの両方を活用したマルチモーダルユーモアタグ付けフレームワークを導入した。
- Why it matters: 動画プレビューやスナックコンテンツ作成に役立ち、ストリーミングプラットフォームでのユーザーエンゲージメントを向上させる。

## 13. Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs
- arXiv: http://arxiv.org/abs/2602.15318v1
- PDF: https://arxiv.org/pdf/2602.15318v1
- Authors: Libo Zhang, Zhaoning Zhang, Wangyang Hong, Peng Qiao, Dongsheng Li
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.

</details>

**LLM Summary**

- What: Video LLMの推論を高速化するために、Sparrowというフレームワークを提案した。これは、テキストアンカーウィンドウアテンションと視覚的・意味的なグリンシングを活用する。
- Novelty: 隠れ状態の再利用による視覚的に意識したテキストアンカーウィンドウアテンション、中間層の視覚状態ブリッジング、マルチトークン予測戦略を導入し、ドラフトモデルの性能低下を防ぐ。
- Why it matters: Video LLMの推論速度を大幅に向上させ、大規模な視覚的トークンを扱う場合でも効率的な処理を可能にする。

## 14. Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories
- arXiv: http://arxiv.org/abs/2602.15154v1
- PDF: https://arxiv.org/pdf/2602.15154v1
- Authors: Praditha Alwis, Soumyadeep Chandra, Deepak Ravikumar, Kaushik Roy
- Keyword score: 1 / hits: action recognition

<details><summary>Abstract</summary>

High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

</details>

**LLM Summary**

- What: トレーニング中のフレームごとの累積サンプル損失（CSL）の軌跡を分析することで、動画データセットにおけるアノテーションエラー（誤ラベリングや順序誤り）を検出するモデルに依存しない新しい手法を提案した。
- Novelty: フレームの学習可能性を示す動的なフィンガープリントとしてCSLを利用し、誤ったアノテーションを持つフレームは学習が困難で高い、または不規則な損失パターンを示すことを利用する。
- Why it matters: 高品質な動画データセットの構築を支援し、アクション認識などのタスクにおけるモデルのロバスト性を向上させる。
