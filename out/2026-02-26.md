# Daily CV Digest (2026-02-26)

- Total: 18

## 1. PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning
- arXiv: http://arxiv.org/abs/2602.21992v1
- PDF: https://arxiv.org/pdf/2602.21992v1
- Authors: Zekai Lin, Xu Zheng
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

360 panoramic images are increasingly used in virtual reality, autonomous driving, and robotics for holistic scene understanding. However, current Vision-Language Models (VLMs) struggle with 3D spatial reasoning on Equirectangular Projection (ERP) images due to geometric distortion and limited 3D supervision. We introduce PanoEnv, a large-scale VQA benchmark built from synthetic 3D environments, containing 14.8K questions across five categories (e.g., relative position, volume comparison) grounded in accurate 3D annotations including depth, segmentation, and bounding boxes. Benchmarking 14 state-of-the-art VLMs reveals limited 3D understanding, achieving only 49.34% overall accuracy and 8.36% on open-ended (OE) questions. To enhance 3D reasoning, we propose a reinforcement learning post-training framework based on Group Relative Policy Optimization (GRPO) with a ground-truth-guided reward that incorporates five geometry-aware strategies such as distance tolerance and spatial consistency. A two-stage curriculum further mitigates catastrophic forgetting: Stage 1 trains on structured tasks (true/false and multiple choice), and Stage 2 fine-tunes on mixed open-ended data to improve generalization. Our 7B model achieves new state-of-the-art performance, improving overall accuracy to 52.93% (+3.59%) and open-ended accuracy to 14.83% while maintaining structured-task performance. It also achieves top semantic evaluation scores (Q-Score 6.24, P-Score 5.95), surpassing 32B models. These results demonstrate that PanoEnv-QA and our curriculum-based RL framework effectively instill 3D spatial intelligence in VLMs for omnidirectional perception.

</details>

**LLM Summary**

- What: 360度パノラマ画像における3D空間知能を、強化学習を用いて探求するフレームワーク「PanoEnv」を提案。大規模なVQAベンチマークと、幾何学的な歪みに対応するための強化学習後学習フレームワークを開発。
- Novelty: パノラマ画像特有の幾何学的歪みを考慮した3D空間推論能力の向上。強化学習による、真値誘導型の報酬を用いたモデルの最適化。
- Why it matters: VR、自動運転、ロボティクスなど、パノラマ画像を用いた3D空間理解の精度向上に貢献。

## 2. XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression
- arXiv: http://arxiv.org/abs/2602.21780v1
- PDF: https://arxiv.org/pdf/2602.21780v1
- Authors: Zunhai Su, Weihao Ye, Hansen Feng, Keyu Fan, Jing Zhang, Dahai Yu, Zhengwu Liu, Ngai Wong
- Keyword score: 3 / hits: streaming

<details><summary>Abstract</summary>

Learning-based 3D visual geometry models have significantly advanced with the advent of large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention to deliver robust and efficient streaming 3D reconstruction. However, it suffers from unbounded growth in the Key-Value (KV) cache due to the massive influx of vision tokens from multi-image and long-video inputs, leading to increased memory consumption and inference latency as input frames accumulate. This ultimately limits its scalability for long-horizon applications. To address this gap, we propose XStreamVGGT, a tuning-free approach that seamlessly integrates pruning and quantization to systematically compress the KV cache, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs generated from multi-frame inputs are initially pruned to conform to a fixed KV memory budget using an efficient token-importance identification mechanism that maintains full compatibility with high-performance attention kernels (e.g., FlashAttention). Additionally, leveraging the inherent distribution patterns of KV tensors, we apply dimension-adaptive KV quantization within the pruning pipeline to further minimize memory overhead while preserving numerical accuracy. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\times$ and accelerating inference by 5.48$\times$, enabling practical and scalable streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.

</details>

**LLM Summary**

- What: ストリーミング3D再構成モデルStreamVGGTのKVキャッシュのメモリ効率を劇的に改善する「XStreamVGGT」を提案。KVキャッシュのプルーニングと量子化を組み合わせたチューニング不要のアプローチ。
- Novelty: KVキャッシュの圧縮により、長時間の入力に対してもメモリ消費と推論遅延を大幅に削減。既存のAttentionカーネルとの完全な互換性を維持。
- Why it matters: 長期的なアプリケーションにおける3D再構成の拡張性を向上させ、より大規模なデータセットや長時間の動画処理を可能にする。

## 3. Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning
- arXiv: http://arxiv.org/abs/2602.21670v1
- PDF: https://arxiv.org/pdf/2602.21670v1
- Authors: Tomoya Kawabe, Rin Takano
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large language models (LLMs) can interpret instructions and propose plans but may hallucinate or produce infeasible actions. We present a hierarchical multi-agent LLM-based planner with prompt optimization: an upper layer decomposes tasks and assigns them to lower-layer agents, which generate PDDL problems solved by a classical planner. When plans fail, the system applies TextGrad-inspired textual-gradient updates to optimize each agent's prompt and thereby improve planning accuracy. In addition, meta-prompts are learned and shared across agents within the same layer, enabling efficient prompt optimization in multi-agent settings. On the MAT-THOR benchmark, our planner achieves success rates of 0.95 on compound tasks, 0.84 on complex tasks, and 0.60 on vague tasks, improving over the previous state-of-the-art LaMMA-P by 2, 7, and 15 percentage points respectively. An ablation study shows that the hierarchical structure, prompt optimization, and meta-prompt sharing contribute roughly +59, +37, and +4 percentage points to the overall success rate.

</details>

**LLM Summary**

- What: マルチロボットタスク計画のための、階層的なLLMベースのマルチエージェントフレームワークを提案。プロンプト最適化により、自然言語指示をロボットが実行可能なアクションに分解。
- Novelty: LLMによる計画生成と古典的プランナーの組み合わせ、およびTextGradに触発されたプロンプト最適化手法。メタプロンプトの学習と共有。
- Why it matters: 曖昧な指示や長期間のミッションに対しても、より正確で実行可能なマルチロボットタスク計画を実現。

## 4. CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning
- arXiv: http://arxiv.org/abs/2602.21655v1
- PDF: https://arxiv.org/pdf/2602.21655v1
- Authors: Zhijiang Tang, Linhua Wang, Jiaxin Qi, Weihao Jiang, Peng Hou, Anxiang Zeng, Jianqiang Huang
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

Image captioning remains a fundamental task for vision language understanding, yet ground-truth supervision still relies predominantly on human-annotated references. Because human annotations reflect subjective preferences and expertise, ground-truth captions are often incomplete or even incorrect, which in turn limits caption models. We argue that caption quality should be assessed by two objective aspects: completeness (does the caption cover all salient visual facts?) and correctness (are the descriptions true with respect to the image?). To this end, we introduce CCCaption: a dual-reward reinforcement learning framework with a dedicated fine-tuning corpus that explicitly optimizes these properties to generate \textbf{C}omplete and \textbf{C}orrect \textbf{Captions}. For completeness, we use diverse LVLMs to disentangle the image into a set of visual queries, and reward captions that answer more of these queries, with a dynamic query sampling strategy to improve training efficiency. For correctness, we penalize captions that contain hallucinations by validating the authenticity of sub-caption queries, which are derived from the caption decomposition. Our symmetric dual-reward optimization jointly maximizes completeness and correctness, guiding models toward captions that better satisfy these objective criteria. Extensive experiments across standard captioning benchmarks show consistent improvements, offering a principled path to training caption models beyond human-annotation imitation.

</details>

**LLM Summary**

- What: 画像キャプション生成において、完全性（Completeness）と正確性（Correctness）を同時に最適化する「CCCaption」というデュアル報酬強化学習フレームワークを提案。
- Novelty: 画像の視覚的要素を網羅する「完全性」と、生成されたキャプションの真実性を検証する「正確性」を、それぞれ独立した報酬として導入。
- Why it matters: より網羅的で、誤りのない、高品質な画像キャプション生成を実現し、画像理解タスクの精度向上に貢献。

## 5. ADM-DP: Adaptive Dynamic Modality Diffusion Policy through Vision-Tactile-Graph Fusion for Multi-Agent Manipulation
- arXiv: http://arxiv.org/abs/2602.21622v1
- PDF: https://arxiv.org/pdf/2602.21622v1
- Authors: Enyi Wang, Wen Fan, Dandan Zhang
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

Multi-agent robotic manipulation remains challenging due to the combined demands of coordination, grasp stability, and collision avoidance in shared workspaces. To address these challenges, we propose the Adaptive Dynamic Modality Diffusion Policy (ADM-DP), a framework that integrates vision, tactile, and graph-based (multi-agent pose) modalities for coordinated control. ADM-DP introduces four key innovations. First, an enhanced visual encoder merges RGB and point-cloud features via Feature-wise Linear Modulation (FiLM) modulation to enrich perception. Second, a tactile-guided grasping strategy uses Force-Sensitive Resistor (FSR) feedback to detect insufficient contact and trigger corrective grasp refinement, improving grasp stability. Third, a graph-based collision encoder leverages shared tool center point (TCP) positions of multiple agents as structured kinematic context to maintain spatial awareness and reduce inter-agent interference. Fourth, an Adaptive Modality Attention Mechanism (AMAM) dynamically re-weights modalities according to task context, enabling flexible fusion. For scalability and modularity, a decoupled training paradigm is employed in which agents learn independent policies while sharing spatial information. This maintains low interdependence between agents while retaining collective awareness. Across seven multi-agent tasks, ADM-DP achieves 12-25% performance gains over state-of-the-art baselines. Ablation studies show the greatest improvements in tasks requiring multiple sensory modalities, validating our adaptive fusion strategy and demonstrating its robustness for diverse manipulation scenarios.

</details>

**LLM Summary**

- What: マルチエージェントロボット操作のための、適応的動的モダリティ拡散ポリシー「ADM-DP」を提案。ビジョン、触覚、グラフベースの情報を融合。
- Novelty: FiLMを用いたビジョンエンコーダーの強化、触覚フィードバックによる把持安定性の向上、グラフベースの衝突エンコーダー、タスクコンテキストに応じてモダリティの重みを動的に調整するAMAM。
- Why it matters: 協調性、把持安定性、衝突回避といったマルチエージェントロボット操作の課題を解決し、より安全で効率的な操作を実現。

## 6. Solaris: Building a Multiplayer Video World Model in Minecraft
- arXiv: http://arxiv.org/abs/2602.22208v1
- PDF: https://arxiv.org/pdf/2602.22208v1
- Authors: Georgy Savva, Oscar Michel, Daohan Lu, Suppakit Waiwitlikhit, Timothy Meehan, Dhairya Mishra, Srivats Poddar, Jack Lu...
- Keyword score: 1 / hits: multi-agent

<details><summary>Abstract</summary>

Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.

</details>

**LLM Summary**

- What: Minecraftにおけるマルチエージェントのビデオワールドモデル「Solaris」を開発した。
- Novelty: マルチエージェントの相互作用と複数視点の観測をシミュレートできる。大規模なマルチエージェントデータ収集システムと、シングルプレイヤーからマルチプレイヤーへと段階的に学習を進める新しいトレーニングパイプラインを提案した。
- Why it matters: 現実世界の複雑なマルチエージェント環境をシミュレートし、より高度なAIエージェントの開発に貢献する。

## 7. Lumosaic: Hyperspectral Video via Active Illumination and Coded-Exposure Pixels
- arXiv: http://arxiv.org/abs/2602.22140v1
- PDF: https://arxiv.org/pdf/2602.22140v1
- Authors: Dhruv Verma, Andrew Qiu, Roberto Rangel, Ayandev Barman, Hao Yang, Chenjia Hu, Fengqi Zhang, Roman Genov...
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

We present Lumosaic, a compact active hyperspectral video system designed for real-time capture of dynamic scenes. Our approach combines a narrowband LED array with a coded-exposure-pixel (CEP) camera capable of high-speed, per-pixel exposure control, enabling joint encoding of scene information across space, time, and wavelength within each video frame. Unlike passive snapshot systems that divide light across multiple spectral channels simultaneously and assume no motion during a frame's exposure, Lumosaic actively synchronizes illumination and pixel-wise exposure, improving photon utilization and preserving spectral fidelity under motion. A learning-based reconstruction pipeline then recovers 31-channel hyperspectral (400-700 nm) video at 30 fps and VGA resolution, producing temporally coherent and spectrally accurate reconstructions. Experiments on synthetic and real data demonstrate that Lumosaic significantly improves reconstruction fidelity and temporal stability over existing snapshot hyperspectral imaging systems, enabling robust hyperspectral video across diverse materials and motion conditions.

</details>

**LLM Summary**

- What: アクティブ照明とコード化露光ピクセルを用いたコンパクトなハイパースペクトルビデオシステム「Lumosaic」を提案した。
- Novelty: ナローバンドLEDアレイと高速なピクセルごとの露光制御が可能なカメラを組み合わせ、空間、時間、波長情報を各フレームに同時にエンコードする。これにより、動きのあるシーンでも高い光子利用効率とスペクトル忠実度を実現する。
- Why it matters: リアルタイムでの動的なシーンのハイパースペクトルビデオキャプチャを可能にし、材料科学、リモートセンシング、医療などの分野での応用が期待される。

## 8. World Guidance: World Modeling in Condition Space for Action Generation
- arXiv: http://arxiv.org/abs/2602.22010v1
- PDF: https://arxiv.org/pdf/2602.22010v1
- Authors: Yue Su, Sijin Chen, Haixin Shi, Mingyu Liu, Zhengshen Zhang, Ningyuan Huang, Weiheng Zhong, Zhengbang Zhu...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/

</details>

**LLM Summary**

- What: アクション生成のためのワールドモデリングを条件空間で行うフレームワーク「WoG (World Guidance)」を提案した。
- Novelty: 将来の観測を圧縮された条件にマッピングし、それをアクション推論パイプラインに注入する。VLAモデルは、これらの圧縮された条件と将来のアクションを同時に予測するように訓練される。
- Why it matters: よりきめ細やかなアクション生成と優れた汎化能力を実現し、人間による操作ビデオから効果的に学習できる。シミュレーションおよび実世界環境でのアクション生成タスクの性能を向上させる。

## 9. UniVBench: Towards Unified Evaluation for Video Foundation Models
- arXiv: http://arxiv.org/abs/2602.21835v1
- PDF: https://arxiv.org/pdf/2602.21835v1
- Authors: Jianhui Wei, Xiaotian Zhang, Yichen Li, Yuan Wang, Yan Zhang, Ziyi Chen, Zhihang Tang, Wei Xu...
- Keyword score: 1 / hits: video understanding

<details><summary>Abstract</summary>

Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.

</details>

**LLM Summary**

- What: ビデオ基盤モデルの統一的な評価のためのベンチマーク「UniVBench」を提案した。
- Novelty: ビデオ理解、生成、編集、および新たに提案されたビデオ再構成タスクを含む4つのコア能力を評価する。200本の高品質で多様なマルチショットビデオを使用し、詳細なキャプション、編集指示、参照画像を含む。
- Why it matters: 既存の断片的で限定的な評価ベンチマークのギャップを埋め、ビデオ基盤モデルの統合された能力を包括的に評価するための標準を提供する。

## 10. DexRepNet++: Learning Dexterous Robotic Manipulation with Geometric and Spatial Hand-Object Representations
- arXiv: http://arxiv.org/abs/2602.21811v1
- PDF: https://arxiv.org/pdf/2602.21811v1
- Authors: Qingtao Liu, Zhengnan Sun, Yu Cui, Haoming Li, Gaofeng Li, Lin Shao, Jiming Chen, Qi Ye
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Robotic dexterous manipulation is a challenging problem due to high degrees of freedom (DoFs) and complex contacts of multi-fingered robotic hands. Many existing deep reinforcement learning (DRL) based methods aim at improving sample efficiency in high-dimensional output action spaces. However, existing works often overlook the role of representations in achieving generalization of a manipulation policy in the complex input space during the hand-object interaction. In this paper, we propose DexRep, a novel hand-object interaction representation to capture object surface features and spatial relations between hands and objects for dexterous manipulation skill learning. Based on DexRep, policies are learned for three dexterous manipulation tasks, i.e. grasping, in-hand reorientation, bimanual handover, and extensive experiments are conducted to verify the effectiveness. In simulation, for grasping, the policy learned with 40 objects achieves a success rate of 87.9% on more than 5000 unseen objects of diverse categories, significantly surpassing existing work trained with thousands of objects; for the in-hand reorientation and handover tasks, the policies also boost the success rates and other metrics of existing hand-object representations by 20% to 40%. The grasp policies with DexRep are deployed to the real world under multi-camera and single-camera setups and demonstrate a small sim-to-real gap.

</details>

**LLM Summary**

- What: 幾何学的および空間的な手と物体の表現を用いた、器用なロボット操作学習のための「DexRepNet++」を提案した。
- Novelty: 手と物体の相互作用におけるオブジェクト表面の特徴と空間関係を捉える新しい表現「DexRep」を導入した。これにより、高次元のアクション空間におけるサンプル効率と、複雑な入力空間での操作ポリシーの汎化能力を向上させる。
- Why it matters: グリッピング、インハンド再配向、バイマニュアルハンドオーバーなどの器用なロボット操作タスクにおいて、より高い成功率と優れた汎化能力を達成し、ロボットの操作能力を向上させる。

## 11. Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild
- arXiv: http://arxiv.org/abs/2602.21736v1
- PDF: https://arxiv.org/pdf/2602.21736v1
- Authors: Hao Luo, Ye Wang, Wanpeng Zhang, Haoqi Yuan, Yicheng Feng, Haiweng Xu, Sipeng Zheng, Zongqing Lu
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (>2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data.

</details>

**LLM Summary**

- What: ロボットの操作タスクを学習するための、人間が生成した多様なデータ（ラボ内および実世界の映像）から、関節が整列した潜在的なアクションを学習する事前学習フレームワーク「JALA」を提案。
- Novelty: 視覚的な動的再構築を回避し、逆動力学と実際のアクションの両方に整列した予測アクション埋め込みを学習することで、異種人間データからの学習を可能にする。
- Why it matters: 大規模で多様なロボットデータが不足している状況で、よりスケーラブルなVLA（Vision-Language-Action）の事前学習方法を提供し、ロボット操作の性能を向上させる。

## 12. Primary-Fine Decoupling for Action Generation in Robotic Imitation
- arXiv: http://arxiv.org/abs/2602.21684v1
- PDF: https://arxiv.org/pdf/2602.21684v1
- Authors: Xiaohan Lei, Min Wang, Wengang Zhou, Xingyu Lu, Houqiang Li
- Keyword score: 1 / hits: imitation learning

<details><summary>Abstract</summary>

Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions. To address these limitations, we propose Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework that decouples coarse action consistency from fine-grained variations. First, we compress action chunks into a small set of discrete modes, enabling a lightweight policy to select consistent coarse modes and avoid mode bouncing. Second, a mode conditioned MeanFlow policy is learned to generate high-fidelity continuous actions. Theoretically, we prove PF-DAG's two-stage design achieves a strictly lower MSE bound than single-stage generative policies. Empirically, PF-DAG outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. It further generalizes to real-world tactile dexterous manipulation tasks. Our work demonstrates that explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation.

</details>

**LLM Summary**

- What: ロボットの模倣学習におけるアクション生成の課題に対処するため、粗いアクションの一貫性と細かいバリエーションを分離する2段階フレームワーク「PF-DAG」を提案。
- Novelty: アクションチャンクを離散モードに圧縮し、軽量ポリシーでモードを選択した後、モード条件付きのMeanFlowポリシーで高忠実度の連続アクションを生成する。
- Why it matters: 既存の単一ステージ生成ポリシーよりも低いMSEバウンドを達成し、Adroit、DexArt、MetaWorldベンチマークの多数のタスクで最先端のベースラインを上回る性能を示す。

## 13. LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies
- arXiv: http://arxiv.org/abs/2602.21531v1
- PDF: https://arxiv.org/pdf/2602.21531v1
- Authors: Yue Yang, Shuo Cheng, Yu Fang, Homanga Bharadhwaj, Mingyu Ding, Gedas Bertasius, Daniel Szafir
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/.

</details>

**LLM Summary**

- What: 複数の運動学的構造変化を伴う長期間の操作タスクを、学習データに存在しない新しいタスクに対してもゼロショットで汎化できるモジュラーフレームワーク「LiLo-VLA」を提案。
- Novelty: 移動と相互作用を分離し、オブジェクト中心のVLAを使用する相互作用モジュールにより、関連性のない視覚的特徴に対する頑健性と空間構成に対する不変性を確保する。
- Why it matters: 複雑な長期間操作タスクにおけるエンドツーエンドアプローチの連鎖的失敗を軽減し、動的な再計画とスキル再利用による頑健な失敗回復を可能にする。

## 14. See It, Say It, Sorted: An Iterative Training-Free Framework for Visually-Grounded Multimodal Reasoning in LVLMs
- arXiv: http://arxiv.org/abs/2602.21497v1
- PDF: https://arxiv.org/pdf/2602.21497v1
- Authors: Yongchang Zhang, Xianzheng Ma, Tianyi Liu, Guangquan Zhou, Yang Chen
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Recent large vision-language models (LVLMs) have demonstrated impressive reasoning ability by generating long chain-of-thought (CoT) responses. However, CoT reasoning in multimodal contexts is highly vulnerable to visual hallucination propagation: once an intermediate reasoning step becomes inconsistent with the visual evidence, subsequent steps-even if logically valid-can still lead to incorrect final answers. Existing solutions attempt to mitigate this issue by training models to "think with images" via reinforcement learning (RL). While effective, these methods are costly, model-specific, and difficult to generalize across architectures. Differently, we present a lightweight method that bypasses RL training and provides an iterative, training-free, plug-and-play framework for visually-grounded multimodal reasoning. Our key idea is to supervise each reasoning step at test time with visual evidence, ensuring that every decoded token is justified by corresponding visual cues. Concretely, we construct a textual visual-evidence pool that guides the model's reasoning generation. When existing evidence is insufficient, a visual decider module dynamically extracts additional relevant evidence from the image based on the ongoing reasoning context, expanding the pool until the model achieves sufficient visual certainty to terminate reasoning and produce the final answer. Extensive experiments on multiple LVLM backbones and benchmarks demonstrate the effectiveness of our approach. Our method achieves 16.5%-29.5% improvements on TreeBench and 13.7% RH-AUC gains on RH-Bench, substantially reducing hallucination rates while improving reasoning accuracy without additional training.

</details>

**LLM Summary**

- What: 大規模視覚言語モデル（LVLMs）における視覚的根拠のあるマルチモーダル推論のための、反復的かつ学習不要なプラグアンドプレイフレームワークを提案。
- Novelty: 各推論ステップをテスト時に視覚的証拠で監視し、生成されるトークンが対応する視覚的手がかりによって正当化されるようにする。
- Why it matters: 視覚的幻覚の伝播を防ぎ、コストのかかる強化学習トレーニングを回避しながら、LVLMsの推論能力を向上させる。

## 15. VLA Knows Its Limits
- arXiv: http://arxiv.org/abs/2602.21445v1
- PDF: https://arxiv.org/pdf/2602.21445v1
- Authors: Haoxuan Wang, Gengyu Zhang, Yan Yan, Ramana Rao Kompella, Gaowen Liu
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Action chunking has recently emerged as a standard practice in flow-based Vision-Language-Action (VLA) models. However, the effect and choice of the execution horizon - the number of actions to be executed from each predicted chunk - remains underexplored. In this work, we first show that varying the execution horizon leads to substantial performance deviations, with performance initially improving and then declining as the horizon increases. To uncover the reasons, we analyze the cross- and self-attention weights in flow-based VLAs and reveal two key phenomena: (i) intra-chunk actions attend invariantly to vision-language tokens, limiting adaptability to environmental changes; and (ii) the initial and terminal action tokens serve as stable anchors, forming latent centers around which intermediate actions are organized. Motivated by these insights, we interpret action self-attention weights as a proxy for the model's predictive limit and propose AutoHorizon, the first test-time method that dynamically estimates the execution horizon for each predicted action chunk to adapt to changing perceptual conditions. Across simulated and real-world robotic manipulation tasks, AutoHorizon is performant, incurs negligible computational overhead, and generalizes across diverse tasks and flow-based models.

</details>

**LLM Summary**

- What: VLAモデルにおけるアクションチャンクの実行ホライゾン（各チャンクから実行されるアクション数）の影響を分析し、テスト時に実行ホライゾンを動的に推定する「AutoHorizon」を提案。
- Novelty: アクション自己アテンション重みをモデルの予測限界の代理として解釈し、変化する知覚条件に適応する。
- Why it matters: 実行ホライゾンの選択が性能に大きく影響することを示し、シミュレーションおよび実世界のロボット操作タスクで性能を向上させる。

## 16. Synergizing Understanding and Generation with Interleaved Analyzing-Drafting Thinking
- arXiv: http://arxiv.org/abs/2602.21435v1
- PDF: https://arxiv.org/pdf/2602.21435v1
- Authors: Shengqiong Wu, Bobo Li, Xinkai Wang, Xiangtai Li, Lei Cui, Furu Wei, Shuicheng Yan, Hao Fei...
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Unified Vision-Language Models (UVLMs) aim to advance multimodal learning by supporting both understanding and generation within a single framework. However, existing approaches largely focus on architectural unification while overlooking the need for explicit interaction between the two capabilities during task solving. As a result, current models treat understanding and generation as parallel skills rather than synergistic processes. To achieve real synergy, we introduce the interleaved Analyzing-Drafting problem-solving loop (AD-Loop), a new think paradigm that dynamically alternates between analytic and drafting operations. By interleaving textual thoughts with visual thoughts, AD-Loop enables models to iteratively refine both comprehension and outputs, fostering genuine synergy. To train this mechanism, we design a two-stage strategy: supervised learning on interleaved thought data to initialize alternation, followed by reinforcement learning to promote adaptive and autonomous control. Extensive experiments demonstrate that AD-Loop consistently improves performance across standard benchmarks for both understanding and generation, with strong transferability to various UVLMs architectures. Visual analyses further validate the effectiveness of implicit visual thoughts. These results highlight AD-Loop as a principled and broadly applicable strategy for synergizing comprehension and creation. The project page is at https://sqwu.top/AD-Loop.

</details>

**LLM Summary**

- What: 視覚と言語の理解と生成を統合する統一型ビジョン言語モデル（UVLM）において、分析と生成の思考プロセスを交互に繰り返す「AD-Loop」という新しい思考パラダイムを提案した。
- Novelty: 分析と生成の能力を単に統合するだけでなく、それらを動的に交互に実行することで、真の相乗効果を生み出すことを目指した点。
- Why it matters: 視覚と言語の理解と生成の両方のタスクにおいて、モデルの性能を向上させ、様々なUVLMへの応用が期待できる。

## 17. Exploring Vision-Language Models for Open-Vocabulary Zero-Shot Action Segmentation
- arXiv: http://arxiv.org/abs/2602.21406v1
- PDF: https://arxiv.org/pdf/2602.21406v1
- Authors: Asim Unmesh, Kaki Ramesh, Mayank Patel, Rahul Jain, Karthik Ramani
- Keyword score: 1 / hits: temporal action

<details><summary>Abstract</summary>

Temporal Action Segmentation (TAS) requires dividing videos into action segments, yet the vast space of activities and alternative breakdowns makes collecting comprehensive datasets infeasible. Existing methods remain limited to closed vocabularies and fixed label sets. In this work, we explore the largely unexplored problem of Open-Vocabulary Zero-Shot Temporal Action Segmentation (OVTAS) by leveraging the strong zero-shot capabilities of Vision-Language Models (VLMs). We introduce a training-free pipeline that follows a segmentation-by-classification design: Frame-Action Embedding Similarity (FAES) matches video frames to candidate action labels, and Similarity-Matrix Temporal Segmentation (SMTS) enforces temporal consistency. Beyond proposing OVTAS, we present a systematic study across 14 diverse VLMs, providing the first broad analysis of their suitability for open-vocabulary action segmentation. Experiments on standard benchmarks show that OVTAS achieves strong results without task-specific supervision, underscoring the potential of VLMs for structured temporal understanding.

</details>

**LLM Summary**

- What: オープンボキャブラリー・ゼロショット・アクションセグメンテーション（OVTAS）という、動画内のアクションを事前に定義されたラベルセットに縛られずに識別するタスクを提案し、ビジョン言語モデル（VLM）を活用した学習不要なパイプラインを開発した。
- Novelty: 既存の閉じた語彙の制限を超え、VLMのゼロショット能力を利用して、未知のアクションに対してもセグメンテーションを可能にした点。
- Why it matters: 大規模な動画データセットの収集が困難なアクションセグメンテーションの課題を解決し、VLMが構造化された時間的理解に貢献できる可能性を示す。

## 18. Tool-R0: Self-Evolving LLM Agents for Tool-Learning from Zero Data
- arXiv: http://arxiv.org/abs/2602.21320v1
- PDF: https://arxiv.org/pdf/2602.21320v1
- Authors: Emre Can Acikgoz, Cheng Qian, Jonas Hübotter, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Large language models (LLMs) are becoming the foundation for autonomous agents that can use tools to solve complex tasks. Reinforcement learning (RL) has emerged as a common approach for injecting such agentic capabilities, but typically under tightly controlled training setups. It often depends on carefully constructed task-solution pairs and substantial human supervision, which creates a fundamental obstacle to open-ended self-evolution toward superintelligent systems. In this paper, we propose Tool-R0 framework for training general purpose tool-calling agents from scratch with self-play RL, under a zero-data assumption. Initialized from the same base LLM, Tool-R0 co-evolves a Generator and a Solver with complementary rewards: one proposes targeted challenging tasks at the other's competence frontier and the other learns to solve them with real-world tool calls. This creates a self-evolving cycle that requires no pre-existing tasks or datasets. Evaluation on different tool-use benchmarks show that Tool-R0 yields 92.5 relative improvement over the base model and surpasses fully supervised tool-calling baselines under the same setting. Our work further provides empirical insights into self-play LLM agents by analyzing co-evolution, curriculum dynamics, and scaling behavior.

</details>

**LLM Summary**

- What: ゼロデータ（事前学習データなし）の状況下で、自己進化するLLMエージェントがツール学習を行うための「Tool-R0」フレームワークを提案した。
- Novelty: GeneratorとSolverが互いに補完的な報酬で協調進化する自己対戦型強化学習により、タスクやデータセットなしで汎用的なツール利用エージェントをゼロから学習させる点。
- Why it matters: 人間の介入や事前データに依存しない自律的なエージェント開発を可能にし、超知能システムへの道を開く可能性がある。
