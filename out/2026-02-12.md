# Daily CV Digest (2026-02-12)

- Total: 20

## 1. RISE: Self-Improving Robot Policy with Compositional World Model
- arXiv: http://arxiv.org/abs/2602.11075v1
- PDF: https://arxiv.org/pdf/2602.11075v1
- Authors: Jiazhi Yang, Kunyang Lin, Jinwei Li, Wencong Zhang, Tianwei Lin, Longyan Wu, Zhizhong Su, Hao Zhao...
- Keyword score: 5 / hits: vision-language-action, robot policy, reinforcement learning

<details><summary>Abstract</summary>

Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model, and (ii) evaluates imagined outcomes with a progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts, estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively.

</details>

**LLM Summary**

- What: ロボットの強化学習のための、想像力を活用したスケーラブルなフレームワーク「RISE」を提案。多視点未来予測と進捗評価を行う構成的ワールドモデルを核とし、物理的相互作用なしにポリシーを自己改善する。
- Novelty: 構成的ワールドモデル（制御可能なダイナミクスモデルと進捗価値モデル）と、想像空間での閉ループ自己改善パイプライン。
- Why it matters: 接触が多く動的な操作タスクにおけるロボットポリシーの堅牢性と効率を向上させる。

## 2. Found-RL: foundation model-enhanced reinforcement learning for autonomous driving
- arXiv: http://arxiv.org/abs/2602.10458v1
- PDF: https://arxiv.org/pdf/2602.10458v1
- Authors: Yansong Qu, Zihao Sheng, Zilin Huang, Jiancong Chen, Yuhao Luo, Tianyi Wang, Yiheng Feng, Samuel Labi...
- Keyword score: 4 / hits: real-time, reinforcement learning

<details><summary>Abstract</summary>

Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>

**LLM Summary**

- What: 財団モデル（特にVLM）を活用し、自律走行における強化学習のサンプル効率と解釈可能性を向上させるプラットフォーム「Found-RL」を提案。
- Novelty: 非同期バッチ推論フレームワークによるVLM推論遅延の解消、Value-Margin Regularization (VMR) と Advantage-Weighted Action Guidance (AWAG) によるVLM知識の蒸留、CLIPによる報酬整形。
- Why it matters: 高頻度なRL学習ループで財団モデルを効率的に利用し、自律走行の性能と信頼性を向上させる。

## 3. RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation
- arXiv: http://arxiv.org/abs/2602.10980v1
- PDF: https://arxiv.org/pdf/2602.10980v1
- Authors: Yuhao Chen, Zhihao Zhan, Xiaoxin Lin, Zijian Song, Hao Liu, Qinhan Lyu, Yubo Zu, Xiao Chen...
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

VLA models have achieved remarkable progress in embodied intelligence; however, their evaluation remains largely confined to simulations or highly constrained real-world settings. This mismatch creates a substantial reality gap, where strong benchmark performance often masks poor generalization in diverse physical environments. We identify three systemic shortcomings in current benchmarking practices that hinder fair and reliable model comparison. (1) Existing benchmarks fail to model real-world dynamics, overlooking critical factors such as dynamic object configurations, robot initial states, lighting changes, and sensor noise. (2) Current protocols neglect spatial--physical intelligence, reducing evaluation to rote manipulation tasks that do not probe geometric reasoning. (3) The field lacks scalable fully autonomous evaluation, instead relying on simplistic 2D metrics that miss 3D spatial structure or on human-in-the-loop systems that are costly, biased, and unscalable. To address these limitations, we introduce RADAR (Real-world Autonomous Dynamics And Reasoning), a benchmark designed to systematically evaluate VLA generalization under realistic conditions. RADAR integrates three core components: (1) a principled suite of physical dynamics; (2) dedicated tasks that explicitly test spatial reasoning and physical understanding; and (3) a fully autonomous evaluation pipeline based on 3D metrics, eliminating the need for human supervision. We apply RADAR to audit multiple state-of-the-art VLA models and uncover severe fragility beneath their apparent competence. Performance drops precipitously under modest physical dynamics, with the expectation of 3D IoU declining from 0.261 to 0.068 under sensor noise. Moreover, models exhibit limited spatial reasoning capability. These findings position RADAR as a necessary bench toward reliable and generalizable real-world evaluation of VLA models.

</details>

**LLM Summary**

- What: 現実世界のダイナミクス、空間的・物理的知能、自律評価を考慮した、Vision-Language-Action (VLA) モデルの汎化性能をベンチマークする「RADAR」を提案。
- Novelty: 現実世界の動的な要素、空間的・物理的知能の評価、スケーラブルな自律評価プロトコル。
- Why it matters: シミュレーションと現実世界のギャップを埋め、VLAモデルの真の汎化性能を公正かつ信頼性高く評価するための標準を提供する。

## 4. CMAD: Cooperative Multi-Agent Diffusion via Stochastic Optimal Control
- arXiv: http://arxiv.org/abs/2602.10933v1
- PDF: https://arxiv.org/pdf/2602.10933v1
- Authors: Riccardo Barbano, Alexander Denker, Zeljko Kereta, Runchang Li, Francisco Vargas
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

Continuous-time generative models have achieved remarkable success in image restoration and synthesis. However, controlling the composition of multiple pre-trained models remains an open challenge. Current approaches largely treat composition as an algebraic composition of probability densities, such as via products or mixtures of experts. This perspective assumes the target distribution is known explicitly, which is almost never the case. In this work, we propose a different paradigm that formulates compositional generation as a cooperative Stochastic Optimal Control problem. Rather than combining probability densities, we treat pre-trained diffusion models as interacting agents whose diffusion trajectories are jointly steered, via optimal control, toward a shared objective defined on their aggregated output. We validate our framework on conditional MNIST generation and compare it against a naive inference-time DPS-style baseline replacing learned cooperative control with per-step gradient guidance.

</details>

**LLM Summary**

- What: 複数の事前学習済み拡散モデルを協調的に制御し、望ましい出力を生成するための、確率的最適制御に基づく新しいパラダイム「CMAD」を提案。
- Novelty: 確率密度関数の代数的な組み合わせではなく、拡散モデルを協調的な確率的最適制御問題として定式化。
- Why it matters: 複数の生成モデルを効果的に組み合わせ、より複雑で望ましい生成タスク（例：条件付き生成）を実現する。

## 5. Semi-Supervised Cross-Domain Imitation Learning
- arXiv: http://arxiv.org/abs/2602.10793v1
- PDF: https://arxiv.org/pdf/2602.10793v1
- Authors: Li-Min Chu, Kai-Siang Ma, Ming-Hong Chen, Ping-Chun Hsieh
- Keyword score: 3 / hits: imitation learning

<details><summary>Abstract</summary>

Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.

</details>

**LLM Summary**

- What: 少量のターゲットドメインの専門家デモンストレーションと、ラベルなしの不完全な軌跡データを用いて、ドメイン間の模倣学習を効率的に行う半教師ありクロスドメイン模倣学習（SS-CDIL）手法を提案。
- Novelty: 半教師ありクロスドメイン模倣学習の設定と、ドメイン間の状態行動マッピング学習のための新しいクロスドメイン損失関数、および適応的な重み関数。
- Why it matters: 専門家データの収集が困難な場合に、データ効率が高く安定したポリシー学習を最小限の教師データで実現する。

## 6. AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models
- arXiv: http://arxiv.org/abs/2602.10698v1
- PDF: https://arxiv.org/pdf/2602.10698v1
- Authors: Zhifeng Rao, Wenlong Chen, Lei Xie, Xia Hua, Dongfu Yin, Zhen Tian, F. Richard Yu
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.

</details>

**LLM Summary**

- What: RGB画像と深度推定を組み合わせたVision-Language-Action (VLA) モデルのフレームワークを提案。深度推定により3D情報をリッチにし、アクション補助モジュールでアクションの事前知識を制約として学習。
Novelty: 深度推定をVLAモデルに統合し、3D構造情報を活用。アクション補助モジュールでアクションの信頼性を向上。
Why it matters: 複雑な3D環境でのロボットの知覚と制御能力を向上させ、汎化能力とロバスト性を高める。

## 7. Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning
- arXiv: http://arxiv.org/abs/2602.10594v1
- PDF: https://arxiv.org/pdf/2602.10594v1
- Authors: Runze Tang, Penny Sweetser
- Keyword score: 3 / hits: imitation learning

<details><summary>Abstract</summary>

Imitation Learning (IL) enables robots to learn complex skills from demonstrations without explicit task modeling, but it typically requires large amounts of demonstrations, creating significant collection costs. Prior work has investigated using flow as an intermediate representation to enable the use of human videos as a substitute, thereby reducing the amount of required robot demonstrations. However, most prior work has focused on the flow, either on the object or on specific points of the robot/hand, which cannot describe the motion of interaction. Meanwhile, relying on flow to achieve generalization to scenarios observed only in human videos remains limited, as flow alone cannot capture precise motion details. Furthermore, conditioning on scene observation to produce precise actions may cause the flow-conditioned policy to overfit to training tasks and weaken the generalization indicated by the flow. To address these gaps, we propose SFCrP, which includes a Scene Flow prediction model for Cross-embodiment learning (SFCr) and a Flow and Cropped point cloud conditioned Policy (FCrP). SFCr learns from both robot and human videos and predicts any point trajectories. FCrP follows the general flow motion and adjusts the action based on observations for precision tasks. Our method outperforms SOTA baselines across various real-world task settings, while also exhibiting strong spatial and instance generalization to scenarios seen only in human videos.

</details>

**LLM Summary**

- What: 少量のデモンストレーションで模倣学習を行うためのSFCrPフレームワークを提案。シーンフロー予測モデル(SFCr)と、フローとクロップされた点群で条件付けられたポリシー(FCrP)から構成される。
Novelty: ロボットと人間の両方のビデオから学習し、任意の点軌道を予測するシーンフロー予測モデルを導入。フローと点群情報でポリシーを条件付け、クロスエンボディメント学習を可能にする。
Why it matters: 模倣学習に必要なデモンストレーションの量を削減し、ロボットが人間のような複雑なスキルを学習する能力を向上させる。

## 8. ST4VLA: Spatially Guided Training for Vision-Language-Action Models
- arXiv: http://arxiv.org/abs/2602.10109v1
- PDF: https://arxiv.org/pdf/2602.10109v1
- Authors: Jinhui Ye, Fangjing Wang, Ning Gao, Junqiu Yu, Yangkun Zhu, Bin Wang, Jinyu Zhang, Weiyang Jin...
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/

</details>

**LLM Summary**

- What: VLAモデルの行動学習を空間的知識と連携させるための空間ガイド付きトレーニング(ST4VLA)を提案。空間的グラウンディング事前学習と空間ガイド付き行動後学習の2段階で構成される。
Novelty: VLMに転移可能な空間的知識を付与し、行動生成をガイドするために空間的プロンプトを利用する。
Why it matters: VLAモデルの行動生成能力を向上させ、未知のオブジェクトや指示に対する汎化能力とロバスト性を高める。

## 9. VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model
- arXiv: http://arxiv.org/abs/2602.10098v1
- PDF: https://arxiv.org/pdf/2602.10098v1
- Authors: Jingwen Sun, Wenyao Zhang, Zekun Qi, Shaojie Ren, Zezhi Liu, Hanxin Zhu, Guangzhong Sun, Xin Jin...
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.

</details>

**LLM Summary**

- What: JEPA（Joint Embedding Predictive Architecture）スタイルの事前学習フレームワークであるVLA-JEPAを提案。将来のフレームの潜在表現を予測することで、ピクセル単位の変動ではなく、行動に関連する状態遷移を学習する。
Novelty: ピクセル空間ではなく潜在空間で予測を行い、カメラの動きや無関係な背景変化に対してロバストなダイナミクス抽象化を学習する。
Why it matters: VLAモデルの汎化能力とロバスト性を向上させ、インターネットスケールのビデオデータから効率的に学習できるようにする。

## 10. VideoSTF: Stress-Testing Output Repetition in Video Large Language Models
- arXiv: http://arxiv.org/abs/2602.10639v1
- PDF: https://arxiv.org/pdf/2602.10639v1
- Authors: Yuxin Cao, Wei Song, Shangzhi Xu, Jingling Xue, Jin Song Dong
- Keyword score: 2 / hits: video understanding, video-language

<details><summary>Abstract</summary>

Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: https://github.com/yuxincao22/VideoSTF_benchmark.

</details>

**LLM Summary**

- What: VideoLLMにおける出力繰り返し（生成失敗モード）を測定・ストレステストするためのフレームワークVideoSTFを提案。
Novelty: 3つのn-gramベースの指標を用いて繰り返しを形式化し、10,000本の多様なビデオと制御された時間変換ライブラリからなるテストベッドを提供する。
Why it matters: VideoLLMの出力繰り返し問題を体系的に評価し、その脆弱性を明らかにする。時間的摂動が繰り返しに大きく影響することを示し、セキュリティ上の脆弱性としても指摘する。

## 11. Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning
- arXiv: http://arxiv.org/abs/2602.10285v1
- PDF: https://arxiv.org/pdf/2602.10285v1
- Authors: Ananya Trivedi, Anjian Li, Mohamed Elnoor, Yusuf Umut Ciftci, Avinash Singh, Jovin D'sa, Sangjae Bae, David Isele...
- Keyword score: 2 / hits: real-time, imitation learning

<details><summary>Abstract</summary>

Autonomous driving requires reasoning about interactions with surrounding traffic. A prevailing approach is large-scale imitation learning on expert driving datasets, aimed at generalizing across diverse real-world scenarios. For online trajectory generation, such methods must operate at real-time rates. Diffusion models require hundreds of denoising steps at inference, resulting in high latency. Consistency models mitigate this issue but rely on carefully tuned noise schedules to capture the multimodal action distributions common in autonomous driving. Adapting the schedule, typically requires expensive retraining. To address these limitations, we propose a framework based on conditional flow matching that jointly predicts future motions of surrounding agents and plans the ego trajectory in real time. We train a lightweight variance estimator that selects the number of inference steps online, removing the need for retraining to balance runtime and imitation learning performance. To further enhance ride quality, we introduce a trajectory post-processing step cast as a convex quadratic program, with negligible computational overhead. Trained on the Waymo Open Motion Dataset, the framework performs maneuvers such as lane changes, cruise control, and navigating unprotected left turns without requiring scenario-specific tuning. Our method maintains a 20 Hz update rate on an NVIDIA RTX 3070 GPU, making it suitable for online deployment. Compared to transformer, diffusion, and consistency model baselines, we achieve improved trajectory smoothness and better adherence to dynamic constraints. Experiment videos and code implementations can be found at https://flow-matching-self-driving.github.io/.

</details>

**LLM Summary**

- What: 自律走行車のリアルタイム軌道生成のために、周囲のエージェントの将来の動きと自律走行車の軌道を同時に予測する条件付きフローマッチングに基づくフレームワークを提案。推論ステップ数をオンラインで選択する軽量な分散推定器と、軌道品質を向上させるための凸二次計画法による後処理ステップを導入。
- Novelty: リアルタイムでの軌道生成と、再学習なしでの推論ステップ数の適応、および軌道品質の向上を両立させた点。
- Why it matters: 自律走行車のリアルタイムな軌道生成の遅延を削減し、より安全で快適な運転を実現する。

## 12. Chatting with Images for Introspective Visual Thinking
- arXiv: http://arxiv.org/abs/2602.11073v1
- PDF: https://arxiv.org/pdf/2602.11073v1
- Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tienie Tan
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.

</details>

**LLM Summary**

- What: 画像を対象とした「チャット」という新しいフレームワークを提案。これは、言語プロンプトのガイダンスの下で複数の画像領域の再エンコーディングを動的に実行し、言語的推論と視覚的状態の更新をより緊密に連携させる。このパラダイムを、動的なビジョンエンコーダーを備えたViLaVTというLVLMで実装。
- Novelty: 画像操作を言語誘導型の特徴変調として再構築し、複数の画像領域にわたる動的な再エンコーディングを可能にした点。
- Why it matters: 大規模視覚言語モデル（LVLM）における、細粒度の視覚情報の損失や、視覚的意味論と幾何学的関係の推論におけるクロスモーダルアライメントの課題を解決し、より高度な視覚的思考を可能にする。

## 13. ContactGaussian-WM: Learning Physics-Grounded World Model from Videos
- arXiv: http://arxiv.org/abs/2602.11021v1
- PDF: https://arxiv.org/pdf/2602.11021v1
- Authors: Meizhong Wang, Wanxin Jin, Kun Cao, Lihua Xie, Yiguang Hong
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Developing world models that understand complex physical interactions is essential for advancing robotic planning and simulation.However, existing methods often struggle to accurately model the environment under conditions of data scarcity and complex contact-rich dynamic motion.To address these challenges, we propose ContactGaussian-WM, a differentiable physics-grounded rigid-body world model capable of learning intricate physical laws directly from sparse and contact-rich video sequences.Our framework consists of two core components: (1) a unified Gaussian representation for both visual appearance and collision geometry, and (2) an end-to-end differentiable learning framework that differentiates through a closed-form physics engine to infer physical properties from sparse visual observations.Extensive simulations and real-world evaluations demonstrate that ContactGaussian-WM outperforms state-of-the-art methods in learning complex scenarios, exhibiting robust generalization capabilities.Furthermore, we showcase the practical utility of our framework in downstream applications, including data synthesis and real-time MPC.

</details>

**LLM Summary**

- What: 疎で接触の多いビデオシーケンスから直接物理法則を学習できる、微分可能な物理学に基づいた剛体ワールドモデル「ContactGaussian-WM」を提案。視覚的アピアランスと衝突ジオメトリの両方に対する統一されたガウス表現と、閉形式の物理エンジンを通して微分するエンドツーエンドの学習フレームワークを採用。
- Novelty: 疎で接触の多いビデオデータから、物理学に基づいた剛体ワールドモデルを直接学習できる点。
- Why it matters: ロボットの計画立案やシミュレーションにおいて、データ不足や複雑な接触ダイナミクス下での環境モデリングの精度を向上させ、データ合成やリアルタイムMPCなどの下流アプリケーションに貢献する。

## 14. TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents
- arXiv: http://arxiv.org/abs/2602.10986v1
- PDF: https://arxiv.org/pdf/2602.10986v1
- Authors: Abhishek Vijaya Kumar, Bhaskar Kataria, Byungsoo Oh, Emaad Manzoor, Rachee Singh
- Keyword score: 1 / hits: video understanding

<details><summary>Abstract</summary>

In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.

</details>

**LLM Summary**

- What: 強化学習におけるLLMエージェントのポストトレーニング中に発生する、外部ツール呼び出しの遅延とコストを削減するため、状態を持つツール・バリュー・キャッシュ「TVCACHE」を提案。ツール呼び出しシーケンスのツリーを維持し、最長プレフィックスマッチングによるキャッシュルックアップを行うことで、環境状態の一貫性を保証。
- Novelty: ツール呼び出しの履歴を考慮した状態を持つキャッシュ機構により、環境状態の一貫性を保ちつつキャッシュヒット率を高めた点。
- Why it matters: LLMエージェントのポストトレーニング時間を大幅に短縮し、コストを削減することで、より効率的なエージェント開発を可能にする。

## 15. Scaling World Model for Hierarchical Manipulation Policies
- arXiv: http://arxiv.org/abs/2602.10983v1
- PDF: https://arxiv.org/pdf/2602.10983v1
- Authors: Qian Long, Yueze Wang, Jiaxi Song, Junbo Zhang, Peiyan Li, Wenxuan Wang, Yuqi Wang, Haoyang Li...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \href{https://vista-wm.github.io/}{https://vista-wm.github.io}

</details>

**LLM Summary**

- What: 汎用的なロボット操作のための階層型Vision-Language-Action（VLA）フレームワーク「VISTA」を提案。大規模事前学習済みワールドモデルの汎化能力を活用し、操作タスクを視覚的なサブゴールシーケンスに分割し、低レベルポリシーがこれらの視覚的・言語的ガイダンスに従って行動シーケンスを生成する。
- Novelty: ワールドモデルをハイレベルプランナーとして、VLAモデルをローレベルエグゼキューターとする階層型フレームワークと、視覚的サブゴールの生成により、未知のオブジェクトや新しいシナリオへの汎化能力を向上させた点。
- Why it matters: VLAモデルの汎化能力のボトルネックを解消し、特に限られた実ロボットデータや未知のシナリオにおいて、より堅牢で汎用的なロボット操作を実現する。

## 16. Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation
- arXiv: http://arxiv.org/abs/2602.10880v1
- PDF: https://arxiv.org/pdf/2602.10880v1
- Authors: Minggui He, Mingchen Dai, Jian Zhang, Yilun Liu, Shimin Tao, Pufan Zeng, Osamu Yoshie, Yuya Ieiri
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging. Existing approaches largely rely on supervised fine-tuning, encouraging surface-level token imitation rather than faithful modeling of underlying chart structure, which often leads to hallucinated or semantically inconsistent outputs. We propose Chart Specification, a structured intermediate representation that shifts training from text imitation to semantically grounded supervision. Chart Specification filters syntactic noise to construct a structurally balanced training set and supports a Spec-Align Reward that provides fine-grained, verifiable feedback on structural correctness, enabling reinforcement learning to enforce consistent plotting logic. Experiments on three public benchmarks show that our method consistently outperforms prior approaches. With only 3K training samples, we achieve strong data efficiency, surpassing leading baselines by up to 61.7% on complex benchmarks, and scaling to 4K samples establishes new state-of-the-art results across all evaluated metrics. Overall, our results demonstrate that precise structural supervision offers an efficient pathway to high-fidelity chart-to-code generation. Code and dataset are available at: https://github.com/Mighten/chart-specification-paper

</details>

**LLM Summary**

- What: チャート画像からプロットコードを生成する際に、構造的な忠実性を高めるための「Chart Specification」という構造化された中間表現を提案。これにより、表面的なトークン模倣ではなく、意味的に根拠のある教師あり学習が可能になり、Spec-Align Rewardによる構造的正確性へのフィードバックで強化学習を強化。
- Novelty: チャート構造に焦点を当てた中間表現と、構造的正確性フィードバックによる強化学習の導入。
- Why it matters: チャート画像からのコード生成の精度と信頼性を向上させ、特に複雑なチャートでのデータ効率と性能を大幅に改善する。

## 17. SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios
- arXiv: http://arxiv.org/abs/2602.10840v1
- PDF: https://arxiv.org/pdf/2602.10840v1
- Authors: Yanan Wang, Renxi Wang, Yongxin Wang, Xuezhi Liang, Fajri Koto, Timothy Baldwin, Xiaodan Liang, Haonan Li
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Large language models (LLMs) have been extensively studied for tasks like math competitions, complex coding, and scientific reasoning, yet their ability to accurately represent and simulate physical scenarios via code remains underexplored. We propose SimuScene, the first systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 physical concepts. We build an automatic pipeline to collect data, with human verification to ensure quality. The final dataset contains 7,659 physical scenarios with 334 human-verified examples as the test set. We evaluated 10 contemporary LLMs and found that even the strongest model achieves only a 21.5% pass rate, demonstrating the difficulty of the task. Finally, we introduce a reinforcement learning pipeline with visual rewards that uses a vision-language model as a judge to train textual models. Experiments show that training with our data improves physical simulation via code while substantially enhancing general code generation performance.

</details>

**LLM Summary**

- What: 物理シナリオをコードでシミュレートするためのLLMの能力を訓練・評価する初の体系的な研究「SimuScene」を提案。5つの物理ドメインと52の物理概念にわたる7,659の物理シナリオデータセットを構築し、10のLLMを評価。視覚報酬を用いた強化学習パイプラインも導入。
- Novelty: 物理シナリオのコード生成に特化した大規模データセットと評価フレームワーク、およびそれを改善するための強化学習パイプライン。
- Why it matters: LLMの物理現象の理解とコードによるシミュレーション能力を向上させ、科学的推論や教育分野への応用を促進する。

## 18. Flow caching for autoregressive video generation
- arXiv: http://arxiv.org/abs/2602.10825v1
- PDF: https://arxiv.org/pdf/2602.10825v1
- Authors: Yuexiao Ma, Xuzhe Zheng, Jing Xu, Xiwei Xu, Feng Ling, Xiawu Zheng, Huafeng Kuang, Huixia Li...
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.

</details>

**LLM Summary**

- What: オートリグレッシブビデオ生成の速度を向上させるためのキャッシュフレームワーク「FlowCache」を提案。各ビデオチャンクが独立したキャッシュポリシーを持つようにし、チャンクごとのデノイジング特性に動的に適応するキャッシュ戦略と、メモリ制約内で品質を維持するKVキャッシュ圧縮メカニズムを導入。
- Novelty: オートリグレッシブビデオ生成に特化した、チャンクごとの動的なキャッシュ戦略とKVキャッシュ圧縮。
- Why it matters: 長尺ビデオの生成速度を大幅に向上させ、より効率的で高品質なビデオ生成を可能にする。

## 19. Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training
- arXiv: http://arxiv.org/abs/2602.10815v1
- PDF: https://arxiv.org/pdf/2602.10815v1
- Authors: Aojun Lu, Tao Feng, Hangjie Yuan, Wei Li, Yanan Sun
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

The adaptation of large-scale Vision-Language Models (VLMs) through post-training reveals a pronounced generalization gap: models fine-tuned with Reinforcement Learning (RL) consistently achieve superior out-of-distribution (OOD) performance compared to those trained with Supervised Fine-Tuning (SFT). This paper posits a data-centric explanation for this phenomenon, contending that RL's generalization advantage arises from an implicit data filtering mechanism that inherently prioritizes medium-difficulty training samples. To test this hypothesis, we systematically evaluate the OOD generalization of SFT models across training datasets of varying difficulty levels. Our results confirm that data difficulty is a critical factor, revealing that training on hard samples significantly degrades OOD performance. Motivated by this finding, we introduce Difficulty-Curated SFT (DC-SFT), a straightforward method that explicitly filters the training set based on sample difficulty. Experiments show that DC-SFT not only substantially enhances OOD generalization over standard SFT, but also surpasses the performance of RL-based training, all while providing greater stability and computational efficiency. This work offers a data-centric account of the OOD generalization gap in VLMs and establishes a more efficient pathway to achieving robust generalization. Code is available at https://github.com/byyx666/DC-SFT.

</details>

**LLM Summary**

- What: 強化学習（RL）が教師ありファインチューニング（SFT）よりも汎化性能に優れる理由を、データ中心の観点から分析。RLは暗黙的に中程度の難易度のトレーニングサンプルを優先するデータフィルタリングメカニズムを持つと仮説を立て、難易度を考慮したSFT（DC-SFT）を提案。
- Novelty: RLの汎化性能の優位性をデータ難易度の観点から説明し、難易度ベースのSFT手法を提案。
- Why it matters: VLMの後処理における汎化性能を向上させ、RLと同等以上の性能をより安定かつ効率的に達成する手法を提供する。

## 20. From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving
- arXiv: http://arxiv.org/abs/2602.10719v1
- PDF: https://arxiv.org/pdf/2602.10719v1
- Authors: Sining Ang, Yuguang Yang, Chenxu Dang, Canyu Chen, Cheng Chi, Haiyan Liu, Xuanyao Mao, Jason Bao...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains unclear what changes beyond the usual accuracy--cost trade-off. We revisit this question with 3--RQ analysis in RecogDrive by instantiating the system with a full VLM and vision-only backbones, all under an identical diffusion Transformer planner. RQ1: At the backbone level, the VLM can introduce additional subspaces upon the vision-only backbones. RQ2: This unique subspace leads to a different behavioral in some long-tail scenario: the VLM tends to be more aggressive whereas ViT is more conservative, and each decisively wins on about 2--3% of test scenarios; With an oracle that selects, per scenario, the better trajectory between the VLM and ViT branches, we obtain an upper bound of 93.58 PDMS. RQ3: To fully harness this observation, we propose HybridDriveVLA, which runs both ViT and VLM branches and selects between their endpoint trajectories using a learned scorer, improving PDMS to 92.10. Finally, DualDriveVLA implements a practical fast--slow policy: it runs ViT by default and invokes the VLM only when the scorer's confidence falls below a threshold; calling the VLM on 15% of scenarios achieves 91.00 PDMS while improving throughput by 3.2x. Code will be released.

</details>

**LLM Summary**

- What: VLAドライビングにおいて、VLMとビジョンのみのバックボーンを組み合わせたエンドツーエンドドライビングシステム「HybridDriveVLA」と「DualDriveVLA」を提案。VLMが追加のサブスペースを提供し、特定の長尾シナリオで異なる挙動を示すことを発見。
- Novelty: VLMとビジョンのみのバックボーンの相補性を活用し、状況に応じて最適な出力を選択するハイブリッドおよびデュアルシステムアーキテクチャ。
- Why it matters: エンドツーエンドドライビングの性能を向上させ、特に予期せぬ状況や長尾シナリオでの安全性と信頼性を高める。
