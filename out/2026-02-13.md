# Daily CV Digest (2026-02-13)

- Total: 20

## 1. The Five Ws of Multi-Agent Communication: Who Talks to Whom, When, What, and Why -- A Survey from MARL to Emergent Language and LLMs
- arXiv: http://arxiv.org/abs/2602.11583v1
- PDF: https://arxiv.org/pdf/2602.11583v1
- Authors: Jingdi Chen, Hanqing Yang, Zongjun Liu, Carlee Joe-Wong
- Keyword score: 7 / hits: reinforcement learning, multi-agent, marl

<details><summary>Abstract</summary>

Multi-agent sequential decision-making powers many real-world systems, from autonomous vehicles and robotics to collaborative AI assistants. In dynamic, partially observable environments, communication is often what reduces uncertainty and makes collaboration possible. This survey reviews multi-agent communication (MA-Comm) through the Five Ws: who communicates with whom, what is communicated, when communication occurs, and why communication is beneficial. This framing offers a clean way to connect ideas across otherwise separate research threads. We trace how communication approaches have evolved across three major paradigms. In Multi-Agent Reinforcement Learning (MARL), early methods used hand-designed or implicit protocols, followed by end-to-end learned communication optimized for reward and control. While successful, these protocols are frequently task-specific and hard to interpret, motivating work on Emergent Language (EL), where agents can develop more structured or symbolic communication through interaction. EL methods, however, still struggle with grounding, generalization, and scalability, which has fueled recent interest in large language models (LLMs) that bring natural language priors for reasoning, planning, and collaboration in more open-ended settings. Across MARL, EL, and LLM-based systems, we highlight how different choices shape communication design, where the main trade-offs lie, and what remains unsolved. We distill practical design patterns and open challenges to support future hybrid systems that combine learning, language, and control for scalable and interpretable multi-agent collaboration.

</details>

**LLM Summary**

- What: マルチエージェントコミュニケーション（MA-Comm）に関する包括的なサーベイ論文。
- Novelty: コミュニケーションを「誰が」「誰と」「何を」「いつ」「なぜ」コミュニケーションするかという「5W」のフレームワークで整理し、MARL、Emergent Language、LLMの3つのパラダイムを横断して解説。
- Why it matters: マルチエージェントシステムにおけるコミュニケーションの進化と課題を理解し、今後の研究開発の方向性を示す。

## 2. GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning
- arXiv: http://arxiv.org/abs/2602.12099v1
- PDF: https://arxiv.org/pdf/2602.12099v1
- Authors: GigaBrain Team, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Hao Li, Jie Li, Jindi Lv...
- Keyword score: 4 / hits: vision-language-action, reinforcement learning

<details><summary>Abstract</summary>

Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\% on challenging tasks including \texttt{Laundry Folding}, \texttt{Box Packing}, and \texttt{Espresso Preparation}. Critically, \textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \href{https://gigabrain05m.github.io}{project page}.

</details>

**LLM Summary**

- What: 世界モデルベースの強化学習を用いたVision-Language-Action (VLA) モデル「GigaBrain-0.5M*」を提案。
- Novelty: 事前学習済みのビデオ世界モデルを活用し、RAMP (Reinforcement leArning via world Model-conditioned Policy) という手法で強化学習を行うことで、タスク間の適応性を向上。
- Why it matters: ロボット操作などの実世界タスクにおいて、より頑健で汎用的なVLAモデルの実現に貢献する。

## 3. Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment
- arXiv: http://arxiv.org/abs/2602.12281v1
- PDF: https://arxiv.org/pdf/2602.12281v1
- Authors: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the "intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce "boot-time compute" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.

</details>

**LLM Summary**

- What: Vision-Language-Action (VLA) モデルの指示追従における「意図と行動のギャップ」を縮小するため、テスト時の検証（Verification）のスケーリングに着目。
- Novelty: 指示の言い換えと生成される行動の多様性を同時にスケールさせることで、テスト時のサンプル効率を向上させるスケーリング則を発見。CoVerというコントラスティブ検証器を提案。
- Why it matters: 自然言語指示に基づいた汎用ロボットの実現に向け、VLAモデルの精度と信頼性を向上させるための効果的なアプローチを提供する。

## 4. MonarchRT: Efficient Attention for Real-Time Video Generation
- arXiv: http://arxiv.org/abs/2602.12271v1
- PDF: https://arxiv.org/pdf/2602.12271v1
- Authors: Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng, Xun Huang, Atri Rudra, Beidi Chen
- Keyword score: 3 / hits: real-time

<details><summary>Abstract</summary>

Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.

</details>

**LLM Summary**

- What: リアルタイム動画生成における拡散モデルの計算コストを削減する「MonarchRT」を提案。
- Novelty: 動画の注意機構（Attention）の構造的特性（周期性、意味的対応、密な混合）を捉えるために、Monarch行列を用いた構造化された注意機構を導入。
- Why it matters: リアルタイムでの高品質な動画生成を可能にし、インタラクティブなアプリケーションやコンテンツ生成の分野に貢献する。

## 5. FAIL: Flow Matching Adversarial Imitation Learning for Image Generation
- arXiv: http://arxiv.org/abs/2602.12155v1
- PDF: https://arxiv.org/pdf/2602.12155v1
- Authors: Yeyao Ma, Chen Li, Xiaosong Zhang, Han Hu, Weidi Xie
- Keyword score: 3 / hits: imitation learning

<details><summary>Abstract</summary>

Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.

</details>

**LLM Summary**

- What: フローマッチングモデルの事後学習を模倣学習として捉え、報酬なしでポリシーと専門家の分布を一致させる「FAIL (Flow Matching Adversarial Imitation Learning)」を提案。
- Novelty: 強化学習の報酬モデリングやペア比較を必要とせず、敵対的学習を用いて模倣学習を行う新しいフレームワーク。
- Why it matters: 画像生成や動画生成タスクにおいて、より効率的かつ頑健な模倣学習を実現し、報酬ハッキングなどの問題を軽減する。

## 6. VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model
- arXiv: http://arxiv.org/abs/2602.12063v1
- PDF: https://arxiv.org/pdf/2602.12063v1
- Authors: Yanjiang Guo, Tony Lee, Lucy Xiaoyang Shi, Jianyu Chen, Percy Liang, Chelsea Finn
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w

</details>

**LLM Summary**

- What: 視覚・言語・行動（VLA）モデルの性能と信頼性を、オンラインでの反復的な相互作用を通じて向上させる手法を提案。学習したシミュレータ（行動条件付きビデオ生成モデル）を用いて追加のロールアウトデータを生成し、そのデータでVLAモデルを改善する。
- Novelty: 実世界のロールアウトデータを用いてシミュレータの忠実度を向上させ、そのシミュレータで生成した合成データでVLAモデルを改善するという反復的なアルゴリズム。
- Why it matters: ロボットによる実世界タスクの成功率を大幅に向上させ、より信頼性の高いVLAモデルの実現に貢献する。

## 7. Adaptive-Horizon Conflict-Based Search for Closed-Loop Multi-Agent Path Finding
- arXiv: http://arxiv.org/abs/2602.12024v1
- PDF: https://arxiv.org/pdf/2602.12024v1
- Authors: Jiarui Li, Federico Pecora, Runyu Zhang, Gioele Zardini
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

MAPF is a core coordination problem for large robot fleets in automated warehouses and logistics. Existing approaches are typically either open-loop planners, which generate fixed trajectories and struggle to handle disturbances, or closed-loop heuristics without reliable performance guarantees, limiting their use in safety-critical deployments. This paper presents ACCBS, a closed-loop algorithm built on a finite-horizon variant of CBS with a horizon-changing mechanism inspired by iterative deepening in MPC. ACCBS dynamically adjusts the planning horizon based on the available computational budget, and reuses a single constraint tree to enable seamless transitions between horizons. As a result, it produces high-quality feasible solutions quickly while being asymptotically optimal as the budget increases, exhibiting anytime behavior. Extensive case studies demonstrate that ACCBS combines flexibility to disturbances with strong performance guarantees, effectively bridging the gap between theoretical optimality and practical robustness for large-scale robot deployment.

</details>

**LLM Summary**

- What: 閉鎖ループのマルチエージェント経路探索（MAPF）のための適応型ホライズンを持つ衝突ベース探索（ACCBS）アルゴリズムを提案。
- Novelty: 計算リソースに応じて計画ホライズンを動的に調整し、単一の制約ツリーを再利用することで、異なるホライズン間でのシームレスな移行を可能にする。
- Why it matters: 大規模ロボット群の協調問題において、摂動への柔軟性と強力な性能保証を両立させ、理論的な最適性と実用的なロバスト性のギャップを埋める。

## 8. How to Sample High Quality 3D Fractals for Action Recognition Pre-Training?
- arXiv: http://arxiv.org/abs/2602.11810v1
- PDF: https://arxiv.org/pdf/2602.11810v1
- Authors: Marko Putak, Thomas B. Moeslund, Joakim Bruslund Haurum
- Keyword score: 3 / hits: action recognition

<details><summary>Abstract</summary>

Synthetic datasets are being recognized in the deep learning realm as a valuable alternative to exhaustively labeled real data. One such synthetic data generation method is Formula Driven Supervised Learning (FDSL), which can provide an infinite number of perfectly labeled data through a formula driven approach, such as fractals or contours. FDSL does not have common drawbacks like manual labor, privacy and other ethical concerns. In this work we generate 3D fractals using 3D Iterated Function Systems (IFS) for pre-training an action recognition model. The fractals are temporally transformed to form a video that is used as a pre-training dataset for downstream task of action recognition. We find that standard methods of generating fractals are slow and produce degenerate 3D fractals. Therefore, we systematically explore alternative ways of generating fractals and finds that overly-restrictive approaches, while generating aesthetically pleasing fractals, are detrimental for downstream task performance. We propose a novel method, Targeted Smart Filtering, to address both the generation speed and fractal diversity issue. The method reports roughly 100 times faster sampling speed and achieves superior downstream performance against other 3D fractal filtering methods.

</details>

**LLM Summary**

- What: 行動認識モデルの事前学習のために、高品質な3Dフラクタルをサンプリングする方法を研究。特に、3D iterated function systems (IFS) を用いてフラクタルを生成し、時間的に変換してビデオデータを作成する。
- Novelty: フラクタル生成の速度と多様性の問題を解決する「Targeted Smart Filtering」という新しい手法を提案。
- Why it matters: 行動認識タスクの事前学習に有効な高品質な合成データセットを効率的に生成し、データ収集コストを削減できる。

## 9. Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation
- arXiv: http://arxiv.org/abs/2602.11790v1
- PDF: https://arxiv.org/pdf/2602.11790v1
- Authors: Lingyong Yan, Jiulong Wu, Dong Xie, Weixian Shi, Deguo Xia, Jizhou Huang
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.

</details>

**LLM Summary**

- What: 教育的なビデオ生成のために、LLMベースの階層的なマルチエージェントシステム「LAVES」を提案。
- Novelty: 教育ビデオ生成を、正確な推論、教育的なナレーション、忠実な視覚デモンストレーション、正確な音声・映像アライメントを同時に要求するマルチ目的タスクとして定式化し、専門エージェントを協調させるアーキテクチャを採用。
- Why it matters: 従来のビデオ生成モデルの限界（低手順忠実度、高コスト、限定的な制御性）を克服し、高品質で論理的に厳密な教育ビデオの生成を可能にする。

## 10. STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning
- arXiv: http://arxiv.org/abs/2602.11730v1
- PDF: https://arxiv.org/pdf/2602.11730v1
- Authors: Xiaowen Zhang, Zhi Gao, Licheng Jiao, Lingling Li, Qing Li
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG). Prior approaches typically focus on enhancing visual-textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation tasks, achieving a SOTA 47.3% J&F on MeViS.

</details>

**LLM Summary**

- What: 空間時間ビデオグラウンディング（STVG）における、テキストと視覚座標の不一致による幻覚問題を解決するため、インスタンスレベルの推論とグラウンディングを強化する新しい視覚プロンプトパラダイムを提案。
- Novelty: 各オブジェクトに一意で時間的に一貫したIDを割り当て、それを視覚プロンプトとして埋め込むことで、座標の直接的なアライメント問題を回避。また、STVGのための初の強化学習フレームワーク「STVG-R1」を導入。
- Why it matters: STVGタスクにおける精度と一貫性を向上させ、追加の学習モジュールや高コストなアノテーションを必要とせずに、より効果的なビデオ理解を実現する。

## 11. EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos
- arXiv: http://arxiv.org/abs/2602.11464v1
- PDF: https://arxiv.org/pdf/2602.11464v1
- Authors: Tao Zhang, Song Xia, Ye Wang, Qin Jin
- Keyword score: 3 / hits: imitation learning

<details><summary>Abstract</summary>

Robot imitation learning is often hindered by the high cost of collecting large-scale, real-world data. This challenge is especially significant for low-cost robots designed for home use, as they must be both user-friendly and affordable. To address this, we propose the EasyMimic framework, a low-cost and replicable solution that enables robots to quickly learn manipulation policies from human video demonstrations captured with standard RGB cameras. Our method first extracts 3D hand trajectories from the videos. An action alignment module then maps these trajectories to the gripper control space of a low-cost robot. To bridge the human-to-robot domain gap, we introduce a simple and user-friendly hand visual augmentation strategy. We then use a co-training method, fine-tuning a model on both the processed human data and a small amount of robot data, enabling rapid adaptation to new tasks. Experiments on the low-cost LeRobot platform demonstrate that EasyMimic achieves high performance across various manipulation tasks. It significantly reduces the reliance on expensive robot data collection, offering a practical path for bringing intelligent robots into homes. Project website: https://zt375356.github.io/EasyMimic-Project/.

</details>

**LLM Summary**

- What: 低コストロボットが人間のビデオデモンストレーションから模倣学習するためのフレームワーク「EasyMimic」を提案。ビデオから3D手首軌道を抽出し、ロボットのグリッパー制御空間にマッピングし、ドメインギャップを埋めるための視覚拡張と共同学習手法を用いる。
- Novelty: 低コストロボット向けに、高価なロボットデータ収集への依存を減らし、人間からの模倣学習を容易にするフレームワーク。
- Why it matters: 家庭用低コストロボットが、より手軽に多様な操作タスクを学習できるようになり、インテリジェントロボットの普及を促進する。

## 12. Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning
- arXiv: http://arxiv.org/abs/2602.11455v1
- PDF: https://arxiv.org/pdf/2602.11455v1
- Authors: Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao, Hu Wei, Linfeng Zhang
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by token quantity but by the fidelity of cross-modal anchoring.

</details>

**LLM Summary**

- What: マルチモーダル大規模言語モデル（MLLM）の推論における、視覚的証拠の統合メカニズムをクロスモーダルアテンション接続性から分析し、高接続性トークン（アンカー）が推論の基盤となることを発見。この知見に基づき、アンカー・トークンを強化する軽量フレームワーク「AT-RL」を提案。
- Novelty: MLLMの推論における視覚的証拠の役割を解明し、アンカー・トークンに焦点を当てた新しい強化学習手法を提案。
- Why it matters: MLLMの推論能力を、計算コストを抑えつつ大幅に向上させ、数学、STEM、ビデオタスクなどでの性能改善に貢献する。

## 13. When would Vision-Proprioception Policies Fail in Robotic Manipulation?
- arXiv: http://arxiv.org/abs/2602.12032v1
- PDF: https://arxiv.org/pdf/2602.12032v1
- Authors: Jingxian Lu, Wenke Xia, Yuxuan Wu, Zhiwu Lu, Di Hu
- Keyword score: 2 / hits: real-time, vision-language-action

<details><summary>Abstract</summary>

Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot's motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception's gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation.

</details>

**LLM Summary**

- What: ロボット操作におけるビジョン・プロプリオセプションポリシーの失敗条件を調査し、特にロボットの動作遷移時にビジョンモダリティの役割が限定的になることを発見。プロプリオセプション信号が学習を支配するため、ビジョンモダリティの学習が抑制されることを明らかにした。これを解決するため、プロプリオセプションの最適化を適応的に調整する「GAP」アルゴリズムを提案。
- Novelty: ビジョン・プロプリオセプションポリシーが失敗する状況を特定し、その原因を分析。動作遷移時に有効な新しい最適化手法を提案。
- Why it matters: ロボット操作におけるビジョンとプロプリオセプションの協調学習を改善し、よりロバストで汎用的なロボット制御ポリシーの実現に貢献する。

## 14. LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion
- arXiv: http://arxiv.org/abs/2602.12215v1
- PDF: https://arxiv.org/pdf/2602.12215v1
- Authors: Jiangran Lyu, Kai Liu, Xuheng Zhang, Haoran Liao, Yusen Feng, Wenxuan Zhu, Tingrui Shen, Jiayi Chen...
- Keyword score: 1 / hits: behavior cloning

<details><summary>Abstract</summary>

Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestion by jointly learning dynamics, policy, and visual forecasting, assigning distinct roles to data of varying quality. To support this regime at scale, we assemble and standardize EI-30k, an embodied interaction dataset comprising over 30k hours of human and robot trajectories in a unified format. Scalable dynamics learning over such heterogeneous data is enabled by prediction in a structured DINO latent space, which avoids redundant pixel-space appearance modeling. Complementing this representation, LDA-1B employs a multi-modal diffusion transformer to handle asynchronous vision and action streams, enabling stable training at the 1B-parameter scale. Experiments in simulation and the real world show LDA-1B outperforms prior methods (e.g., $π_{0.5}$) by up to 21\%, 48\%, and 23\% on contact-rich, dexterous, and long-horizon tasks, respectively. Notably, LDA-1B enables data-efficient fine-tuning, gaining 10\% by leveraging 30\% low-quality trajectories typically harmful and discarded.

</details>

**LLM Summary**

- What: 大規模な行動クローニングに依存する従来のロボット基盤モデルに対し、異種埋め込みデータからダイナミクス知識を抽出する「LDA-1B」を提案。多様なデータ品質に役割を割り当て、統一フォーマットのデータセット「EI-30k」を構築。DINO潜在空間での予測とマルチモーダル拡散トランスフォーマーにより、10億パラメータ規模でのスケーラブルな学習を実現。
- Novelty: 異種埋め込みデータを統合的に活用し、ダイナミクス、ポリシー、視覚予測を同時に学習するスケーラブルなロボット基盤モデル。
- Why it matters: より効率的かつ汎用的なロボット学習を可能にし、多様な環境でのロボットの自律性を向上させる。

## 15. DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing
- arXiv: http://arxiv.org/abs/2602.12205v1
- PDF: https://arxiv.org/pdf/2602.12205v1
- Authors: Dianyi Wang, Ruihang Li, Feng Han, Chaofan Ma, Wei Song, Siyuan Wang, Yibin Wang, Yi Xin...
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.

</details>

**LLM Summary**

- What: 大規模モデルに依存する既存の統一マルチモーダルモデルに対し、軽量な50億パラメータモデル「DeepGen 1.0」を提案。階層的特徴を融合する「SCB」フレームワークと、段階的なデータ中心トレーニング戦略（アライメント事前学習、共同教師ありファインチューニング、MR-GRPOによる強化学習）により、画像生成・編集能力を向上。
- Novelty: 軽量ながら高性能な統一マルチモーダルモデルと、そのための新しい特徴融合フレームワークおよびデータ中心トレーニング戦略。
- Why it matters: 計算リソースの制約がある環境でも、高品質な画像生成・編集を実現し、より多くのユーザーが高度な画像操作を行えるようになる。

## 16. 3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting
- arXiv: http://arxiv.org/abs/2602.12159v1
- PDF: https://arxiv.org/pdf/2602.12159v1
- Authors: Wancai Zheng, Hao Chen, Xianlong Lu, Linlin Ou, Xinyi Yu
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/

</details>

**LLM Summary**

- What: 3DGSNavという、3Dガウシアンスプラッティング（3DGS）を永続的な記憶として活用し、ビジョン・言語モデル（VLM）の空間推論能力を向上させる物体ナビゲーションフレームワークを提案した。
- Novelty: 3DGSを環境の永続的な記憶として組み込み、能動的な知覚を通じて環境の3DGS表現を構築し、軌道誘導による自由視点レンダリングを可能にした点。また、構造化された視覚プロンプトとChain-of-Thought（CoT）プロンプティングを統合した。
- Why it matters: 既存の抽象化手法による制約を克服し、より正確で効率的な物体ナビゲーションを実現する。

## 17. HoloBrain-0 Technical Report
- arXiv: http://arxiv.org/abs/2602.12062v1
- PDF: https://arxiv.org/pdf/2602.12062v1
- Authors: Xuewu Lin, Tianwei Lin, Yun Du, Hongyu Xie, Yiwei Jin, Jiawei Li, Shijie Wu, Qingze Wang...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.

</details>

**LLM Summary**

- What: ロボットの現実世界への展開を支援する包括的なVision-Language-Action（VLA）フレームワーク「HoloBrain-0」を提案した。
- Novelty: ロボットの具現化（embodiment）の事前知識（複数視点カメラパラメータ、URDFなど）を明示的に組み込み、3D空間推論能力と多様な具現化への対応を強化した。スケーラブルな「事前学習後、追加学習」パラダイムを採用し、効率的な0.2Bパラメータ版でも大規模モデルに匹敵する性能を達成した。
- Why it matters: VLA研究と現実世界でのロボット展開のギャップを埋め、シミュレーションおよび実世界タスクで最先端の性能を達成し、低遅延でのオンデバイス展開を可能にする。

## 18. Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control
- arXiv: http://arxiv.org/abs/2602.11934v1
- PDF: https://arxiv.org/pdf/2602.11934v1
- Authors: Yu Deng, Yufeng Jin, Xiaogang Jia, Jiahong Xue, Gerhard Neumann, Georgia Chalvatzaki
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

We hypothesize that a key bottleneck in generalizable robot manipulation is not solely data scale or policy capacity, but a structural mismatch between current visual backbones and the physical requirements of closed-loop control. While state-of-the-art vision encoders (including those used in VLAs) optimize for semantic invariance to stabilize classification, manipulation typically demands geometric sensitivity the ability to map millimeter-level pose shifts to predictable feature changes. Their discriminative objective creates a "blind spot" for fine-grained control, whereas generative diffusion models inherently encode geometric dependencies within their latent manifolds, encouraging the preservation of dense multi-scale spatial structure. However, directly deploying stochastic diffusion features for control is hindered by stochastic instability, inference latency, and representation drift during fine-tuning. To bridge this gap, we propose Robot-DIFT, a framework that decouples the source of geometric information from the process of inference via Manifold Distillation. By distilling a frozen diffusion teacher into a deterministic Spatial-Semantic Feature Pyramid Network (S2-FPN), we retain the rich geometric priors of the generative model while ensuring temporal stability, real-time execution, and robustness against drift. Pretrained on the large-scale DROID dataset, Robot-DIFT demonstrates superior geometric consistency and control performance compared to leading discriminative baselines, supporting the view that how a model learns to see dictates how well it can learn to act.

</details>

**LLM Summary**

- What: 幾何学的に一貫した視覚運動制御のための、拡散モデルの特徴を蒸留するフレームワーク「Robot-DIFT」を提案した。
- Novelty: 拡散モデルが持つ幾何学的な依存性を、決定論的なSpatial-Semantic Feature Pyramid Network（S2-FPN）に蒸留することで、拡散モデルの推論遅延や表現ドリフトの問題を克服した。
- Why it matters: ロボット操作における汎化性能のボトルネックを解消し、ミリメートル単位の姿勢変化に敏感な、より信頼性の高い視覚運動制御を実現する。

## 19. JEPA-VLA: Video Predictive Embedding is Needed for VLA Models
- arXiv: http://arxiv.org/abs/2602.11832v1
- PDF: https://arxiv.org/pdf/2602.11832v1
- Authors: Shangchen Miao, Ningya Feng, Jialong Wu, Ye Lin, Xu He, Dong Li, Mingsheng Long
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.

</details>

**LLM Summary**

- What: Vision-Language-Action（VLA）モデルのサンプル効率と汎化性能の向上には、動画予測による埋め込み（Video Predictive Embedding）が重要であることを主張し、V-JEPA 2を提案した。
- Novelty: 一般的なVLAモデルで用いられる画像ベースの事前学習済み表現では、環境の理解やポリシーの事前知識が不十分であることを指摘し、動画予測による埋め込みがタスク関連の動的情報を効果的に捉えることを示した。
- Why it matters: VLAモデルのサンプル効率と汎化性能を大幅に向上させ、より効率的で強力なロボット操作を実現する。

## 20. ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation
- arXiv: http://arxiv.org/abs/2602.11598v1
- PDF: https://arxiv.org/pdf/2602.11598v1
- Authors: Zedong Chu, Shichao Xie, Xiaolong Wu, Yanfen Shen, Minghua Luo, Zhengbo Wang, Fei Liu, Xiaoxu Leng...
- Keyword score: 1 / hits: vision-language-action

<details><summary>Abstract</summary>

Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation. To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 $\text{km}^2$). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.

</details>

**LLM Summary**

- What: 多様な具現化ナビゲーションタスクに対応する統一的なVision-Language-Action（VLA）基盤モデル「ABot-N0」を提案した。
- Novelty: LLMベースの認知脳とFlow Matchingベースのアクションエキスパートを組み合わせた階層的な「脳-行動」アーキテクチャを採用し、大規模なデータエンジンで16.9Mの専門家軌跡と5.0Mの推論サンプルをキュレーションした。
- Why it matters: 5つのコアナビゲーションタスク（Point-Goal, Object-Goal, Instruction-Following, POI-Goal, Person-Following）で統一的なモデルとして最先端の性能を達成し、実環境での長期ミッションを可能にする。
