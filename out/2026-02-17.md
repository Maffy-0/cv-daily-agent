# Daily CV Digest (2026-02-17)

- Total: 20

## 1. Fluid-Agent Reinforcement Learning
- arXiv: http://arxiv.org/abs/2602.14559v1
- PDF: https://arxiv.org/pdf/2602.14559v1
- Authors: Shishir Sharma, Doina Precup, Theodore J. Perkins
- Keyword score: 5 / hits: reinforcement learning, multi-agent, marl

<details><summary>Abstract</summary>

The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.

</details>

**LLM Summary**

- What: エージェントが動的に生成・消滅する「流動エージェント環境」を提案し、この環境下でのマルチエージェント強化学習（MARL）アルゴリズムの性能を評価した。
- Novelty: エージェント数が固定されない流動的な環境でのMARLを扱った点。
- Why it matters: 環境の要求に合わせてエージェントチームのサイズを動的に調整する戦略を可能にし、現実世界の複雑な状況に対応できるAIシステムの開発に貢献する。

## 2. Adapting VACE for Real-Time Autoregressive Video Diffusion
- arXiv: http://arxiv.org/abs/2602.14381v1
- PDF: https://arxiv.org/pdf/2602.14381v1
- Authors: Ryan Fosdick
- Keyword score: 4 / hits: streaming, real-time

<details><summary>Abstract</summary>

We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope.

</details>

**LLM Summary**

- What: VACE（Video All-in-one Creation and Editing）をリアルタイムの自己回帰型ビデオ生成に適応させた。参照フレームを拡散潜在空間から並列条件付けパスウェイに移動させることで、ストリーミングパイプラインとの互換性を実現した。
- Novelty: VACEのアーキテクチャをリアルタイムストリーミングに適応させ、追加学習なしで既存の重みを再利用できるようにした点。
- Why it matters: リアルタイムでのビデオ生成・編集の効率と品質を向上させ、ストリーミングアプリケーションやインタラクティブなビデオ編集ツールへの応用が期待できる。

## 3. EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing
- arXiv: http://arxiv.org/abs/2602.15031v1
- PDF: https://arxiv.org/pdf/2602.15031v1
- Authors: Yehonathan Litman, Shikun Liu, Dario Seyb, Nicholas Milef, Yang Zhou, Carl Marshall, Shubham Tulsiani, Caleb Leak
- Keyword score: 3 / hits: real-time

<details><summary>Abstract</summary>

High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.

</details>

**LLM Summary**

- What: EditCtrlという、計算リソースを必要な箇所にのみ集中させる効率的なビデオ編集フレームワークを提案した。局所的な編集モジュールと軽量なグローバルコンテキストエンベッダーを組み合わせている。
- Novelty: 編集サイズに比例する計算コストを持つ局所的なモジュールと、ビデオ全体の整合性を保つグローバルコンテキストエンベッダーを組み合わせた点。
- Why it matters: 従来のビデオ編集手法よりも計算効率が10倍向上し、編集品質も改善される。これにより、リアルタイムでの高精度なビデオ編集、複数領域のテキストプロンプトによる編集、コンテンツの自己回帰的な伝播などが可能になる。

## 4. BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames
- arXiv: http://arxiv.org/abs/2602.15010v1
- PDF: https://arxiv.org/pdf/2602.15010v1
- Authors: Max Sobol Mark, Jacky Liang, Maria Attarian, Chuyuan Fu, Debidatta Dwibedi, Dhruv Shah, Aviral Kumar
- Keyword score: 3 / hits: imitation learning

<details><summary>Abstract</summary>

Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.

</details>

**LLM Summary**

- What: ロボットの模倣学習において、過去の観測履歴全体ではなく、ビジョン・言語モデルによって検出された重要なキーフレームに焦点を当てるアプローチ（BPP: Big Picture Policies）を提案した。
- Novelty: 過去の履歴全体ではなく、タスク関連のイベントを表す最小限のキーフレームに条件付けることで、学習データと展開時の分布シフトを大幅に削減した点。
- Why it matters: ロボットが過去の履歴を効果的に利用できるようになり、複雑なタスクや長期間の記憶が必要なタスクでの性能が向上する。

## 5. DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI
- arXiv: http://arxiv.org/abs/2602.14974v1
- PDF: https://arxiv.org/pdf/2602.14974v1
- Authors: En Yu, Haoran Lv, Jianjian Sun, Kangheng Lin, Ruitao Zhang, Yukang Shi, Yuyang Chen, Ze Chen...
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.

</details>

**LLM Summary**

- What: DM0という、物理AIのための「エンボディメントネイティブ」なビジョン・言語・アクション（VLA）フレームワークを提案した。ウェブテキスト、自動運転、エンボディメントインタラクションログなど、多様なデータソースから学習する。
- Novelty: 物理的な接地を後付けではなく、初期段階から統合したエンボディメントネイティブなVLAモデルである点。
- Why it matters: ロボットが物理世界での操作やナビゲーションをより効果的に学習できるようになり、物理AIの発展に貢献する。

## 6. Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation
- arXiv: http://arxiv.org/abs/2602.14837v1
- PDF: https://arxiv.org/pdf/2602.14837v1
- Authors: Lorenzo Mur Labadia, Ruben Martinez-Cantin, Jose J. Guerrero, Giovanni M. Farinella, Antonino Furnari
- Keyword score: 3 / hits: action anticipation

<details><summary>Abstract</summary>

Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.

</details>

**LLM Summary**

- What: 短期的な物体インタラクションの予測（STA）を改善する新しいアテンションベースのアーキテクチャ「STAformer」と「STAformer plus plus」を提案。環境の潜在的機能（affordances）をモデリングするモジュールを導入し、予測を人間の行動に結びつける。
- Novelty: フレームガイド付きテンポラルプーリング、デュアル画像-ビデオアテンション、マルチスケール特徴融合を統合したSTAformerアーキテクチャ。環境の潜在的機能モデルを統合し、適応的に融合する手法。
- Why it matters: ウェアラブルアシスタントがユーザーの意図を理解し、適切なタイミングで支援を提供したり、人間とロボットのインタラクションを可能にするために重要。

## 7. Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs
- arXiv: http://arxiv.org/abs/2602.14697v1
- PDF: https://arxiv.org/pdf/2602.14697v1
- Authors: Lunjun Zhang, Ryan Chen, Bradly C. Stadie
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL

</details>

**LLM Summary**

- What: 大規模言語モデル（LLM）の強化学習（RL）を促進する「進化的システムプロンプト学習（E-SPL）」を提案。モデルのコンテキストと重みの両方を同時に改善する。
- Novelty: 各RLイテレーションで複数のシステムプロンプトを選択し、並列で実行。システムプロンプトごとにRL更新を行い、LLM駆動の突然変異と交差によってシステムプロンプト集団に進化的更新を適用する。
- Why it matters: プロンプトにエンコードされた宣言的知識と、重みにエンコードされた手続き的知識の自然な分割を促進し、推論およびエージェントタスクにおけるパフォーマンスを向上させる。

## 8. ST-EVO: Towards Generative Spatio-Temporal Evolution of Multi-Agent Communication Topologies
- arXiv: http://arxiv.org/abs/2602.14681v1
- PDF: https://arxiv.org/pdf/2602.14681v1
- Authors: Xingjian Wu, Xvyuan Liu, Junkai Lu, Siyuan Wang, Yang Shu, Jilin Hu, Chenjuan Guo, Bin Yang
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

LLM-powered Multi-Agent Systems (MAS) have emerged as an effective approach towards collaborative intelligence, and have attracted wide research interests. Among them, ``self-evolving'' MAS, treated as a more flexible and powerful technical route, can construct task-adaptive workflows or communication topologies, instead of relying on a predefined static structue template. Current self-evolving MAS mainly focus on Spatial Evolving or Temporal Evolving paradigm, which only considers the single dimension of evolution and does not fully incentivize LLMs' collaborative capability. In this work, we start from a novel Spatio-Temporal perspective by proposing ST-EVO, which supports dialogue-wise communication scheduling with a compact yet powerful flow-matching based Scheduler. To make precise Spatio-Temporal scheduling, ST-EVO can also perceive the uncertainty of MAS, and possesses self-feedback ability to learn from accumulated experience. Extensive experiments on nine benchmarks demonstrate the state-of-the-art performance of ST-EVO, achieving about 5%--25% accuracy improvement.

</details>

**LLM Summary**

- What: マルチエージェント通信トポロジーの生成的な時空間進化を目指す「ST-EVO」を提案。コンパクトで強力なフローマッチングベースのスケジューラを用いて、対話ごとの通信スケジューリングをサポートする。
- Novelty: 時空間的な視点から、対話ごとの通信スケジューリングを可能にする。MASの不確実性を認識し、蓄積された経験から学習する自己フィードバック能力を持つ。
- Why it matters: 現在の自己進化型MASが単一次元の進化しか考慮していないという限界を克服し、LLMの協調能力を最大限に引き出す。実験で最先端のパフォーマンスを達成。

## 9. Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow
- arXiv: http://arxiv.org/abs/2602.14587v1
- PDF: https://arxiv.org/pdf/2602.14587v1
- Authors: Minh Nguyen
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.

</details>

**LLM Summary**

- What: ハミルトニアンフローを用いた、分離された連続時間強化学習（RL）アルゴリズムを提案。Q関数と価値関数を分離し、連続時間での学習を安定化させる。
- Novelty: Q関数をVから拡散生成器で学習し、Vをハミルトニアンベースの値フローで更新する交互更新を採用。無限小の時間ステップでも情報量を保つ。
- Why it matters: 従来の離散時間RLが苦手とする連続時間制御問題（金融、ロボット工学など）において、より信頼性の高い学習と収束を保証する。

## 10. TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning
- arXiv: http://arxiv.org/abs/2602.14482v1
- PDF: https://arxiv.org/pdf/2602.14482v1
- Authors: Hao Ding, Zhichuan Yang, Weijie Ge, Ziqin Gao, Chaoyi Lu, Lei Zhao
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

We address fine-grained visual reasoning in multimodal large language models (MLLMs), where key evidence may reside in tiny objects, cluttered regions, or subtle markings that are lost under a single global image encoding. We introduce TikArt (Thinking Aperture), an aperture-guided agent that casts multi-step vision-language reasoning as a decision process over regions of interest. TikArt follows a Think-Aperture-Observe loop, alternating between language generation and two aperture actions: Zoom extracts rectangular crops, while Segment invokes SAM2 to obtain mask-based crops for irregular targets. After every action, the model must produce an explicit observation, turning local visual cues into persistent linguistic memory. Built on Qwen3-VL-8B, TikArt optimizes its reasoning policy with AGRPO, a GRPO-style reinforcement learning algorithm with a two-stage curriculum: it warms up segmentation actions and then jointly optimizes visual math, fine-grained VQA, and segmentation, using rewards that couple task success with purposeful aperture use. Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over the backbone and yield interpretable aperture trajectories for high-resolution reasoning.

</details>

**LLM Summary**

- What: マルチモーダル大規模言語モデル（MLLM）における、微細な物体や複雑な領域に注目するファイングレイン視覚推論のための「TikArt」を提案。開口部（Aperture）ガイド型の強化学習エージェント。
- Novelty: 「Think-Aperture-Observe」ループを採用し、言語生成と「Zoom」（矩形クロップ）、「Segment」（SAM2によるマスクベースクロップ）の開口部アクションを交互に行う。
- Why it matters: 単一のグローバル画像エンコーディングでは失われがちな重要な証拠を捉え、より正確な視覚推論を可能にする。様々なベンチマークで性能向上を示す。

## 11. RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems
- arXiv: http://arxiv.org/abs/2602.14438v1
- PDF: https://arxiv.org/pdf/2602.14438v1
- Authors: Hamid Khabazi, Ali F. Meghdari, Alireza Taheri
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

This study proposes an intelligent multi-agent framework built on LLMs and VLMs and specifically tailored to robotics. The goal is to integrate the strengths of LLMs and VLMs with computational tools to automatically analyze and solve problems related to robotic manipulators. Our developed framework accepts both textual and visual inputs and can automatically perform forward and inverse kinematics, compute velocities and accelerations of key points, generate 3D simulations of the robot, and ultimately execute motion control within the simulated environment, all according to the user's query. To evaluate the framework, three benchmark tests were designed, each consisting of ten questions. In the first benchmark test, the framework was evaluated while connected to GPT-4o, DeepSeek-V3.2, and Claude-Sonnet-4.5, as well as their corresponding raw models. The objective was to extract the forward kinematics of robots directly from textual descriptions. The results showed that the framework integrated with GPT-4o achieved the highest accuracy, reaching 0.97 in computing the final solution, whereas the raw model alone attained an accuracy of only 0.30 for the same task. Similarly, for the other two models, the framework consistently outperformed the corresponding raw models in terms of accuracy. The second benchmark test was identical to the first, except that the input was provided in visual form. In this test, the GPT-4o LLM was used alongside the Gemini 2.5 Pro VLM. The results showed that the framework achieved an accuracy of 0.93 in obtaining the final answer, which is approximately 20% higher than that of the corresponding raw model. The third benchmark test encompassed a range of robotic tasks, including simulation, control, velocity and acceleration computation, as well as inverse kinematics and Jacobian calculation, for which the framework achieved an accuracy of 0.97.

</details>

**LLM Summary**

- What: ロボットアームの問題解決に特化した、LLMとVLMを統合したマルチエージェントフレームワーク「RoboSolver」を提案。テキストと画像の両方の入力を受け付け、順運動学、逆運動学、速度・加速度計算、3Dシミュレーション、運動制御を自動実行する。
- Novelty: LLMとVLMの能力を計算ツールと統合し、ロボットアームの解析・解決を自動化するフレームワーク。特にGPT-4oとの統合で、テキストからの順運動学抽出精度が大幅に向上した。
- Why it matters: ロボットアームの複雑な問題を、より少ない手作業で、高精度かつ効率的に解決できるようになる。

## 12. Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models
- arXiv: http://arxiv.org/abs/2602.14236v1
- PDF: https://arxiv.org/pdf/2602.14236v1
- Authors: Vishnu Sai, Dheeraj Sai, Srinath B, Girish Varma, Priyesh Shukla
- Keyword score: 3 / hits: video understanding

<details><summary>Abstract</summary>

Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.

</details>

**LLM Summary**

- What: 長尺動画理解におけるVLMsのメモリボトルネックを解消するため、双信号適応型KVキャッシュ最適化手法「Sali-Cache」を提案。
- Novelty: 光学フローと顕著性検出に基づく時間的・空間的フィルタリングを組み合わせ、計算コストの高いアテンション計算前にメモリ割り当てをインテリジェントに管理する先行的メモリ管理フレームワーク。
- Why it matters: メモリ使用量を大幅に圧縮しつつ、長尺動画でも文脈情報を維持し、精度を損なわずにVLMsの性能を向上させる。

## 13. HiVid: LLM-Guided Video Saliency For Content-Aware VOD And Live Streaming
- arXiv: http://arxiv.org/abs/2602.14214v1
- PDF: https://arxiv.org/pdf/2602.14214v1
- Authors: Jiahui Chen, Bo Peng, Lianchen Jia, Zeyu Zhang, Tianchi Huang, Lifeng Sun
- Keyword score: 3 / hits: streaming

<details><summary>Abstract</summary>

Content-aware streaming requires dynamic, chunk-level importance weights to optimize subjective quality of experience (QoE). However, direct human annotation is prohibitively expensive while vision-saliency models generalize poorly. We introduce HiVid, the first framework to leverage Large Language Models (LLMs) as a scalable human proxy to generate high-fidelity weights for both Video-on-Demand (VOD) and live streaming. We address 3 non-trivial challenges: (1) To extend LLMs' limited modality and circumvent token limits, we propose a perception module to assess frames in a local context window, autoregressively building a coherent understanding of the video. (2) For VOD with rating inconsistency across local windows, we propose a ranking module to perform global re-ranking with a novel LLM-guided merge-sort algorithm. (3) For live streaming which requires low-latency, online inference without future knowledge, we propose a prediction module to predict future weights with a multi-modal time series model, which comprises a content-aware attention and adaptive horizon to accommodate asynchronous LLM inference. Extensive experiments show HiVid improves weight prediction accuracy by up to 11.5\% for VOD and 26\% for live streaming over SOTA baselines. Real-world user study validates HiVid boosts streaming QoE correlation by 14.7\%.

</details>

**LLM Summary**

- What: LLMを活用して、オンデマンド（VOD）およびライブストリーミング向けのコンテンツ認識型ストリーミングを実現するフレームワーク「HiVid」を提案。
- Novelty: LLMをスケーラブルな人間代理として利用し、高忠実度の動画重要度重みを生成。ローカルコンテキストウィンドウでのフレーム評価、グローバル再ランキング、低遅延オンライン推論のための将来重み予測を行う。
- Why it matters: ストリーミングの主観的品質を向上させ、VODやライブストリーミングの体験を最適化する。

## 14. It Takes Two to Tango: A Holistic Simulator for Joint Order Scheduling and Multi-Agent Path Finding in Robotic Warehouses
- arXiv: http://arxiv.org/abs/2602.13999v1
- PDF: https://arxiv.org/pdf/2602.13999v1
- Authors: Haozheng Xu, Wenhao Li, Zifan Wei, Bo Jin, Hongxing Bai, Ben Yang, Xiangfeng Wang
- Keyword score: 3 / hits: multi-agent

<details><summary>Abstract</summary>

The prevailing paradigm in Robotic Mobile Fulfillment Systems (RMFS) typically treats order scheduling and multi-agent pathfinding as isolated sub-problems. We argue that this decoupling is a fundamental bottleneck, masking the critical dependencies between high-level dispatching and low-level congestion. Existing simulators fail to bridge this gap, often abstracting away heterogeneous kinematics and stochastic execution failures. We propose WareRover, a holistic simulation platform that enforces a tight coupling between OS and MAPF via a unified, closed-loop optimization interface. Unlike standard benchmarks, WareRover integrates dynamic order streams, physics-aware motion constraints, and non-nominal recovery mechanisms into a single evaluation loop. Experiments reveal that SOTA algorithms often falter under these realistic coupled constraints, demonstrating that WareRover provides a necessary and challenging testbed for robust, next-generation warehouse coordination. The project and video is available at https://hhh-x.github.io/WareRover/.

</details>

**LLM Summary**

- What: ロボット倉庫における注文スケジューリングとマルチエージェント経路探索（MAPF）を統合した、ホリスティックなシミュレーター「WareRover」を提案。
- Novelty: 注文スケジューリングとMAPFを分離せず、統一されたインターフェースで緊密に結合。動的な注文ストリーム、物理的な運動制約、非公称回復メカニズムを統合した評価ループを持つ。
- Why it matters: 現実的な制約下でのロボット倉庫の協調動作の頑健性を評価するための、より挑戦的なテストベッドを提供する。

## 15. WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL
- arXiv: http://arxiv.org/abs/2602.13977v1
- PDF: https://arxiv.org/pdf/2602.13977v1
- Authors: Zhennan Jiang, Shangqing Zhou, Yutong Jiang, Zefang Huang, Mingjie Wei, Yuhui Chen, Tianxing Zhou, Zhen Guo...
- Keyword score: 3 / hits: vision-language-action, reinforcement learning, imitation learning

<details><summary>Abstract</summary>

Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.

</details>

**LLM Summary**

- What: 強化学習（RL）を用いたVLA（Vision-Language-Action）モデルのポストトレーニングのために、信頼性の高いワールドモデルベースのシミュレーターフレームワーク「WoVR」を提案。
- Novelty: 不完全なワールドモデルとの相互作用を明示的に調整し、制御可能なアクション条件付きビデオワールドモデルによるロールアウト安定化、キーフレーム初期化ロールアウトによる実効誤差深度の削減、ワールドモデルとポリシーの協調進化による整合性維持を行う。
- Why it matters: RLを用いたVLAモデルの学習を、現実世界での大規模な相互作用なしに、より安全かつ効果的に行うことを可能にする。

## 16. ProAct: A Dual-System Framework for Proactive Embodied Social Agents
- arXiv: http://arxiv.org/abs/2602.14048v1
- PDF: https://arxiv.org/pdf/2602.14048v1
- Authors: Zeyi Zhang, Zixi Kang, Ruijie Zhao, Yusen Feng, Biao Jiang, Libin Liu
- Keyword score: 2 / hits: streaming, real-time

<details><summary>Abstract</summary>

Embodied social agents have recently advanced in generating synchronized speech and gestures. However, most interactive systems remain fundamentally reactive, responding only to current sensory inputs within a short temporal window. Proactive social behavior, in contrast, requires deliberation over accumulated context and intent inference, which conflicts with the strict latency budget of real-time interaction. We present \emph{ProAct}, a dual-system framework that reconciles this time-scale conflict by decoupling a low-latency \emph{Behavioral System} for streaming multimodal interaction from a slower \emph{Cognitive System} which performs long-horizon social reasoning and produces high-level proactive intentions. To translate deliberative intentions into continuous non-verbal behaviors without disrupting fluency, we introduce a streaming flow-matching model conditioned on intentions via ControlNet. This mechanism supports asynchronous intention injection, enabling seamless transitions between reactive and proactive gestures within a single motion stream. We deploy ProAct on a physical humanoid robot and evaluate both motion quality and interactive effectiveness. In real-world interaction user studies, participants and observers consistently prefer ProAct over reactive variants in perceived proactivity, social presence, and overall engagement, demonstrating the benefits of dual-system proactive control for embodied social interaction.

</details>

**LLM Summary**

- What: 低遅延の行動システムと、長期的な社会的推論を行う遅延のある認知システムを組み合わせた、二重システムフレームワーク「ProAct」を提案。
- Novelty: 意図をControlNetで条件付けたストリーミングフローマッチングモデルを導入し、非同期的な意図注入と、リアクティブな動作とプロアクティブな動作のシームレスな移行を可能にした。
- Why it matters: リアルタイムなインタラクションの遅延制約と、長期的な社会的推論の必要性を両立させ、より高度で自然な対話が可能なロボットエージェントの実現に貢献する。

## 17. Scalable Multi-Robot Path Planning via Quadratic Unconstrained Binary Optimization
- arXiv: http://arxiv.org/abs/2602.14799v1
- PDF: https://arxiv.org/pdf/2602.14799v1
- Authors: Javier González Villasmil
- Keyword score: 1 / hits: multi-agent

<details><summary>Abstract</summary>

Multi-Agent Path Finding (MAPF) remains a fundamental challenge in robotics, where classical centralized approaches exhibit exponential growth in joint-state complexity as the number of agents increases. This paper investigates Quadratic Unconstrained Binary Optimization (QUBO) as a structurally scalable alternative for simultaneous multi-robot path planning. This approach is a robotics-oriented QUBO formulation incorporating BFS-based logical pre-processing (achieving over 95% variable reduction), adaptive penalty design for collision and constraint enforcement, and a time-windowed decomposition strategy that enables execution within current hardware limitations. An experimental evaluation in grid environments with up to four robots demonstrated near-optimal solutions in dense scenarios and favorable scaling behavior compared to sequential classical planning. These results establish a practical and reproducible baseline for future quantum and quantum-inspired multi-robot coordinations.

</details>

**LLM Summary**

- What: 多ロボット経路計画問題に対し、Quadratic Unconstrained Binary Optimization (QUBO) を用いたスケーラブルなアプローチを提案。
- Novelty: BFSによる論理的前処理、適応的なペナルティ設計、時間窓分解戦略を組み合わせたロボティクス指向のQUBO定式化を開発。
- Why it matters: ロボット数が増加しても指数関数的に増大する計算量を回避し、実用的なハードウェアで実行可能な、スケーラブルで効率的な多ロボット協調経路計画を実現する。

## 18. Simulation-based Learning of Electrical Cabinet Assembly Using Robot Skills
- arXiv: http://arxiv.org/abs/2602.14561v1
- PDF: https://arxiv.org/pdf/2602.14561v1
- Authors: Arik Laemmle, Balázs András Bálint, Philipp Tenbrock, Frank Naegele, David Traunecker, József Váncza, Marco F. Huber
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

This paper presents a simulation-driven approach for automating the force-controlled assembly of electrical terminals on DIN-rails, a task traditionally hindered by high programming effort and product variability. The proposed method integrates deep reinforcement learning (DRL) with parameterizable robot skills in a physics-based simulation environment. To realistically model the snap-fit assembly process, we develop and evaluate two types of joining models: analytical models based on beam theory and rigid-body models implemented in the MuJoCo physics engine. These models enable accurate simulation of interaction forces, essential for training DRL agents. The robot skills are structured using the pitasc framework, allowing modular, reusable control strategies. Training is conducted in simulation using Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithms. Domain randomization is applied to improve robustness. The trained policies are transferred to a physical UR10e robot system without additional tuning. Experimental results demonstrate high success rates (up to 100%) in both simulation and real-world settings, even under significant positional and rotational deviations. The system generalizes well to new terminal types and positions, significantly reducing manual programming effort. This work highlights the potential of combining simulation-based learning with modular robot skills for flexible, scalable automation in small-batch manufacturing. Future work will explore hybrid learning methods, automated environment parameterization, and further refinement of joining models for design integration.

</details>

**LLM Summary**

- What: シミュレーションベースの深層強化学習（DRL）とパラメータ化可能なロボットスキルを統合し、電気盤の端子組み立てを自動化する手法を提案。
- Novelty: ビーム理論に基づく解析モデルとMuJoCoエンジンを用いた剛体モデルを組み合わせた、スナップフィット組み立ての正確なシミュレーションモデルを開発。
- Why it matters: プログラミング労力と製品変動性の問題を解決し、物理シミュレーションで学習したポリシーを実機にチューニングなしで適用可能にすることで、ロボットによる組み立て作業の自動化を促進する。

## 19. Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction
- arXiv: http://arxiv.org/abs/2602.14551v1
- PDF: https://arxiv.org/pdf/2602.14551v1
- Authors: Taichi Kato, Takuya Kiyokawa, Namiko Saito, Kensuke Harada
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Human-Robot Collaboration (HRC) plays an important role in assembly tasks by enabling robots to plan and adjust their motions based on interactive, real-time human instructions. However, such instructions are often linguistically ambiguous and underspecified, making it difficult to generate physically feasible and cooperative robot behaviors. To address this challenge, many studies have applied Vision-Language Models (VLMs) to interpret high-level instructions and generate corresponding actions. Nevertheless, VLM-based approaches still suffer from hallucinated reasoning and an inability to anticipate physical execution failures. To address these challenges, we propose an HRC framework that augments a VLM-based reasoning with a dual-correction mechanism: an internal correction model that verifies logical consistency and task feasibility prior to action execution, and an external correction model that detects and rectifies physical failures through post-execution feedback. Simulation ablation studies demonstrate that the proposed method improves the success rate compared to baselines without correction models. Our real-world experiments in collaborative assembly tasks supported by object fixation or tool preparation by an upper body humanoid robot further confirm the framewor's effectiveness in enabling interactive replanning across different collaborative tasks in response to human instructions, validating its practical feasibility.

</details>

**LLM Summary**

- What: 人間とロボットの協調タスクにおいて、Vision-Language Models (VLMs) を利用した指示理解と、意味的・物理的な二重補正メカニズムを組み合わせたリプランニングフレームワークを提案。
- Novelty: VLMの推論を、タスク実行前の論理的一貫性と実現可能性を検証する内部補正モデルと、実行後の物理的失敗を検出・修正する外部補正モデルで拡張。
- Why it matters: 言語的に曖昧な指示や物理的な実行失敗に対処し、より安全で効率的な人間とロボットの協調作業を実現する。

## 20. MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation
- arXiv: http://arxiv.org/abs/2602.14534v1
- PDF: https://arxiv.org/pdf/2602.14534v1
- Authors: Hongpeng Wang, Zeyu Zhang, Wenhao Li, Hao Tang
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.

</details>

**LLM Summary**

- What: 人間の動作理解と生成を統一する、強化学習を用いたマルチモーダルモデル「MoRL」を提案。
- Novelty: 意味的整合性、推論の一貫性、物理的妥当性、テキストと動作の一貫性を組み合わせたタスク固有の報酬設計と、テスト時の段階的計画・反省を可能にする「Chain-of-Motion (CoM)」を導入。
- Why it matters: 動作理解と生成の両方において、論理的推論能力と知覚的リアリズムを向上させ、より高度で人間らしい動作の理解と生成を可能にする。
