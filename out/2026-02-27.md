# Daily CV Digest (2026-02-27)

- Total: 20

## 1. SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation
- arXiv: http://arxiv.org/abs/2602.22514v1
- PDF: https://arxiv.org/pdf/2602.22514v1
- Authors: Xinyu Tan, Ningwei Bai, Harry Gardener, Zhengyang Zhong, Luoyu Zhang, Liuhaichen Yang, Zhekai Duan, Monkgogi Galeitsiwe...
- Keyword score: 6 / hits: real-time, vision-language-action

<details><summary>Abstract</summary>

We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction. In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction. Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.

</details>

**LLM Summary**

- What: 手話によるロボット操作のための、注釈不要なリアルタイム Vision-Language-Action (VLA) フレームワークを提案。手話ジェスチャーを直接意味のある指示にマッピングする。
- Novelty: グロス（手話の単語表現）に依存しない直接的なマッピング。アルファベットレベルの指文字インターフェースに焦点を当てる。
- Why it matters: 注釈コストの削減、情報損失の回避、より自然でスケーラブルなマルチモーダル対話、ロボット制御の信頼性・解釈性・展開可能性の向上。

## 2. MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding
- arXiv: http://arxiv.org/abs/2602.22932v1
- PDF: https://arxiv.org/pdf/2602.22932v1
- Authors: Wenhui Tan, Xiaoyi Yu, Jiaze Li, Yijing Chen, Jianzhong Ju, Zhenbo Luo, Ruihua Song, Jian Luan
- Keyword score: 4 / hits: video understanding, reinforcement learning

<details><summary>Abstract</summary>

Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\% accuracy gain upon the base MLLM, and 1.1\% higher accuracy than strongest baseline method.

</details>

**LLM Summary**

- What: 長尺動画理解のための、マルチモーダル大規模言語モデル (MLLM) と軽量キーフレームサンプラーを共同で進化させるフレームワーク (MSJoE) を提案。
- Novelty: 質問に関連する多様な視点からのクエリを生成し、それらと CLIP モデルを相互作用させてクエリ-フレーム類似度行列を生成。この行列からサンプラーがキーフレームのサンプリング重みを予測する。
- Why it matters: 長尺動画の効率的な理解を可能にし、MLLM とサンプラーの協調的な最適化により、クエリ推論、フレームサンプリング、キーフレーム理解の精度を向上させる。

## 3. DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation
- arXiv: http://arxiv.org/abs/2602.22896v1
- PDF: https://arxiv.org/pdf/2602.22896v1
- Authors: Zebin Yang, Yijiahao Qi, Tong Xie, Bo Yu, Shaoshan Liu, Meng Li
- Keyword score: 4 / hits: real-time, vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.

</details>

**LLM Summary**

- What: ロボット操作における Vision-Language-Action (VLA) モデルの推論を効率化するため、アクションの重要度に応じて動的に VLA レイヤーをスキップするフレームワーク (DySL-VLA) を提案。
- Novelty: レイヤーを「情報レイヤー」と「増分レイヤー」に分類し、増分レイヤーを選択的にスキップ。スキップの開始タイミングを決定する「事前-事後スキップガイダンス機構」を導入。
- Why it matters: 計算コストを削減しつつ、精度を維持しながらリアルタイム性能を実現。ロボット操作における VLA モデルの応用範囲を広げる。

## 4. Pixel2Catch: Multi-Agent Sim-to-Real Transfer for Agile Manipulation with a Single RGB Camera
- arXiv: http://arxiv.org/abs/2602.22733v1
- PDF: https://arxiv.org/pdf/2602.22733v1
- Authors: Seongyong Kim, Junhyeon Cho, Kang-Won Lee, Soo-Chul Lim
- Keyword score: 4 / hits: reinforcement learning, multi-agent

<details><summary>Abstract</summary>

To catch a thrown object, a robot must be able to perceive the object's motion and generate control actions in a timely manner. Rather than explicitly estimating the object's 3D position, this work focuses on a novel approach that recognizes object motion using pixel-level visual information extracted from a single RGB image. Such visual cues capture changes in the object's position and scale, allowing the policy to reason about the object's motion. Furthermore, to achieve stable learning in a high-DoF system composed of a robot arm equipped with a multi-fingered hand, we design a heterogeneous multi-agent reinforcement learning framework that defines the arm and hand as independent agents with distinct roles. Each agent is trained cooperatively using role-specific observations and rewards, and the learned policies are successfully transferred from simulation to the real world.

</details>

**LLM Summary**

- What: 単一の RGB カメラからのピクセルレベルの視覚情報を用いて物体の動きを認識し、アジャイルな操作を行うためのマルチエージェント sim-to-real 転移手法 (Pixel2Catch) を提案。
- Novelty: 物体の3D位置を明示的に推定せず、ピクセルレベルの視覚的特徴（位置とスケールの変化）から物体の動きを推論。アームとハンドを独立したエージェントとする異種マルチエージェント強化学習フレームワークを採用。
- Why it matters: シミュレーションから実世界への安定した学習と転移を可能にし、単一の RGB カメラで複雑な操作（物体のキャッチ）を実現する。

## 5. Skarimva: Skeleton-based Action Recognition is a Multi-view Application
- arXiv: http://arxiv.org/abs/2602.23231v1
- PDF: https://arxiv.org/pdf/2602.23231v1
- Authors: Daniel Bermuth, Alexander Poeppel, Wolfgang Reif
- Keyword score: 3 / hits: action recognition

<details><summary>Abstract</summary>

Human action recognition plays an important role when developing intelligent interactions between humans and machines. While there is a lot of active research on improving the machine learning algorithms for skeleton-based action recognition, not much attention has been given to the quality of the input skeleton data itself. This work demonstrates that by making use of multiple camera views to triangulate more accurate 3D~skeletons, the performance of state-of-the-art action recognition models can be improved significantly. This suggests that the quality of the input data is currently a limiting factor for the performance of these models. Based on these results, it is argued that the cost-benefit ratio of using multiple cameras is very favorable in most practical use-cases, therefore future research in skeleton-based action recognition should consider multi-view applications as the standard setup.

</details>

**LLM Summary**

- What: スケルトンベースのアクション認識において、複数のカメラビューからより正確な3Dスケルトンを三角測量することで、最先端のアクション認識モデルの性能を大幅に向上させることを実証。
- Novelty: 入力スケルトンデータの品質がアクション認識モデルの性能の限界要因であることを指摘し、マルチビューアプリケーションの有効性を示す。
- Why it matters: アクション認識の精度を向上させ、人間と機械のインテリジェントなインタラクションの実現に貢献する。実用的なユースケースにおいて、マルチカメラのコストメリットが高いことを示唆。

## 6. EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents
- arXiv: http://arxiv.org/abs/2602.23205v1
- PDF: https://arxiv.org/pdf/2602.23205v1
- Authors: Wenjia Wang, Liang Pan, Huaijin Pi, Yuke Lou, Xuqian Ren, Yifan Wu, Zhouyingcheng Liao, Lei Yang...
- Keyword score: 3 / hits: embodied agent

<details><summary>Abstract</summary>

Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.

</details>

**LLM Summary**

- What: 2台のiPhoneを用いて、現実世界での人間とシーンの4D再構成を行うためのポータブルで安価なデータ収集パイプライン「EmbodMocap」を提案。RGB-Dシーケンスを共同でキャリブレーションし、人間とシーンを統一されたメトリック座標系で再構成する。
- Novelty: 従来のスタジオセットアップやウェアラブルデバイスに依存しない、日常環境でのメトリックスケールかつシーン整合的なキャプチャを実現。デュアルビュー設定による深度曖昧性の軽減と、単一iPhoneや単眼モデルに対する優れた再構成性能。
- Why it matters: 身体性を持つエージェント（ロボットなど）の知覚、理解、行動を訓練するための、大規模でシーン連動型の人間運動データを現実世界で収集可能にする。これにより、モンキュラー人間-シーン再構成などのエンボディドAIタスクを強化できる。

## 7. FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time
- arXiv: http://arxiv.org/abs/2602.23115v1
- PDF: https://arxiv.org/pdf/2602.23115v1
- Authors: David Dirnfeld, Fabien Delattre, Pedro Miraldo, Erik Learned-Miller
- Keyword score: 3 / hits: real-time

<details><summary>Abstract</summary>

Estimating camera motion from monocular video is a fundamental problem in computer vision, central to tasks such as SLAM, visual odometry, and structure-from-motion. Existing methods that recover the camera's heading under known rotation, whether from an IMU or an optimization algorithm, tend to perform well in low-noise, low-outlier conditions, but often decrease in accuracy or become computationally expensive as noise and outlier levels increase. To address these limitations, we propose a novel generalization of the Hough transform on the unit sphere (S(2)) to estimate the camera's heading. First, the method extracts correspondences between two frames and generates a great circle of directions compatible with each pair of correspondences. Then, by discretizing the unit sphere using a Fibonacci lattice as bin centers, each great circle casts votes for a range of directions, ensuring that features unaffected by noise or dynamic objects vote consistently for the correct motion direction. Experimental results on three datasets demonstrate that the proposed method is on the Pareto frontier of accuracy versus efficiency. Additionally, experiments on SLAM show that the proposed method reduces RMSE by correcting the heading during camera pose initialization.

</details>

**LLM Summary**

- What: 単眼ビデオからのカメラ姿勢推定（特にヨー方向）において、ノイズや外れ値が多い状況でも高精度かつ効率的な推定を可能にする「FLIGHT」を提案。単位球面上の一般化されたHough変換を用いる。
- Novelty: フィボナッチ格子を用いた単位球の離散化により、ノイズや動体オブジェクトの影響を受けない特徴も一貫して正しい運動方向へ投票させる。精度と効率のトレードオフにおいて、パレートフロンティア上に位置する。
- Why it matters: SLAM、ビジュアルオドメトリ、Structure-from-Motionなどの基本的なコンピュータビジョンタスクにおいて、ロバストで計算効率の良いカメラ姿勢推定を実現し、これらの応用分野の性能向上に貢献する。

## 8. FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning
- arXiv: http://arxiv.org/abs/2602.22963v1
- PDF: https://arxiv.org/pdf/2602.22963v1
- Authors: Zehao Li, Hongwei Yu, Hao Jiang, Qiang Sheng, Yilong Xu, Baolong Bi, Yang Li, Zhenlong Yuan...
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.

</details>

**LLM Summary**

- What: マルチモーダル大規模言語モデル（MLLM）を用いたビデオ誤情報検出において、外部ツールを効果的に活用し、不確実性を管理しながら段階的に推論を進めるエージェント型フレームワーク「FactGuard」を提案。
- Novelty: タスクの曖昧性を明示的に評価し、外部ツールを戦略的に呼び出すことで、推論軌跡を段階的に洗練させる。ドメイン固有のエージェント型教師ありファインチューニングと、リスクを考慮した意思決定を最適化する決定認識型強化学習を組み合わせた2段階学習戦略。
- Why it matters: 証拠が希薄または断片的な状況下でも、ビデオ誤情報の検出精度と頑健性を大幅に向上させる。MLLMの過信を抑制し、より信頼性の高い誤情報検出システムを構築できる。

## 9. Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning
- arXiv: http://arxiv.org/abs/2602.22703v1
- PDF: https://arxiv.org/pdf/2602.22703v1
- Authors: Hao Yu, Shuning Jia, Guanghao Li, Wenhao Jiang, Chun Yuan
- Keyword score: 3 / hits: reinforcement learning

<details><summary>Abstract</summary>

Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\%$ on in-domain data, $+8.0\%$ on out-of-domain data, and $+39.0\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive to ensure reproducibility.

</details>

**LLM Summary**

- What: Vision-Language Models (VLMs) の幾何学的知覚能力を向上させるため、図解インスタンスとドメイン固有言語（DSL）表現のペアからなるベンチマーク「GeoPerceive」と、それを活用する翻訳者誘導型強化学習フレームワーク「GeoDPO」を提案。
- Novelty: GeoPerceiveは、幾何学的知覚を推論から独立して評価可能。GeoDPOは、自然言語とDSLを橋渡しする翻訳者を用いて、DSLレベルの細かいスコアを強化学習の報酬信号として利用する。
- Why it matters: VLMが図解要素の知覚に苦労する問題を解決し、幾何学的知覚能力を向上させる。これにより、幾何学的な推論を必要とする下流タスクの性能改善が期待できる。

## 10. Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline
- arXiv: http://arxiv.org/abs/2602.22663v1
- PDF: https://arxiv.org/pdf/2602.22663v1
- Authors: Wenxuan Song, Jiayi Chen, Xiaoquan Sun, Huashuo Lei, Yikai Qin, Wei Zhao, Pengxiang Ding, Han Zhao...
- Keyword score: 3 / hits: vision-language-action

<details><summary>Abstract</summary>

Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect 14.4k simulated trajectories and 1.6k real-world expert-curated trajectories to support training on CEBench. Second, using CEBench as our testbed, we study three critical aspects of VLAs' practicality and offer several key findings. Informed by these findings, we introduce LLaVA-VLA, a lightweight yet powerful VLA designed for practical deployment on consumer-grade GPUs. Architecturally, it integrates a compact VLM backbone with multi-view perception, proprioceptive tokenization, and action chunking. To eliminate reliance on costly pre-training, LLaVA-VLA adopts a two-stage training paradigm including post-training and fine-tuning. Furthermore, LLaVA-VLA extends the action space to unify navigation and manipulation. Experiments across embodiments demonstrate the capabilities of generalization and versatility of LLaVA-VLA , while real-world mobile manipulation experiments establish it as the first end-to-end VLA model for mobile manipulation. We will open-source all datasets, codes, and checkpoints upon acceptance to foster reproducibility and future research.

</details>

**LLM Summary**

- What: 実用的なVision-Language-Action (VLA) モデルの実現を目指し、多様なロボット環境に対応する包括的なベンチマーク「CEBench」と、軽量かつ高性能なベースラインモデル「LLaVA-VLA」を提案。
- Novelty: CEBenchは、シミュレーションと実世界の両方で多様なロボット環境を網羅。LLaVA-VLAは、コンパクトなVLMバックボーン、マルチビュー知覚、プロプリオセプティブトークン化、アクションチャンキングを統合し、高価な事前学習に依存しない2段階学習パラダイムを採用。
- Why it matters: VLAモデルの過剰なパラメータサイズ、高コストな事前学習、限定的な適用範囲といった実用上の課題を克服する。コンシューマーグレードのGPUでも展開可能な、汎用性の高いロボットエージェントの実現を促進する。

## 11. EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow
- arXiv: http://arxiv.org/abs/2602.22461v1
- PDF: https://arxiv.org/pdf/2602.22461v1
- Authors: Daesol Cho, Youngseok Jang, Danfei Xu, Sehoon Ha
- Keyword score: 3 / hits: robot policy

<details><summary>Abstract</summary>

Egocentric human videos provide a scalable source of manipulation demonstrations; however, deploying them on robots requires active viewpoint control to maintain task-critical visibility, which human viewpoint imitation often fails to provide due to human-specific priors. We propose EgoAVFlow, which learns manipulation and active vision from egocentric videos through a shared 3D flow representation that supports geometric visibility reasoning and transfers without robot demonstrations. EgoAVFlow uses diffusion models to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints at test time with reward-maximizing denoising under a visibility-aware reward computed from predicted motion and scene geometry. Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.

</details>

**LLM Summary**

- What: ロボットが人間の視点からの動画から操作とアクティブビジョン（能動的な視点制御）を学習する手法「EgoAVFlow」を提案。3Dフロー表現を共有することで、幾何学的な可視性推論を可能にし、ロボットのデモンストレーションなしで転移学習を実現する。
- Novelty: 人間の視点動画から、ロボットの操作と能動的な視点制御を同時に学習する点。3Dフロー表現を用いた幾何学的な可視性推論。
- Why it matters: ロボットが人間の操作デモを、より汎用的に、かつ能動的な視点制御能力を持って学習できるようになる。これにより、ロボットの操作能力と環境適応能力が向上する。

## 12. CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines
- arXiv: http://arxiv.org/abs/2602.22452v1
- PDF: https://arxiv.org/pdf/2602.22452v1
- Authors: Chayan Banerjee
- Keyword score: 3 / hits: embodied agent

<details><summary>Abstract</summary>

A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>

**LLM Summary**

- What: 身体性エージェントのパイプラインにおける行動の実現可能性を学習するための「Contrastive World Model (CWM)」を提案。LLMをファインチューニングし、正しい行動と誤った行動を対比させることで、物理的に実行可能な行動をより正確に識別する。
- Novelty: InfoNCE対照学習目的と、特に意味的に類似しているが物理的に互換性のない「ハードネガティブ」な例を用いることで、行動の実現可能性スコアラーの精度を向上させる点。
- Why it matters: ロボットやエージェントが、計画や推論を行う前に、実行可能な行動を正確に判断できるようになる。これにより、エージェントの意思決定の信頼性と効率性が向上する。

## 13. Align then Adapt: Rethinking Parameter-Efficient Transfer Learning in 4D Perception
- arXiv: http://arxiv.org/abs/2602.23069v1
- PDF: https://arxiv.org/pdf/2602.23069v1
- Authors: Yiding Sun, Jihua Zhu, Haozhe Cheng, Chaoyi Lu, Zhichuan Yang, Lin Chen, Yaonan Wang
- Keyword score: 2 / hits: action recognition, video understanding

<details><summary>Abstract</summary>

Point cloud video understanding is critical for robotics as it accurately encodes motion and scene interaction. We recognize that 4D datasets are far scarcer than 3D ones, which hampers the scalability of self-supervised 4D models. A promising alternative is to transfer 3D pre-trained models to 4D perception tasks. However, rigorous empirical analysis reveals two critical limitations that impede transfer capability: overfitting and the modality gap. To overcome these challenges, we develop a novel "Align then Adapt" (PointATA) paradigm that decomposes parameter-efficient transfer learning into two sequential stages. Optimal-transport theory is employed to quantify the distributional discrepancy between 3D and 4D datasets, enabling our proposed point align embedder to be trained in Stage 1 to alleviate the underlying modality gap. To mitigate overfitting, an efficient point-video adapter and a spatial-context encoder are integrated into the frozen 3D backbone to enhance temporal modeling capacity in Stage 2. Notably, with the above engineering-oriented designs, PointATA enables a pre-trained 3D model without temporal knowledge to reason about dynamic video content at a smaller parameter cost compared to previous work. Extensive experiments show that PointATA can match or even outperform strong full fine-tuning models, whilst enjoying the advantage of parameter efficiency, e.g. 97.21 \% accuracy on 3D action recognition, $+8.7 \%$ on 4 D action segmentation, and 84.06\% on 4D semantic segmentation.

</details>

**LLM Summary**

- What: 3D事前学習モデルを4D（点群動画）認識タスクに転移学習させる際の課題（過学習とモダリティギャップ）を解決する「Align then Adapt (PointATA)」パラダイムを提案。
- Novelty: 最適輸送理論を用いて3Dと4Dデータセット間の分布の不一致を定量化し、モダリティギャップを軽減するアライメント段階と、過学習を抑制し時間モデリング能力を強化するアダプト段階を分離したこと。
- Why it matters: 3Dデータで事前学習されたモデルを、より少ないデータで4D（動的な点群）認識タスクに効率的に適用できるようになる。これにより、ロボットビジョンにおける動的なシーン理解の能力が向上する。

## 14. Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving
- arXiv: http://arxiv.org/abs/2602.23259v1
- PDF: https://arxiv.org/pdf/2602.23259v1
- Authors: Jiangxin Sun, Feng Xue, Teng Long, Chang Liu, Jian-Fang Hu, Wei-Shi Zheng, Nicu Sebe
- Keyword score: 1 / hits: imitation learning

<details><summary>Abstract</summary>

With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of "only driving like the expert" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.

</details>

**LLM Summary**

- What: 模倣学習（IL）の限界である、専門家のデモンストレーション外のシナリオでの汎化性能の低さを克服するため、リスクを考慮した世界モデル予測制御（RaWMPC）フレームワークを提案。専門家の行動に依存せず、リスク評価を通じて低リスクな行動を選択する。
- Novelty: 専門家の行動監督に依存せず、世界モデルを用いて複数の候補行動の結果を予測し、明示的なリスク評価に基づいて安全な行動を選択するエンドツーエンド自動運転システム。
- Why it matters: 予期せぬ状況や長尾分布のシナリオにおいても、安全で信頼性の高い自動運転を実現する。これにより、自動運転システムの安全性と汎化性能が向上する。

## 15. Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents
- arXiv: http://arxiv.org/abs/2602.23235v1
- PDF: https://arxiv.org/pdf/2602.23235v1
- Authors: Zhou Xu, Bowen Zhou, Qi Wang, Shuwen Feng, Jingyu Xiao
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's "fading memory" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid integrity required for precise coordinate grounding, inducing spatial hallucinations. To address these challenges, we introduce GUIPruner, a training-free framework tailored for high-resolution GUI navigation. It synergizes Temporal-Adaptive Resolution (TAR), which eliminates historical redundancy via decay-based resizing, and Stratified Structure-aware Pruning (SSP), which prioritizes interactive foregrounds and semantic anchors while safeguarding global layout. Extensive evaluations across diverse benchmarks demonstrate that GUIPruner consistently achieves state-of-the-art performance, effectively preventing the collapse observed in large-scale models under high compression. Notably, on Qwen2-VL-2B, our method delivers a 3.4x reduction in FLOPs and a 3.3x speedup in vision encoding latency while retaining over 94% of the original performance, enabling real-time, high-precision navigation with minimal resource consumption.

</details>

**LLM Summary**

- What: 高解像度GUIエージェントの効率化のため、時空間トークンのプルーニング手法「GUIPruner」を提案。時間的な冗長性を排除する「Temporal-Adaptive Resolution (TAR)」と、空間的な構造を維持しつつ重要な要素を優先する「Stratified Structure-aware Pruning (SSP)」を組み合わせる。
- Novelty: GUIエージェントの「薄れる記憶」パターンに合わせた時間的適応解像度と、グリッド構造を維持する階層的構造認識プルーニングを組み合わせた、トレーニング不要のフレームワーク。
- Why it matters: 高解像度のGUI操作を行うエージェントの計算効率を大幅に向上させ、より複雑なタスクやリアルタイムでの応答性を実現する。

## 16. ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation
- arXiv: http://arxiv.org/abs/2602.23203v1
- PDF: https://arxiv.org/pdf/2602.23203v1
- Authors: Junhu Fu, Shuyu Liang, Wutong Li, Chen Ma, Peng Huang, Kehao Wang, Ke Chen, Shengli Lin...
- Keyword score: 1 / hits: real-time

<details><summary>Abstract</summary>

Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.

</details>

**LLM Summary**

- What: 消化器内視鏡検査の動的でコンテンツを意識したビデオ生成フレームワーク「ColoDiff」を提案した。
- Novelty: 時間的依存性を分離するTimeStreamモジュールと、臨床属性の精密な制御を可能にするContent-Awareモジュールを導入した。また、ステップ数を90%以上削減する非マルコフサンプリング戦略を採用した。
- Why it matters: データ不足のシナリオで診断に不可欠な高品質な内視鏡ビデオを生成し、データ不足を緩和し臨床分析を支援する。

## 17. Towards Intelligible Human-Robot Interaction: An Active Inference Approach to Occluded Pedestrian Scenarios
- arXiv: http://arxiv.org/abs/2602.23109v1
- PDF: https://arxiv.org/pdf/2602.23109v1
- Authors: Kai Chen, Yuyao Huang, Guang Chen
- Keyword score: 1 / hits: reinforcement learning

<details><summary>Abstract</summary>

The sudden appearance of occluded pedestrians presents a critical safety challenge in autonomous driving. Conventional rule-based or purely data-driven approaches struggle with the inherent high uncertainty of these long-tail scenarios. To tackle this challenge, we propose a novel framework grounded in Active Inference, which endows the agent with a human-like, belief-driven mechanism. Our framework leverages a Rao-Blackwellized Particle Filter (RBPF) to efficiently estimate the pedestrian's hybrid state. To emulate human-like cognitive processes under uncertainty, we introduce a Conditional Belief Reset mechanism and a Hypothesis Injection technique to explicitly model beliefs about the pedestrian's multiple latent intentions. Planning is achieved via a Cross-Entropy Method (CEM) enhanced Model Predictive Path Integral (MPPI) controller, which synergizes the efficient, iterative search of CEM with the inherent robustness of MPPI. Simulation experiments demonstrate that our approach significantly reduces the collision rate compared to reactive, rule-based, and reinforcement learning (RL) baselines, while also exhibiting explainable and human-like driving behavior that reflects the agent's internal belief state.

</details>

**LLM Summary**

- What: 隠蔽された歩行者シナリオにおける自律走行車の安全性を向上させるため、アクティブ推論に基づいたフレームワークを提案した。
- Novelty: 人間のような信念駆動型のメカニズムをエージェントに与え、Rao-Blackwellized Particle Filter (RBPF) を使用して歩行者のハイブリッド状態を効率的に推定する。また、条件付き信念リセット機構と仮説注入技術を導入し、歩行者の複数の潜在的意図に関する信念を明示的にモデル化する。
- Why it matters: 衝突率を大幅に削減し、説明可能で人間のような運転行動を示すことで、自律走行車の安全性を向上させる。

## 18. PackUV: Packed Gaussian UV Maps for 4D Volumetric Video
- arXiv: http://arxiv.org/abs/2602.23040v1
- PDF: https://arxiv.org/pdf/2602.23040v1
- Authors: Aashish Rai, Angela Xing, Anushka Agarwal, Xiaoyan Cong, Zekun Li, Tao Lu, Aayush Prakash, Srinath Sridhar
- Keyword score: 1 / hits: streaming

<details><summary>Abstract</summary>

Volumetric videos offer immersive 4D experiences, but remain difficult to reconstruct, store, and stream at scale. Existing Gaussian Splatting based methods achieve high-quality reconstruction but break down on long sequences, temporal inconsistency, and fail under large motions and disocclusions. Moreover, their outputs are typically incompatible with conventional video coding pipelines, preventing practical applications. We introduce PackUV, a novel 4D Gaussian representation that maps all Gaussian attributes into a sequence of structured, multi-scale UV atlas, enabling compact, image-native storage. To fit this representation from multi-view videos, we propose PackUV-GS, a temporally consistent fitting method that directly optimizes Gaussian parameters in the UV domain. A flow-guided Gaussian labeling and video keyframing module identifies dynamic Gaussians, stabilizes static regions, and preserves temporal coherence even under large motions and disocclusions. The resulting UV atlas format is the first unified volumetric video representation compatible with standard video codecs (e.g., FFV1) without losing quality, enabling efficient streaming within existing multimedia infrastructure. To evaluate long-duration volumetric capture, we present PackUV-2B, the largest multi-view video dataset to date, featuring more than 50 synchronized cameras, substantial motion, and frequent disocclusions across 100 sequences and 2B (billion) frames. Extensive experiments demonstrate that our method surpasses existing baselines in rendering fidelity while scaling to sequences up to 30 minutes with consistent quality.

</details>

**LLM Summary**

- What: 4Dボリュームビデオのための新しいガウシアン表現「PackUV」を提案し、ガウシアン属性を構造化されたマルチスケールUVアトラスにマッピングする。
- Novelty: UVドメインで直接ガウシアンパラメータを最適化する時間的に一貫したフィッティング手法「PackUV-GS」を開発した。また、フローガイド付きガウシアンラベリングとビデオキーフレーム化モジュールを導入し、大きな動きやオクルージョン下でも時間的整合性を維持する。
- Why it matters: 標準的なビデオコーデックと互換性のある、コンパクトで画像ネイティブなストレージ形式を提供し、効率的なストリーミングを可能にする。

## 19. WaterVideoQA: ASV-Centric Perception and Rule-Compliant Reasoning via Multi-Modal Agents
- arXiv: http://arxiv.org/abs/2602.22923v1
- PDF: https://arxiv.org/pdf/2602.22923v1
- Authors: Runwei Guan, Shaofeng Liang, Ningwei Ouyang, Weichen Fei, Shanliang Yao, Wei Dai, Chenhao Ge, Penglei Sun...
- Keyword score: 1 / hits: multi-agent

<details><summary>Abstract</summary>

While autonomous navigation has achieved remarkable success in passive perception (e.g., object detection and segmentation), it remains fundamentally constrained by a void in knowledge-driven, interactive environmental cognition. In the high-stakes domain of maritime navigation, the ability to bridge the gap between raw visual perception and complex cognitive reasoning is not merely an enhancement but a critical prerequisite for Autonomous Surface Vessels to execute safe and precise maneuvers. To this end, we present WaterVideoQA, the first large-scale, comprehensive Video Question Answering benchmark specifically engineered for all-waterway environments. This benchmark encompasses 3,029 video clips across six distinct waterway categories, integrating multifaceted variables such as volatile lighting and dynamic weather to rigorously stress-test ASV capabilities across a five-tier hierarchical cognitive framework. Furthermore, we introduce NaviMind, a pioneering multi-agent neuro-symbolic system designed for open-ended maritime reasoning. By synergizing Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification, NaviMind transitions ASVs from superficial pattern matching to regulation-compliant, interpretable decision-making. Experimental results demonstrate that our framework significantly transcends existing baselines, establishing a new paradigm for intelligent, trustworthy interaction in dynamic maritime environments.

</details>

**LLM Summary**

- What: 自律型水上船舶（ASV）向けの、水上環境に特化した大規模なビデオ質問応答（VQA）ベンチマーク「WaterVideoQA」と、多モーダルエージェントシステム「NaviMind」を提案した。
- Novelty: 3,029個のビデオクリップと5段階の階層的認知フレームワークを含む、初の包括的な水上環境向けVQAベンチマークを構築した。また、適応的意味ルーティング、状況認識型階層的推論、自律的自己反省的検証を組み合わせたニューロシンボリックシステム「NaviMind」を開発した。
- Why it matters: ASVの知識駆動型でインタラクティブな環境認知能力を評価・向上させ、安全で正確な操船を実現するための基盤を提供する。

## 20. GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion
- arXiv: http://arxiv.org/abs/2602.22862v1
- PDF: https://arxiv.org/pdf/2602.22862v1
- Authors: Enda Xiang, Haoxiang Ma, Xinzhu Ma, Zicheng Liu, Di Huang
- Keyword score: 1 / hits: imitation learning

<details><summary>Abstract</summary>

This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.

</details>

**LLM Summary**

- What: 模倣学習による操作ポリシーの把持精度と汎化性能を向上させるための潜在拡散モデルベースのアプローチ「GraspLDP」を提案した。
- Novelty: 把持の事前知識を拡散ポリシーフレームワークに組み込み、把持ポーズの事前知識でアクションチャンクデコーディングをガイドする。また、拡散中に自己教師あり再構成目的を導入し、把持性の事前知識を埋め込む。
- Why it matters: より正確で汎用性の高い把持ポリシーを実現し、ロボット操作タスクの性能を向上させる。
